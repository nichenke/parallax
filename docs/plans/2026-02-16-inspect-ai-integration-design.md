# Design: Inspect AI Integration for Parallax Eval Framework

**Date:** 2026-02-16
**Status:** Approved
**Related:** Requirements v1.1 (`docs/requirements/inspect-ai-integration-requirements-v1.md`), ADR-005

---

## Problem

Parallax skills are developed iteratively with no systematic way to measure effectiveness. Prompt changes may improve, degrade, or have no effect on finding quality — we cannot tell which. This blocks empirical improvement and risks shipping broken skills.

**Ground truth challenge:** v3 findings were generated by the skill under test. Validating fresh review outputs against the current design (not old outputs) breaks the circular dependency.

---

## Approach

Use Inspect AI's conventional patterns (Dataset/Sample/Task/Scorer) as the eval foundation. Build only parallax-specific scorers and dataset loaders. Skills and evals remain decoupled — skill content is loaded at eval runtime so prompt changes flow through automatically.

**Key workflow insight:** Ground truth is living, not a one-time capture. After significant design changes, run a fresh review, validate findings in the browser UI, then re-establish ground truth. Evals then verify the skill catches confirmed real flaws without regression.

---

## Repository Structure

```
parallax/
├── evals/
│   ├── __init__.py
│   ├── severity_calibration.py        # @task: Critical finding detection
│   ├── ablation_tests.py              # @task: Skill content contribution tests
│   └── utils/
│       ├── __init__.py
│       └── dataset_loader.py          # JSONL → Inspect AI Dataset converter
├── scorers/
│   ├── __init__.py
│   └── severity_scorer.py             # @scorer: recall, precision, F1
├── datasets/
│   └── v3_review_validated/
│       ├── critical_findings.jsonl    # Ground truth (from validation UI)
│       └── metadata.json              # Dataset info, design doc hash
├── tools/
│   ├── validate_findings.py           # Browser-based validation UI (Flask)
│   └── compare_to_baseline.py        # Regression detection script
├── evals/baselines/
│   └── v3_critical_baseline.json     # Stored after first passing run
├── pyproject.toml
├── .python-version                    # 3.11
├── Makefile
└── README-evals.md
```

**Separation of concerns:**
- **Skills** (Markdown) — design review logic, runs in Claude Code
- **Evals** (Python) — validation framework, runs via `inspect eval` CLI
- **Datasets** (JSONL + metadata) — ground truth, version-controlled in git
- **Scorers** (Python) — reusable scoring functions, callable from any eval task

---

## Dataset Schema

**`datasets/v3_review_validated/critical_findings.jsonl`**

One finding per line. Extends existing parallax JSONL schema with validation fields:

```jsonl
{
  "id": "v3-assumption-hunter-001",
  "title": "Ground truth validity assumed",
  "severity": "Critical",
  "issue": "...",
  "suggestion": "...",
  "reviewer": "assumption-hunter",
  "confidence": 0.92,
  "validated": true,
  "validation_status": "real_flaw",
  "validation_notes": "Confirmed — v3 review had no prior expert validation",
  "validator_id": "nic",
  "validation_date": "2026-02-16"
}
```

`validation_status` values: `real_flaw` | `false_positive` | `ambiguous`

Only `real_flaw` findings are used as ground truth.

**`datasets/v3_review_validated/metadata.json`**

```json
{
  "source_review": "parallax-review-v1",
  "design_doc_path": "docs/plans/parallax-design-v4.md",
  "design_doc_hash": "sha256:abc123...",
  "review_date": "2026-02-16",
  "validation_date": "2026-02-16",
  "validator": "nic",
  "total_findings": 15,
  "severity_distribution": {"Critical": 15, "Important": 0, "Minor": 0},
  "false_positive_rate": 0.12,
  "skill_version": "git:44965c9"
}
```

`design_doc_hash` tracks whether the design has changed since ground truth was created. Stale ground truth (hash drift) triggers a warning in `make validate`.

---

## Dataset Conversion

**`evals/utils/dataset_loader.py`**

Converts parallax JSONL findings into Inspect AI's `Dataset`/`Sample` format:

```python
def load_validated_findings(dataset_path: str) -> Dataset:
    findings = read_jsonl(dataset_path / "critical_findings.jsonl")
    metadata = read_json(dataset_path / "metadata.json")

    sample = Sample(
        input={
            "design_doc": metadata["design_doc_path"],
            "skill_version": metadata.get("skill_version", "current")
        },
        target={
            "expected_findings": findings,
            "severity_distribution": count_by_severity(findings)
        },
        metadata=metadata
    )

    return Dataset(samples=[sample])
```

- `input` — what the skill needs to run a review
- `target` — ground truth the scorer compares against
- `metadata` — traceability (validator, date, source review)

Phase 2 adds more Samples (requirements-light, pattern-extraction datasets).

---

## Eval Task Definition

**`evals/severity_calibration.py`**

Loads actual skill content at runtime — skill changes automatically flow through to evals:

```python
def load_skill_content(skill_name: str) -> str:
    skill_path = Path(__file__).parent.parent / "skills" / skill_name / "SKILL.md"
    return skill_path.read_text()

@task
def severity_calibration_eval():
    return Task(
        dataset=load_validated_findings("datasets/v3_review_validated"),
        plan=[
            system_message(load_skill_content("parallax:requirements")),
            generate()
        ],
        scorer=severity_calibration(),
        max_tokens=16000
    )
```

**`evals/ablation_tests.py`**

Tests that skill content actually contributes to detection (FR4):

```python
@task
def ablation_no_personas():
    skill = load_skill_content("parallax:requirements")
    ablated = drop_section(skill, "## Personas")

    return Task(
        dataset=load_validated_findings("datasets/v3_review_validated"),
        plan=[system_message(ablated), generate()],
        scorer=severity_calibration()
    )
```

Ablation test passes if dropping a section causes detection rate to drop >50%.

---

## Scorer Implementation

**`scorers/severity_scorer.py`**

```python
@scorer(metrics=[accuracy()])
def severity_calibration():
    async def score(state, target):
        actual_findings = parse_review_output(state.output.completion)
        expected_findings = target.target["expected_findings"]

        detected = match_findings(actual_findings, expected_findings)

        recall    = len(detected) / len(expected_findings)
        precision = len(detected) / len(actual_findings) if actual_findings else 0
        f1        = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        passes = recall >= 0.90 and precision >= 0.80

        return Score(
            value={"recall": recall, "precision": precision, "f1": f1},
            answer=detected,
            explanation=f"Detected {len(detected)}/{len(expected_findings)} findings. "
                       f"Recall: {recall:.2%}, Precision: {precision:.2%}",
            metadata={
                "passes_threshold": passes,
                "missed_findings": [f["id"] for f in expected_findings if f not in detected],
                "false_positives": [f for f in actual_findings if f not in detected]
            }
        )
    return score
```

**Finding matching strategy:**
1. Exact ID match first
2. Fuzzy match fallback: same severity + 80%+ title/issue text overlap
3. Phase 2: replace fuzzy match with embedding similarity

**Thresholds (FR2.2, provisional — adjust after first runs):**
- Recall ≥ 90%
- Precision ≥ 80%
- F1 ≥ 0.74

---

## Developer Workflow

### Makefile

```makefile
DESIGN_DOC ?= docs/plans/parallax-design-v4.md
MODEL       ?= anthropic/claude-sonnet-4-5
LOG_DIR     ?= logs/

setup:
	python3.11 -m venv .venv-evals
	. .venv-evals/bin/activate && pip install -e ".[dev]"

## Ground truth creation (run after significant design changes)
review:
	parallax:review $(DESIGN_DOC)

validate:
	. .venv-evals/bin/activate && python tools/validate_findings.py

## Eval loop (run when iterating on skills)
eval:
	. .venv-evals/bin/activate && \
	inspect eval evals/severity_calibration.py \
	    --model $(MODEL) \
	    --log-dir $(LOG_DIR) \
	    --tags "git=$(shell git rev-parse --short HEAD)"

baseline:
	cp $(shell ls -t $(LOG_DIR)/*.json | head -1) evals/baselines/v3_critical_baseline.json

regression:
	. .venv-evals/bin/activate && python tools/compare_to_baseline.py

ablation:
	. .venv-evals/bin/activate && \
	inspect eval evals/ablation_tests.py \
	    --model $(MODEL) --log-dir $(LOG_DIR)

view:
	. .venv-evals/bin/activate && inspect view

## Full validation cycle
cycle: eval regression view

.PHONY: setup review validate eval baseline regression ablation view cycle
```

### TDD Loop

**When design evolves significantly:**
```
make review     → fresh findings on current design doc
make validate   → classify findings in browser UI
                  (mark real_flaw / false_positive / ambiguous)
# Ground truth is now current and reflects actual design state.
```

**When iterating on skill prompts:**
```
[edit skills/parallax:requirements/SKILL.md]
make eval       → skill detects findings against ground truth?
make regression → nothing should drop vs baseline
make view       → inspect missed findings, false positives
make baseline   → store when satisfied
```

**When fixing actual design issues:**
```
[fix design doc based on findings]
make review     → re-run review on updated design
make eval       → fixed findings no longer appear
                  regression check confirms no new gaps introduced
```

---

## Regression Detection

**`tools/compare_to_baseline.py`** output:

```
Comparing to baseline: evals/baselines/v3_critical_baseline.json

  Baseline (git:44965c9):  recall=0.93  precision=0.87  f1=0.90
  Current  (git:current):  recall=0.91  precision=0.85  f1=0.88

  Delta: recall=-0.02  precision=-0.02  f1=-0.02
  Status: PASS (all deltas within 10% threshold)

  Missed (vs baseline): none
  New detections: none
```

**Status levels:**
- **PASS** — all metrics within 10% of baseline
- **WARN** — any metric drops 5–10% (investigate before accepting)
- **FAIL** — any metric drops >10% (fix skill before merging)

Statistical significance tests deferred: N=15–22 is too small for reliable p-values. Right fix is more test data, not stats machinery.

---

## Python Environment

**`pyproject.toml`:**
```toml
[project]
name = "parallax-evals"
requires-python = ">=3.11"

[project.dependencies]
inspect-ai = ">=0.3"
flask = ">=3.0"

[tool.uv]
dev-dependencies = ["pytest"]
```

**`.python-version`:** `3.11`

**`.gitignore` additions:**
```
.venv-evals/
logs/
*.env
```

**API key pattern (never committed):**
```bash
# ~/.zshrc or local .env (gitignored)
export ANTHROPIC_API_KEY="sk-..."
```

Work context uses Bedrock IAM credentials (separate from personal key).

---

## MVP Scope (Phase 1)

**In scope:**
- `evals/severity_calibration.py` — Critical finding detection eval
- `evals/ablation_tests.py` — Coarse-grained ablation (section-level)
- `scorers/severity_scorer.py` — Recall, precision, F1 scorer
- `evals/utils/dataset_loader.py` — JSONL → Inspect AI Dataset
- `tools/validate_findings.py` — Browser-based validation UI
- `tools/compare_to_baseline.py` — Regression detection
- `Makefile` — Developer workflow targets
- `datasets/v3_review_validated/` — Initial ground truth (≥15 validated Critical findings)
- `pyproject.toml`, `.python-version` — Environment setup

**Explicitly deferred to Phase 2:**
- `finding_quality_scorer.py` — LLM-as-judge
- `pattern_extraction_scorer.py` — Semantic clustering validation
- `transcript_analysis_scorer.py` — Agent config bug detection (Issue #37)
- Multi-model comparison (Codex/GPT compatibility confirmed, not tested yet)
- Planted flaw test datasets
- Batch API integration
- CI/CD automation

---

## Open Questions (To Resolve During Implementation)

1. **Inspect AI Dataset schema:** Research exact `Sample` field names and types before writing `dataset_loader.py`. Start from Inspect AI tutorial examples.
2. **Skill loading path:** Confirm relative path from `evals/` to `skills/` directory is stable (or use absolute path from repo root).
3. **Fuzzy matching threshold:** 80% text overlap is a starting hypothesis. Tune after first eval run — if too many false negatives, lower threshold.
4. **Baseline update policy:** Update baseline after intentional skill improvements. Block updates if regression detected.

---

## References

- Requirements v1.1: `docs/requirements/inspect-ai-integration-requirements-v1.md`
- ADR-005: `docs/requirements/adr-005-prior-art-integration-decisions.md`
- Inspect AI docs: https://inspect.aisi.org.uk/
- Issue #5: Skill testing eval/grader framework
