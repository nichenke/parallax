OpenAI Codex v0.101.0 (research preview)
--------
workdir: /Users/nic/src/design-parallax/parallax
model: gpt-5.3-codex
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: medium
reasoning summaries: auto
session id: 019c64b3-8a9e-77f0-a6eb-30608baf0e7d
--------
user
You are executing reviewer lane: requirement-auditor for parallax:review.
Follow these agent instructions exactly.


You are the Requirement Auditor — an adversarial design reviewer who checks whether the design actually delivers what was required, nothing more, nothing less.

**Your core question:** "Does this actually satisfy the requirements?"

**Your focus areas:**
- Coverage gaps: requirements that the design doesn't address
- Contradictions: design choices that conflict with stated requirements
- Gold-plating: design features that no requirement asked for (YAGNI violations)
- Anti-goal violations: design choices that do something the requirements explicitly said not to do
- Priority misalignment: design spending disproportionate effort on low-priority requirements
- Testability: can you verify each requirement is met by looking at the design?
- Traceability: can you trace each design decision back to a requirement?

**Voice rules:**
- Active voice. Lead with impact, then evidence.
- No hedging ("might", "could", "possibly"). State findings directly.
- Quantify blast radius where possible.
- SRE-style framing: what's the failure mode, what's the blast radius, what's the mitigation.

**Review process:**
0. Before evaluating any element, ask: "Should this exist at all?" Never optimize or critique something that should be deleted entirely.
1. For each requirement, first ask: "Should this requirement exist at all?" Flag requirements that add complexity without clear value as calibrate gaps.
2. Read the requirements document — build a mental checklist
3. Read the design document against that checklist
4. For each requirement, ask: "Is this addressed? How? Is the approach sufficient?"
5. For each design feature, ask: "Which requirement drove this? If none, is it gold-plating?"
6. Check for anti-goals: "Does the design do anything the requirements said to avoid?"
7. Check priorities: "Are must-haves fully addressed before nice-to-haves?"

**Auto-escalation rule:** If requirements themselves are contradictory or impossible to satisfy together, flag as automatic escalate (calibrate gap, severity Critical). Do not attempt design iteration — requirements need fixing first.

**Output format:**

Write your findings as structured markdown:

```
# Requirement Auditor Review

## Coverage Matrix
| Requirement | Addressed? | Design Section | Notes |
|---|---|---|---|
| [req 1] | Yes/Partial/No | [section] | [brief note] |

## Findings

### Finding N: [Title]
- **Severity:** Critical | Important | Minor
- **Phase:** [primary phase] (primary), [contributing phase] (contributing, if applicable)
- **Section:** [which part of the design]
- **Issue:** [what requirement problem was found]
- **Why it matters:** [impact on delivery]
- **Suggestion:** [how to resolve]

## Blind Spot Check (optional — being empirically validated)
[What might I have missed given my focus on requirements? What design quality issues would other reviewers catch?]
```

**Severity guidelines:**
- **Critical:** A must-have requirement is unaddressed or contradicted.
- **Important:** A should-have requirement is partially addressed or a clear YAGNI violation adds significant complexity.
- **Minor:** A nice-to-have is missing or a small gold-plating instance.

**Phase classification (assign primary, optionally note contributing):**
- **survey:** Requirement references something that wasn't researched
- **calibrate:** Requirements themselves are contradictory or incomplete (upstream problem)
- **design:** Design fails to cover or contradicts a requirement
- **plan:** Requirement will need specific implementation attention

**Important:** The coverage matrix is mandatory — it forces systematic checking rather than impression-based review. Even if every requirement is covered, include the matrix.

Task:
- Stage: design
- Topic label: deterministic-runner-smoke-v3
- Design artifact path: /Users/nic/src/design-parallax/parallax/docs/codex/plans/2026-02-16-deterministic-runner-design.md
- Requirements artifact path: /Users/nic/src/design-parallax/parallax/docs/codex/2026-02-16-deterministic-runner-requirements.md
- Supplemental context paths:
  - /Users/nic/src/design-parallax/parallax/docs/codex/2026-02-16-deterministic-runner-options.md
  - /Users/nic/src/design-parallax/parallax/docs/codex/2026-02-16-deterministic-runner-recommendation.md
  - /Users/nic/src/design-parallax/parallax/docs/codex/2026-02-16-deterministic-runner-plan.md
- Review the design against requirements and supplemental context.
- Write findings in the exact output format mandated above.
- Output markdown only. Do not use code fences.

Artifact text begins below.

### DESIGN ARTIFACT (/Users/nic/src/design-parallax/parallax/docs/codex/plans/2026-02-16-deterministic-runner-design.md)
# Design: Deterministic Cross-Runtime Review Runner

**Date:** 2026-02-16  
**Status:** Proposed (updated after smoke review dispositions)  
**Scope:** `parallax:review` design-stage runner only

## Overview

Build a local deterministic runner that executes the existing review workflow with explicit state, replayable artifacts, and bounded failure handling across Codex and Claude runtimes.

For this increment, markdown remains canonical (`summary.md` + reviewer markdown files) for human and semantic comparison. Structured canonical JSON/JSONL (`summary.json`, `findings.jsonl`) is deferred until Codex workflow + telemetry are in place.

## Inputs

- Design artifact path (markdown)
- Requirements artifact path (markdown)
- Stage (`design`, default)
- Topic label
- Runtime (`codex` or `claude`)
- Runner config (reviewer order, mandatory/optional set, retry policy)

## Outputs

For each run:

`docs/codex/reviews/<topic>/runs/<run_id>/`

- `run_manifest.json`
- `run_state.json`
- `events.jsonl`
- `reviewer_outcomes.jsonl`
- Reviewer markdown outputs (`assumption-hunter.md`, ...)
- `summary.md` (canonical summary artifact for this increment)

Deferred outputs (future increment):
- `summary.json`
- `findings.jsonl`

## Architecture

### Runner Core

Single script entrypoint with subcommands:

- `plan`: validate inputs, snapshot hashes, compute deterministic IDs, write manifest
- `run`: execute reviewers + synthesizer with state/event logging
- `resume --run-id <id>`: continue `pending|retryable|manual_handoff` work only

Run collision policy:
- If `docs/codex/reviews/<topic>/runs/<run_id>/` already exists, fail fast by default.
- Replays require explicit `--replay-attempt <n>` and write to `.../runs/<run_id>/replays/<n>/`.
- Operators should commit review outputs to git between runs when artifact history must be preserved.

### Adapter Layer

Adapter contract:

`run_reviewer(reviewer_name, manifest, attempt, timeout_s) -> ReviewerOutcome`

Adapters:
- `claude_adapter`
- `codex_adapter`
- `codex_app_handoff_adapter` (development fallback for GUI-only flow)

Handoff policy:
- GUI handoff remains supported during development.
- Goal is to close GUI/CLI drift with prompt generation/runtime alignment, not maintain separate long-term comparability models.
- Handoff ingest must validate schema + hashes before state advancement.

### Config Layer

`config/review_runner.yaml` defines:
- canonical reviewer order
- mandatory vs optional reviewers
- per-reviewer timeout/retry overrides
- runtime defaults (`effort`, `temperature`) read from agent frontmatter hints
- retry backoff profile (base delay, jitter, max attempts)

## Determinism Controls

- `run_id` generated from canonical manifest hash (exclude wall-clock timestamps).
- `content_fingerprint` generated from canonicalized content inputs only (topic-agnostic).
- Stable reviewer dispatch order from config.
- Stable JSON serialization (sorted keys, normalized newlines).
- Prompt/input hashing stored in manifest for replayability.
- Effective runtime settings captured per reviewer attempt (`effective_runtime_settings`).
- Deterministic summary rendering order:
  - severity (`Critical`, `Important`, `Minor`)
  - phase (`survey`, `calibrate`, `design`, `plan`)
  - reviewer canonical order

## Failure Semantics

### Reviewer-Level

- `pending`
- `running`
- `succeeded`
- `failed_transient`
- `failed_timeout`
- `failed_fatal`
- `skipped_capability` (example: no web for `prior-art-scout`)
- `manual_handoff`

### Retry Policy

- Default attempts: 2 total (1 retry) for transient failures.
- Retry uses bounded exponential backoff with jitter:
  - attempt 2 delay: `base_delay_ms * 2^(attempt-2) + jitter_ms`
  - default `base_delay_ms = 2000`, `jitter_ms = 0..500`
- No retry for `skipped_capability` or `failed_fatal`.

### Run-Level Terminal States

- `created`
- `running`
- `waiting_for_handoff`
- `success`
- `partial_success`
- `failed`

### State Transition Rules

Run transitions:
- `created -> running` on `run_started`
- `running -> waiting_for_handoff` on reviewer terminal `manual_handoff`
- `waiting_for_handoff -> running` on valid handoff ingest
- `running -> success` when all mandatory reviewers succeed and synthesizer succeeds
- `running -> partial_success` when mandatory reviewers succeed, optional reviewer is skipped/failed, and synthesizer succeeds
- `running -> failed` when mandatory quorum is not met or synthesizer fails

Reviewer transitions:
- `pending -> running` on dispatch
- `running -> succeeded | failed_transient | failed_timeout | failed_fatal | skipped_capability | manual_handoff`
- `failed_transient -> running` on retry schedule (attempt < max)
- `manual_handoff -> succeeded | failed_fatal` after ingest validation

### Atomic Synthesis

- Write synthesis output to temporary paths (`summary.md.tmp`).
- Validate content and state marker.
- Atomically rename to final path.
- Persist synthesis checkpoint info in `run_state.json` before terminal run status.

## Partial Failure Policy

Design-stage mandatory reviewers:
- assumption-hunter
- edge-case-prober
- requirement-auditor
- feasibility-skeptic
- first-principles

Optional reviewer:
- prior-art-scout

If optional reviewer fails or is capability-skipped:
- run remains valid but is marked PARTIAL in `summary.md`
- summary includes confidence downgrade note for prior-art coverage
- if the same topic has 2 consecutive prior-art capability skips, add an escalation note to revisit calibrate-phase build-vs-buy assumptions

Capability preflight:
- runner performs capability probe before dispatch (for web-dependent reviewers)
- when capability is unavailable, log explicit reason and remediation path in events + summary metadata

## Data Model (Minimal)

### run_manifest.json

- `run_id`
- `content_fingerprint`
- `created_at`
- `stage`
- `topic`
- `runtime`
- `inputs` (paths + content hashes)
- `agent_prompt_hashes`
- `config_hash`
- `reviewer_plan` (ordered list, mandatory flags, policy snapshot)
- `retry_policy`
- `manifest_schema_version`

### run_state.json

- `run_status`
- `run_transition_version`
- `reviewers` map:
  - `status`
  - `attempts`
  - `last_error_class`
  - `output_path`
  - `effective_runtime_settings`
  - `last_retry_delay_ms`
- `synthesizer_status`
- `synthesis_checkpoint`

### reviewer_outcomes.jsonl

One line per attempt:
- `run_id`
- `content_fingerprint`
- `reviewer`
- `attempt`
- `status`
- `error_class`
- `duration_ms`
- `retry_delay_ms`
- `effective_runtime_settings`
- `artifact_hash`

## Testing Strategy

1. Determinism tests:
   - identical inputs => identical `run_id`, `content_fingerprint`, and run plan
2. Replay collision tests:
   - existing `run_id` path fails without `--replay-attempt`
3. Resume tests:
   - interrupted run resumes without rerunning completed reviewers
4. State-machine tests:
   - invalid transitions are rejected
5. Partial failure tests:
   - `prior-art-scout` capability skip => `partial_success` + confidence downgrade marker
   - repeated skip threshold (2 consecutive runs on same topic) => escalation note
6. Retry tests:
   - transient failures use bounded backoff+jitter and stop at max attempts
7. Synthesis atomicity tests:
   - interrupted synthesis never publishes stale/partial final `summary.md`
8. Cross-runtime contract tests:
   - same manifest through `codex` and `claude` preserves schema and semantic comparison metadata

## Migration Strategy

Phase 1 (this design): local deterministic runner, markdown-canonical outputs, semantic comparison metadata.  
Phase 2: add canonical structured outputs (`summary.json`, `findings.jsonl`) once Codex workflow + telemetry are ready.  
Phase 3: optional LangGraph wrapper around the same state model and artifacts.

LangGraph adoption gate:
- move when we automate 2+ stages and need durable multi-stage pause/resume/branching beyond local script complexity.

## Open Questions

1. What exact readiness criteria switch canonical output from markdown-only to JSON/JSONL (telemetry coverage, parser confidence, eval maturity)?
2. Do we require runtime-level seed support for stronger repeatability, or treat process determinism as sufficient for v1?
3. What test protocol will prove GUI vs CLI output drift (or confirm parity) after prompt generator/runtime alignment work?

### REQUIREMENTS ARTIFACT (/Users/nic/src/design-parallax/parallax/docs/codex/2026-02-16-deterministic-runner-requirements.md)
# Deterministic Runner Requirements

**Date:** 2026-02-16  
**Status:** Updated after smoke-review dispositions for `deterministic-runner-smoke-v2`

## Problem

Current review orchestration is useful but not deterministic enough for reproducible evals across Codex and Claude runtimes. Partial failures (especially web-dependent reviewer behavior) need explicit, replayable semantics.

## Goal

Define requirements for a design-stage runner that executes review workflows reliably, captures deterministic run state, and supports resume/replay without changing core reviewer behavior.

## In Scope

- `parallax:review` design stage only
- Cross-runtime execution support (Codex CLI, Codex GUI handoff flow, Claude Code)
- Deterministic run identity, ordering, and artifact layout
- Explicit partial-failure handling (including no-web cases)
- Markdown-first semantic comparison for this increment

## Out of Scope

- Immediate implementation of `survey`, `calibrate`, `plan`, `orchestrate`, or `eval` stages
- Replacing reviewer prompts with a new review methodology
- Full LangGraph adoption in this increment
- Fully deterministic LLM text output across providers

## Constraints

- Keep existing reviewer/synthesizer prompts and design-stage semantics intact.
- Preserve markdown outputs for human review.
- Prefer incremental changes over a big-bang rewrite.

## MoSCoW

### Must Have

- Deterministic `run_id` from canonical run manifest.
- Topic-agnostic `content_fingerprint` for same-content cross-topic replay analysis.
- Snapshot hashes for all run-critical inputs (design, requirements, agent prompts, config).
- Explicit state machine for run + reviewer outcomes, with transition guards.
- Structured event/outcome logs (`events.jsonl`, `reviewer_outcomes.jsonl`).
- `reviewer_outcomes.jsonl` records effective runtime settings per attempt.
- Resume mode that does not rerun completed reviewers.
- Atomic summary publish behavior (no stale/partial final `summary.md` on interruption).
- Partial-run semantics with explicit statuses for:
  - `success`
  - `partial_success`
  - `failed`
- Default policy for no-web reviewer capability:
  - mark `prior-art-scout` as capability-skipped (not silent failure)
  - include explicit confidence downgrade in summary output
- Bounded transient retry policy with backoff + jitter.
- Run output collision policy (fail fast or explicit replay-attempt path) to avoid artifact clobbering.

### Should Have

- Runtime adapter abstraction (`claude`, `codex`) under one runner command.
- Semantic comparison metadata section in `summary.md` that captures non-finding context (reviewer completion map, capability constraints, runtime caveats).
- Consecutive prior-art capability skips (same topic) trigger explicit escalation note.

### Could Have

- Canonical `summary.json` and `findings.jsonl` once Codex workflow + telemetry are in place.
- Run comparison utility separating orchestration drift from model-content drift.
- Manual handoff adapter for Codex App GUI-only execution path.
- Per-reviewer override policy (timeouts/retries/model hints) in config.

### Won't Have (This Iteration)

- Full multi-stage orchestration framework migration (LangGraph).
- Advanced evaluator/judge automation for finding quality scoring.
- JSON/JSONL as canonical outputs in this increment.

## Non-Functional Requirements

- Reproducibility:
  - identical manifest inputs produce identical run plan and artifact ordering
  - identical content inputs produce identical `content_fingerprint`
- Reliability:
  - transient failures use bounded retries with backoff+jitter
  - deterministic handling for non-retryable capability failures
- Observability:
  - each reviewer attempt and terminal outcome is logged with timestamps + error class
  - effective runtime settings are logged per attempt
- Portability:
  - same runner contract works across Codex and Claude adapters
- Comparability:
  - for this increment, semantic comparison is driven by `summary.md` plus semantic metadata

## Acceptance Criteria

1. Two runs with identical inputs produce the same `run_id` and same reviewer execution plan.
2. Two runs with identical content but different topic labels produce the same `content_fingerprint`.
3. If a run is interrupted mid-review, resume completes without rerunning already-completed reviewers.
4. If synthesis is interrupted, final `summary.md` is either valid/final or absent (never partial/stale).
5. If `prior-art-scout` has no web access, run completes with `partial_success` (assuming mandatory reviewers succeeded), summary includes explicit PARTIAL marker, and confidence downgrade note.
6. If prior-art capability is skipped in 2 consecutive runs for the same topic, the summary adds an escalation note.
7. Reviewer outcome records include effective runtime settings and retry timing metadata.
8. Existing design-stage review outputs remain human-readable and compatible with current workflow expectations.

### SUPPLEMENTAL CONTEXT (/Users/nic/src/design-parallax/parallax/docs/codex/2026-02-16-deterministic-runner-options.md)
# Deterministic Cross-Runtime Runner Options

**Date:** 2026-02-16  
**Scope:** `parallax:review` design-stage runner only (keep current reviewer behavior intact)

## Decision Frame

We need deterministic execution across runtimes (Codex/Claude), explicit partial-failure handling, and replayable artifacts without a big-bang rewrite.

Evaluation criteria:
- Reliability under partial failures (timeouts, no-web, transient API errors)
- Reproducibility for eval comparisons
- Operational simplicity for current repo maturity
- Migration path to richer orchestration later

## Option A: Lightweight Local Runner (Single Script + Config)

### Architecture Sketch

```text
runner.py
  -> load run config + snapshot inputs
  -> write run_manifest.json (deterministic run_id)
  -> execute reviewers in fixed order (or bounded parallel workers with deterministic scheduling)
  -> persist per-reviewer state transitions to run_state.json
  -> write reviewer artifacts + outcomes.jsonl
  -> execute synthesizer
  -> render summary.md from structured outputs
```

### Failure Semantics

- Timeout per reviewer: fixed (ex: 120s), marked `timeout`.
- Retry policy: max 1 retry for transient failures, fixed backoff (ex: 5s).
- Capability failure (no web for `prior-art-scout`): mark `skipped_capability`, continue.
- Partial run status: `partial_success` when at least mandatory reviewers complete.
- Resume: rerun with `--resume <run_id>` to execute only `pending|retryable` reviewers.

### Determinism Controls

- Canonical reviewer order in config.
- `run_id = sha256(canonical_manifest_without_timestamps)`.
- Snapshot input file hashes (design, requirements, prompts) in manifest.
- Fixed output paths per run (`docs/reviews/<topic>/runs/<run_id>/...`).
- Stable serialization for JSON artifacts (sorted keys, newline-normalized text).
- Deterministic summary rendering order (severity -> phase -> reviewer order).

### Operational Complexity

- Low. Standard library script + JSON files.
- Easy local debugging and CI portability.

### Migration Cost

- Low initial cost, medium later if state model is ad hoc.
- Keep migration low by defining state schema now as graph-compatible.

### Fit For Current Parallax Stage

- Strong fit. Keeps prototype velocity and solves the immediate deterministic-runner gap.

## Option B: LangGraph Orchestration Now

### Architecture Sketch

```text
LangGraph StateGraph
  Nodes: snapshot -> dispatch reviewers -> collect -> synthesize -> render
  Checkpointer: SQLite/Postgres
  Human-interrupt hooks for partial failures and retries
```

### Failure Semantics

- Native stateful retries per node.
- Durable checkpoints and resume after crash.
- Clear branch logic for skip/partial/fail paths.

### Determinism Controls

- Deterministic graph topology and node ordering.
- Persisted state snapshots per step.
- Reproducible replay from checkpoints if node code is deterministic.

### Operational Complexity

- Medium-high.
- Adds framework concepts, dependency management, and runtime plumbing now.

### Migration Cost

- Higher upfront integration; lower long-term if multi-stage orchestration is imminent.

### Fit For Current Parallax Stage

- Premature for a single active stage unless we already need long-lived execution, queueing, and human interrupts beyond local runs.

## Option C: Hybrid Phased Approach (Recommended Candidate)

### Architecture Sketch

```text
Phase 1 (now): Local deterministic runner
  - strict state schema
  - explicit event log
  - runtime adapters (codex/claude)

Phase 2 (later): Wrap same state model in LangGraph
  - map existing step handlers to graph nodes
  - reuse artifact schema and replay rules
```

### Failure Semantics

- Same as Option A immediately.
- Adds richer branch/interrupt behavior later without changing artifact contract.

### Determinism Controls

- Deterministic contracts implemented now (IDs, ordering, snapshots).
- LangGraph migration reuses contracts, preserving eval continuity.

### Operational Complexity

- Medium overall, but low immediate disruption.

### Migration Cost

- Lowest risk path if Phase 1 defines clean interfaces (`RunnerState`, `ReviewerResult`, `RetryPolicy`).

### Fit For Current Parallax Stage

- Best fit: incremental, reliable, and compatible with BUILD-novel/LEVERAGE-infra philosophy.

## Head-to-Head Summary

| Dimension | Option A | Option B | Option C |
|---|---|---|---|
| Time to first deterministic runner | Fastest | Slowest | Fast |
| Reliability now | Good | Excellent | Good now, excellent later |
| Reproducibility now | Good | Good | Good |
| Complexity now | Low | High | Low-medium |
| Long-term orchestration ceiling | Medium | High | High |
| Risk of over-engineering | Low | High | Low |
| Fit for current repo maturity | High | Medium-low | High |

## Answers to Research Questions

1. **Minimum deterministic runner:** Option A core from Option C: one local runner with manifest/state/event artifacts and resume support.
2. **When LangGraph is worth it:** once we automate 2+ stages with persistent checkpoints/human interrupts, or need durable headless execution in CI/services.
3. **Run state representation:** explicit state machine + event log + per-reviewer outcomes (attempt-level).
4. **Canonical artifact format:** JSONL/JSON as source of truth; markdown as rendered human view.
5. **Default retry/failure policy:** one retry for transient errors, zero retries for deterministic capability failures (no-web), explicit partial status.
6. **Reproducibility controls:** manifest hashing, input snapshot hashes, stable ordering/serialization, deterministic render pipeline, pinned prompt files.

## Assumptions and Unknowns

Assumptions:
- Design-stage review remains the only implemented stage for now.
- `prior-art-scout` may run in environments without web access.
- We can add runner artifacts without breaking existing review docs.

Unknowns:
- Cross-runtime differences in model settings (temperature/seed support) may limit strict output equivalence.
- JSON extraction quality from reviewer markdown may require prompt tuning if we do not move reviewers to structured output quickly.

### SUPPLEMENTAL CONTEXT (/Users/nic/src/design-parallax/parallax/docs/codex/2026-02-16-deterministic-runner-recommendation.md)
# Deterministic Runner Recommendation

**Date:** 2026-02-16  
**Recommendation:** **Option C (Hybrid phased approach): build a lightweight deterministic local runner now, with a LangGraph-compatible state contract from day one.**

## Executive Decision

Implement a minimal local orchestrator in this repo next. Do not adopt LangGraph yet.  
Design all runner artifacts and interfaces so migration to LangGraph is a wrapper change, not a data model rewrite.

## Why This Is The Right Choice

### Why now

- We have one active stage (`review`), so framework overhead is not justified yet.
- Determinism/replay/partial-failure handling are immediate blockers and can be solved quickly with local orchestration.
- This keeps review behavior intact while adding reproducibility required for eval comparisons.

### Why not now (LangGraph now)

- LangGraph is strongest when we need durable multi-stage orchestration with complex branching and human interrupts across long-lived runs.
- Adopting it before state/artifact contracts are settled increases churn and hides core design decisions behind framework abstraction.
- Current risk is not graph control flow; it is lack of deterministic run contracts.

## Target Architecture (Implement Next)

### 1) Runner core

- File: `scripts/review_runner.py` (or equivalent local script)
- Inputs: stage, topic, design path, requirements path, runtime profile, retry profile
- Output root: `docs/reviews/<topic>/runs/<run_id>/`

### 2) Canonical state + artifacts

Use structured artifacts as source of truth:

- `run_manifest.json`
  - immutable snapshot of inputs and config
  - includes hashes of design/requirements/reviewer prompt files
  - deterministic `run_id` derived from manifest (excluding runtime timestamps)
- `run_state.json`
  - current state machine snapshot:
    - run: `created|running|partial_success|success|failed|cancelled`
    - reviewer: `pending|running|succeeded|failed|skipped|timeout`
- `events.jsonl`
  - append-only event stream (`reviewer_started`, `reviewer_failed`, `retry_scheduled`, `synthesizer_completed`, etc.)
- `reviewer_outcomes.jsonl`
  - one record per reviewer attempt/outcome with error class and artifact hashes
- `summary.json`
  - machine-readable verdict and finding counts
- `summary.md`
  - rendered human view from `summary.json` + finding records

Keep per-reviewer markdown outputs for continuity:
- `assumption-hunter.md`, `edge-case-prober.md`, etc.

### 3) Deterministic controls

- Fixed reviewer order in config.
- Stable JSON serialization (sorted keys, normalized newlines).
- Deterministic summary rendering sort order.
- Prompt snapshot hashing (agent file hash in manifest).
- `--resume` replays only unfinished/retryable reviewer tasks from persisted state.

### 4) Failure and retry policy (default)

Reviewer policy defaults:

- Non-web reviewers:
  - timeout: 120s
  - retry: 1 retry only for transient runtime/tool failures
- `prior-art-scout` (web-dependent):
  - if web unavailable: `skipped_capability` (no retry)
  - if transient runtime failure and web available: 1 retry

Run completion policy:

- `success`: all mandatory reviewers succeeded, synthesizer succeeded.
- `partial_success`: mandatory reviewers succeeded, optional reviewer(s) skipped/failed, synthesizer succeeded with partial flag.
- `failed`: mandatory reviewer quorum not met or synthesizer failed.

Mandatory set for design stage:
- assumption-hunter
- edge-case-prober
- requirement-auditor
- feasibility-skeptic
- first-principles

Optional by default:
- prior-art-scout

## Canonical Artifact Format Decision

**Decision:** JSON/JSONL canonical, markdown rendered.

Rationale:
- explicit replay/debug mechanics
- robust diffing and eval comparison across runtimes
- avoids parsing markdown as primary machine interface
- preserves existing markdown UX for human review

## Reproducibility for Cross-Runtime Eval

Define reproducibility as **process-deterministic + artifact-deterministic**, not byte-identical model text.

Minimum reproducibility contract:
- same manifest hash => same run plan (reviewers/order/policies)
- same state transitions allowed from same failures
- same output schema and rendering rules
- differences in model wording captured as content variance, not orchestration variance

This is enough to compare runtime behavior cleanly (Codex vs Claude) without pretending LLM text generation is fully deterministic.

## LangGraph Adoption Trigger (Revisit Gate)

Adopt LangGraph when at least two are true:

1. We automate 2+ pipeline stages end-to-end.
2. We need durable pause/resume with human checkpoints beyond a single local process.
3. We run concurrent reviews where queueing/scheduling reliability matters.
4. Local runner state logic grows into complex branching that is harder to reason about than a graph.

Until then, local runner + strict contracts is the highest-leverage next step.

## Assumptions and Unknowns

Assumptions:
- Immediate goal is deterministic orchestration, not broad platformization.
- Existing reviewer prompts remain markdown-first in near term.

Unknowns:
- Whether reviewers should emit structured finding JSON directly vs synthesizer conversion.
- How much cross-runtime variance appears once deterministic orchestration is in place.

### SUPPLEMENTAL CONTEXT (/Users/nic/src/design-parallax/parallax/docs/codex/2026-02-16-deterministic-runner-plan.md)
# Deterministic Runner Implementation Plan

**Date:** 2026-02-16  
**Goal:** Add a deterministic, replayable design-stage runner with explicit partial-failure handling, while preserving current `skills/review` behavior.

## Increment 0: Lock Contracts Before Code

1. Define and commit artifact schemas:
   - `docs/specs/review-run-manifest.schema.json`
   - `docs/specs/review-run-state.schema.json`
   - `docs/specs/review-event.schema.json`
   - `docs/specs/reviewer-outcome.schema.json`
2. Add deterministic status enums and error classes (`transient`, `timeout`, `capability_unavailable`, `fatal`).
3. Verification:
   - Schema files exist and include required fields for `run_id`, reviewer status, and retry metadata.

## Increment 1: Manifest Builder (No Execution Yet)

1. Add `scripts/review_runner.py` with a `plan` mode:
   - reads input args
   - snapshots file hashes (design/requirements/agents)
   - computes deterministic `run_id`
   - writes `run_manifest.json`
2. Add fixture inputs under `tests/fixtures/review_runner/`.
3. Verification:
   - Running `plan` twice with identical inputs produces identical manifest content except runtime timestamps.

## Increment 2: State Store + Event Log

1. Implement `run_state.json` writer with atomic writes.
2. Implement append-only `events.jsonl`.
3. Add state transitions for run + reviewer lifecycle.
4. Verification:
   - Simulated run transitions follow allowed state graph only.
   - Event log contains ordered lifecycle events.

## Increment 3: Reviewer Config + Ordering

1. Add stage config (`config/review_runner.yaml`) with:
   - reviewer list in canonical order
   - mandatory/optional reviewer flags
   - timeout/retry defaults
2. Add validation that `design` stage reviewer set matches current skill behavior.
3. Verification:
   - Config load fails fast on unknown reviewer names or duplicate order entries.

## Increment 4: Runtime Adapter Interface

1. Define adapter contract (`run_reviewer(reviewer, manifest, attempt) -> outcome`).
2. Implement runtime adapters:
   - `claude_adapter.py`
   - `codex_adapter.py`
   - `codex_app_handoff_adapter.py` (task-pack + ingest mode for GUI-only flow)
3. Add runtime capability detection:
   - CLI available (`codex`/`claude` in PATH or configured binary)
   - non-interactive mode support
   - fallback to handoff adapter when CLI automation is unavailable
4. Start with a fake adapter for deterministic tests.
5. Verification:
   - Fake adapter run produces deterministic `reviewer_outcomes.jsonl` records.
   - Capability probe selects expected adapter in all three cases:
     - Claude Code CLI available
     - Codex CLI available
     - Codex App only (handoff fallback)

## Increment 5: Failure Policy Engine

1. Implement retry decision logic:
   - one retry for transient failures
   - no retry for capability unavailability (no web)
2. Encode `prior-art-scout` policy override.
3. Verification:
   - Unit tests for failure classification and retry scheduling.

## Increment 6: Execution + Resume

1. Add `run` command to execute reviewers and persist outcomes.
2. Add `--resume <run_id>` to continue `pending|retryable` reviewers only.
3. Ensure crash-safe resume from state files.
4. Verification:
   - Kill run mid-flight, resume, and confirm completed reviewers are not rerun.

## Increment 7: Synthesizer Integration

1. Execute synthesizer only after reviewer phase reaches terminal state (`success` or `partial_success`).
2. Persist `summary.json` machine output and render `summary.md`.
3. Include explicit partial-run notice when optional reviewer(s) failed/skipped.
4. Verification:
   - Partial run with missing `prior-art-scout` still produces summary with `partial` marker.

## Increment 8: Canonical Finding Records

1. Add `findings.jsonl` as source-of-truth output for synthesized findings.
2. Render markdown summary from `findings.jsonl` + verdict metadata.
3. Keep existing reviewer markdown files unchanged.
4. Verification:
   - Markdown regeneration from JSONL is deterministic (same input => same rendered ordering).

## Increment 9: Reproducibility and Diff Utilities

1. Add `scripts/compare_runs.py`:
   - compares manifests, state outcomes, finding counts, verdicts across two run IDs
2. Add a portability smoke command for same artifact on Codex vs Claude runtimes.
3. Verification:
   - Comparison output separates orchestration drift from model-content drift.

## Increment 10: CI Safety Nets

1. Add automated checks for:
   - schema validity
   - deterministic serialization
   - resume behavior
2. Add one golden-run fixture for deterministic regression detection.
3. Verification:
   - CI fails on contract-breaking schema or nondeterministic render order.

## Increment 11: Docs and Adoption

1. Update `skills/review/SKILL.md` with runner invocation contract and artifact locations.
2. Add operator doc:
   - `docs/runbooks/review-runner.md` (run, resume, retry, partial interpretation)
3. Document LangGraph adoption trigger criteria in runbook.
4. Verification:
   - New runbook can be followed end-to-end in a clean workspace.

## Default Execution Policy (to implement)

- Runtime selection:
  - `--runtime claude` -> `claude_adapter.py`
  - `--runtime codex` -> `codex_adapter.py` when CLI available
  - `--runtime codex` + no CLI -> `codex_app_handoff_adapter.py` with explicit `manual_handoff` status
- Timeout: `120s` per reviewer attempt.
- Retry: `1` retry for transient errors only.
- Capability unavailable (no web): skip `prior-art-scout` with explicit status.
- Terminal outcomes:
  - `success`: all mandatory reviewers + synthesizer succeeded.
  - `partial_success`: mandatory succeeded, optional reviewer failed/skipped, synthesizer succeeded.
  - `failed`: mandatory quorum not met or synthesizer failed.
  - `waiting_for_handoff`: run paused pending user-provided artifacts from Codex App flow.

## Rollout Sequence (Small, Testable)

1. Land contracts and manifest first.
2. Land state/events second.
3. Land fake-adapter deterministic tests third.
4. Land real adapters (Claude CLI, Codex CLI, Codex App handoff) + retries fourth.
5. Land synthesizer + JSONL canonical output fifth.
6. Land resume and comparison tooling sixth.
7. Update docs last.

This sequence keeps every step shippable and verifiable without requiring immediate multi-stage orchestration changes.

mcp startup: no servers
2026-02-16T04:26:54.029191Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c64ad-1495-7d03-a981-14e2591f922d
2026-02-16T04:26:54.029229Z  WARN codex_core::state_db: state db record_discrepancy: find_thread_path_by_id_str_in_subdir, falling_back
2026-02-16T04:26:54.049235Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c647c-7341-77e1-8065-26150a1e84c6
2026-02-16T04:26:54.049259Z  WARN codex_core::state_db: state db record_discrepancy: find_thread_path_by_id_str_in_subdir, falling_back
2026-02-16T04:26:54.078979Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c6457-f013-76d3-9ed3-180d250b58ea
2026-02-16T04:26:54.079032Z  WARN codex_core::state_db: state db record_discrepancy: find_thread_path_by_id_str_in_subdir, falling_back

thinking
**Evaluating requirement-design gaps**
codex
# Requirement Auditor Review

## Coverage Matrix
| Requirement | Addressed? | Design Section | Notes |
|---|---|---|---|
| In scope: design-stage `parallax:review` only | Yes | Overview, Inputs | Scope is explicitly limited to design stage. |
| In scope: cross-runtime support (Codex CLI, Codex GUI handoff, Claude) | Yes | Adapter Layer | `claude_adapter`, `codex_adapter`, `codex_app_handoff_adapter` are defined. |
| In scope: deterministic identity, ordering, artifact layout | Yes | Determinism Controls, Outputs | `run_id`, stable ordering, stable serialization, fixed run directory are defined. |
| In scope: explicit partial-failure handling (including no-web) | Yes | Partial Failure Policy, Failure Semantics | `prior-art-scout` optional + capability skip policy is explicit. |
| In scope: markdown-first comparison this increment | Yes | Overview, Outputs | `summary.md` is canonical now; JSON outputs are deferred. |
| Constraint: keep reviewer/synthesizer behavior intact | Yes | Overview, Migration Strategy | Design keeps current workflow semantics and prompt hashing. |
| Constraint: preserve markdown outputs for human review | Yes | Outputs | Reviewer markdown + `summary.md` are preserved. |
| Constraint: incremental change, no big-bang rewrite | Yes | Migration Strategy | Phased approach with deferred structured canonical outputs. |
| Must: deterministic `run_id` from canonical manifest | Yes | Determinism Controls | `run_id` from canonical manifest hash excluding timestamps. |
| Must: topic-agnostic `content_fingerprint` | Yes | Determinism Controls | Fingerprint is content-only and topic-agnostic. |
| Must: snapshot hashes for design/requirements/prompts/config | Yes | Inputs, Data Model (`run_manifest.json`) | Inputs hashes, prompt hashes, and `config_hash` are present. |
| Must: explicit run+reviewer state machine with guards | Yes | Failure Semantics, Testing Strategy | Transition rules are explicit; invalid transitions are tested. |
| Must: structured `events.jsonl` + `reviewer_outcomes.jsonl` | Yes | Outputs, Data Model | Both logs are defined in run artifacts. |
| Must: outcomes include effective runtime settings per attempt | Yes | Data Model (`reviewer_outcomes.jsonl`) | `effective_runtime_settings` is required per attempt. |
| Must: resume avoids rerunning completed reviewers | Yes | Runner Core (`resume`), Testing Strategy | Resume targets non-terminal work only; test covers rerun prevention. |
| Must: atomic summary publish | Yes | Atomic Synthesis | Temp write + validation + atomic rename are explicit. |
| Must: terminal run statuses include `success|partial_success|failed` | Yes | Run-Level Terminal States | Required statuses are present. |
| Must: no-web policy skips prior-art + confidence downgrade in summary | Yes | Partial Failure Policy | Capability skip and confidence downgrade are explicit. |
| Must: bounded transient retry with backoff+jitter | Yes | Retry Policy | Attempts, backoff formula, jitter, and no-retry classes are explicit. |
| Must: collision policy prevents clobbering | Yes | Runner Core (collision policy) | Fail-fast default + explicit replay path is defined. |
| Should: unified runtime adapter abstraction | Yes | Adapter Layer | Single contract for reviewer execution across runtimes. |
| Should: semantic comparison metadata section in `summary.md` (completion map, capability constraints, runtime caveats) | Partial | Overview, Partial Failure Policy | Design references semantic comparison and confidence notes, but no explicit required metadata schema/section for all three elements. |
| Should: 2 consecutive prior-art skips trigger escalation note | Yes | Partial Failure Policy | Threshold rule and escalation note are explicit. |
| Could: `summary.json` + `findings.jsonl` when ready | Yes | Outputs (Deferred), Migration Strategy | Explicitly deferred to later phase. |
| Could: run comparison utility | Partial | Migration Strategy | Semantic comparison intent exists, but no concrete utility contract in this design. |
| Could: manual handoff adapter | Yes | Adapter Layer | `codex_app_handoff_adapter` and ingest validation are defined. |
| Could: per-reviewer override policy in config | Yes | Config Layer | Timeout/retry/runtime overrides are explicit. |
| Won’t: full LangGraph migration this iteration | Yes | Migration Strategy | LangGraph is deferred behind an adoption gate. |
| Won’t: evaluator/judge automation this iteration | Yes | Scope/Design coverage | Not introduced. |
| Won’t: JSON/JSONL canonical this iteration | Yes | Overview, Outputs | Markdown remains canonical for this increment. |
| NFR reproducibility: same manifest => same plan/order | Yes | Determinism Controls, Testing Strategy | Deterministic plan and ordering are explicitly tested. |
| NFR reliability: deterministic non-retryable capability handling | Yes | Retry Policy, Partial Failure Policy | Capability skip is explicit and non-retryable. |
| NFR observability: attempt + terminal outcome logs with timestamps and error class | Partial | Data Model, Failure Semantics | `error_class` is explicit; attempt-level timestamp fields are not explicitly specified in `reviewer_outcomes.jsonl`. |
| NFR portability: same contract across Codex/Claude | Yes | Adapter Layer, Cross-runtime tests | Shared contract and cross-runtime contract tests are defined. |
| NFR comparability: semantic comparison via `summary.md` + semantic metadata | Partial | Overview, Partial Failure Policy | `summary.md` is canonical; required semantic metadata contract is underspecified. |
| AC8: outputs remain compatible with current workflow expectations | Partial | Outputs path choice | New root `docs/codex/reviews/...` risks breaking existing `docs/reviews/...`-based workflow assumptions unless compatibility shims are defined. |

## Findings

### Finding 1: Requirement Baseline Is Contradictory Across Input Artifacts
- **Severity:** Critical
- **Phase:** calibrate (primary), design (contributing)
- **Section:** Canonical output format expectations across requirements vs supplemental recommendation/plan
- **Issue:** The requirements artifact sets markdown-first canonical output for this increment and explicitly excludes JSON/JSONL as canonical now, while supplemental recommendation/plan prescribe `summary.json`/`findings.jsonl` as source-of-truth in near-term implementation.
- **Why it matters:** The runner cannot satisfy both baselines simultaneously. This creates churn in artifact contracts, breaks eval continuity, and invalidates acceptance decisions because “done” changes depending on which document is treated as authority.
- **Suggestion:** Freeze one authoritative increment boundary in calibration: either (a) markdown-canonical now with JSON deferred, or (b) JSON-canonical now. Then update all companion docs to that single contract before implementation.

### Finding 2: Observability Contract Misses Explicit Attempt Timestamps
- **Severity:** Critical
- **Phase:** design (primary), plan (contributing)
- **Section:** Data Model (`reviewer_outcomes.jsonl`), Non-Functional observability requirements
- **Issue:** Requirement requires each reviewer attempt and terminal outcome to be logged with timestamps and error class. Design defines `error_class` and durations but does not explicitly require attempt timestamp fields in `reviewer_outcomes.jsonl`.
- **Why it matters:** Missing attempt timestamps blocks deterministic incident reconstruction, retry auditability, and cross-runtime sequence analysis. Blast radius is every failed or retried run where timing evidence is needed.
- **Suggestion:** Add explicit timestamp fields (`attempt_started_at`, `attempt_ended_at`, `recorded_at`) to `reviewer_outcomes.jsonl` and require timestamped terminal transition events in `events.jsonl`.

### Finding 3: Semantic Comparison Metadata Is Underspecified
- **Severity:** Important
- **Phase:** design (primary), plan (contributing)
- **Section:** Overview, Partial Failure Policy, Testing Strategy
- **Issue:** Should-have requirement calls for semantic comparison metadata in `summary.md` including reviewer completion map, capability constraints, and runtime caveats. Design references semantic comparison and partial markers but does not define a mandatory metadata section schema covering all required elements.
- **Why it matters:** Cross-runtime comparisons will mix orchestration variance with model-content variance. Blast radius is every parity analysis and drift triage run.
- **Suggestion:** Define a fixed `summary.md` metadata block with required keys and deterministic ordering, then add acceptance tests validating presence and render stability.

### Finding 4: Output Location Change Risks Workflow Compatibility
- **Severity:** Important
- **Phase:** design (primary), plan (contributing)
- **Section:** Outputs (`docs/codex/reviews/<topic>/runs/<run_id>/`)
- **Issue:** Acceptance requires compatibility with current workflow expectations, but design relocates outputs under `docs/codex/reviews/...` without a compatibility contract for existing `docs/reviews/...` consumers.
- **Why it matters:** Existing tooling, scripts, and operator habits can fail immediately. Blast radius includes all downstream readers expecting legacy paths.
- **Suggestion:** Add an explicit compatibility strategy: either keep legacy root for this increment, or generate deterministic pointers/symlinks/index files that preserve existing consumption paths.

## Blind Spot Check (optional — being empirically validated)
Requirement-focused review does not deeply validate failure-state correctness under concurrent execution, adapter implementation realism, or determinism under filesystem/process nondeterminism. Edge-case, feasibility, and first-principles lanes should pressure-test those areas.
tokens used
14,998
# Requirement Auditor Review

## Coverage Matrix
| Requirement | Addressed? | Design Section | Notes |
|---|---|---|---|
| In scope: design-stage `parallax:review` only | Yes | Overview, Inputs | Scope is explicitly limited to design stage. |
| In scope: cross-runtime support (Codex CLI, Codex GUI handoff, Claude) | Yes | Adapter Layer | `claude_adapter`, `codex_adapter`, `codex_app_handoff_adapter` are defined. |
| In scope: deterministic identity, ordering, artifact layout | Yes | Determinism Controls, Outputs | `run_id`, stable ordering, stable serialization, fixed run directory are defined. |
| In scope: explicit partial-failure handling (including no-web) | Yes | Partial Failure Policy, Failure Semantics | `prior-art-scout` optional + capability skip policy is explicit. |
| In scope: markdown-first comparison this increment | Yes | Overview, Outputs | `summary.md` is canonical now; JSON outputs are deferred. |
| Constraint: keep reviewer/synthesizer behavior intact | Yes | Overview, Migration Strategy | Design keeps current workflow semantics and prompt hashing. |
| Constraint: preserve markdown outputs for human review | Yes | Outputs | Reviewer markdown + `summary.md` are preserved. |
| Constraint: incremental change, no big-bang rewrite | Yes | Migration Strategy | Phased approach with deferred structured canonical outputs. |
| Must: deterministic `run_id` from canonical manifest | Yes | Determinism Controls | `run_id` from canonical manifest hash excluding timestamps. |
| Must: topic-agnostic `content_fingerprint` | Yes | Determinism Controls | Fingerprint is content-only and topic-agnostic. |
| Must: snapshot hashes for design/requirements/prompts/config | Yes | Inputs, Data Model (`run_manifest.json`) | Inputs hashes, prompt hashes, and `config_hash` are present. |
| Must: explicit run+reviewer state machine with guards | Yes | Failure Semantics, Testing Strategy | Transition rules are explicit; invalid transitions are tested. |
| Must: structured `events.jsonl` + `reviewer_outcomes.jsonl` | Yes | Outputs, Data Model | Both logs are defined in run artifacts. |
| Must: outcomes include effective runtime settings per attempt | Yes | Data Model (`reviewer_outcomes.jsonl`) | `effective_runtime_settings` is required per attempt. |
| Must: resume avoids rerunning completed reviewers | Yes | Runner Core (`resume`), Testing Strategy | Resume targets non-terminal work only; test covers rerun prevention. |
| Must: atomic summary publish | Yes | Atomic Synthesis | Temp write + validation + atomic rename are explicit. |
| Must: terminal run statuses include `success|partial_success|failed` | Yes | Run-Level Terminal States | Required statuses are present. |
| Must: no-web policy skips prior-art + confidence downgrade in summary | Yes | Partial Failure Policy | Capability skip and confidence downgrade are explicit. |
| Must: bounded transient retry with backoff+jitter | Yes | Retry Policy | Attempts, backoff formula, jitter, and no-retry classes are explicit. |
| Must: collision policy prevents clobbering | Yes | Runner Core (collision policy) | Fail-fast default + explicit replay path is defined. |
| Should: unified runtime adapter abstraction | Yes | Adapter Layer | Single contract for reviewer execution across runtimes. |
| Should: semantic comparison metadata section in `summary.md` (completion map, capability constraints, runtime caveats) | Partial | Overview, Partial Failure Policy | Design references semantic comparison and confidence notes, but no explicit required metadata schema/section for all three elements. |
| Should: 2 consecutive prior-art skips trigger escalation note | Yes | Partial Failure Policy | Threshold rule and escalation note are explicit. |
| Could: `summary.json` + `findings.jsonl` when ready | Yes | Outputs (Deferred), Migration Strategy | Explicitly deferred to later phase. |
| Could: run comparison utility | Partial | Migration Strategy | Semantic comparison intent exists, but no concrete utility contract in this design. |
| Could: manual handoff adapter | Yes | Adapter Layer | `codex_app_handoff_adapter` and ingest validation are defined. |
| Could: per-reviewer override policy in config | Yes | Config Layer | Timeout/retry/runtime overrides are explicit. |
| Won’t: full LangGraph migration this iteration | Yes | Migration Strategy | LangGraph is deferred behind an adoption gate. |
| Won’t: evaluator/judge automation this iteration | Yes | Scope/Design coverage | Not introduced. |
| Won’t: JSON/JSONL canonical this iteration | Yes | Overview, Outputs | Markdown remains canonical for this increment. |
| NFR reproducibility: same manifest => same plan/order | Yes | Determinism Controls, Testing Strategy | Deterministic plan and ordering are explicitly tested. |
| NFR reliability: deterministic non-retryable capability handling | Yes | Retry Policy, Partial Failure Policy | Capability skip is explicit and non-retryable. |
| NFR observability: attempt + terminal outcome logs with timestamps and error class | Partial | Data Model, Failure Semantics | `error_class` is explicit; attempt-level timestamp fields are not explicitly specified in `reviewer_outcomes.jsonl`. |
| NFR portability: same contract across Codex/Claude | Yes | Adapter Layer, Cross-runtime tests | Shared contract and cross-runtime contract tests are defined. |
| NFR comparability: semantic comparison via `summary.md` + semantic metadata | Partial | Overview, Partial Failure Policy | `summary.md` is canonical; required semantic metadata contract is underspecified. |
| AC8: outputs remain compatible with current workflow expectations | Partial | Outputs path choice | New root `docs/codex/reviews/...` risks breaking existing `docs/reviews/...`-based workflow assumptions unless compatibility shims are defined. |

## Findings

### Finding 1: Requirement Baseline Is Contradictory Across Input Artifacts
- **Severity:** Critical
- **Phase:** calibrate (primary), design (contributing)
- **Section:** Canonical output format expectations across requirements vs supplemental recommendation/plan
- **Issue:** The requirements artifact sets markdown-first canonical output for this increment and explicitly excludes JSON/JSONL as canonical now, while supplemental recommendation/plan prescribe `summary.json`/`findings.jsonl` as source-of-truth in near-term implementation.
- **Why it matters:** The runner cannot satisfy both baselines simultaneously. This creates churn in artifact contracts, breaks eval continuity, and invalidates acceptance decisions because “done” changes depending on which document is treated as authority.
- **Suggestion:** Freeze one authoritative increment boundary in calibration: either (a) markdown-canonical now with JSON deferred, or (b) JSON-canonical now. Then update all companion docs to that single contract before implementation.

### Finding 2: Observability Contract Misses Explicit Attempt Timestamps
- **Severity:** Critical
- **Phase:** design (primary), plan (contributing)
- **Section:** Data Model (`reviewer_outcomes.jsonl`), Non-Functional observability requirements
- **Issue:** Requirement requires each reviewer attempt and terminal outcome to be logged with timestamps and error class. Design defines `error_class` and durations but does not explicitly require attempt timestamp fields in `reviewer_outcomes.jsonl`.
- **Why it matters:** Missing attempt timestamps blocks deterministic incident reconstruction, retry auditability, and cross-runtime sequence analysis. Blast radius is every failed or retried run where timing evidence is needed.
- **Suggestion:** Add explicit timestamp fields (`attempt_started_at`, `attempt_ended_at`, `recorded_at`) to `reviewer_outcomes.jsonl` and require timestamped terminal transition events in `events.jsonl`.

### Finding 3: Semantic Comparison Metadata Is Underspecified
- **Severity:** Important
- **Phase:** design (primary), plan (contributing)
- **Section:** Overview, Partial Failure Policy, Testing Strategy
- **Issue:** Should-have requirement calls for semantic comparison metadata in `summary.md` including reviewer completion map, capability constraints, and runtime caveats. Design references semantic comparison and partial markers but does not define a mandatory metadata section schema covering all required elements.
- **Why it matters:** Cross-runtime comparisons will mix orchestration variance with model-content variance. Blast radius is every parity analysis and drift triage run.
- **Suggestion:** Define a fixed `summary.md` metadata block with required keys and deterministic ordering, then add acceptance tests validating presence and render stability.

### Finding 4: Output Location Change Risks Workflow Compatibility
- **Severity:** Important
- **Phase:** design (primary), plan (contributing)
- **Section:** Outputs (`docs/codex/reviews/<topic>/runs/<run_id>/`)
- **Issue:** Acceptance requires compatibility with current workflow expectations, but design relocates outputs under `docs/codex/reviews/...` without a compatibility contract for existing `docs/reviews/...` consumers.
- **Why it matters:** Existing tooling, scripts, and operator habits can fail immediately. Blast radius includes all downstream readers expecting legacy paths.
- **Suggestion:** Add an explicit compatibility strategy: either keep legacy root for this increment, or generate deterministic pointers/symlinks/index files that preserve existing consumption paths.

## Blind Spot Check (optional — being empirically validated)
Requirement-focused review does not deeply validate failure-state correctness under concurrent execution, adapter implementation realism, or determinism under filesystem/process nondeterminism. Edge-case, feasibility, and first-principles lanes should pressure-test those areas.
