{"type": "finding", "id": "v1-assumption-hunter-001", "title": "Claude Code API availability assumed without fallback", "severity": "Critical", "phase": {"primary": "design", "contributing": "calibrate"}, "section": "Reviewer Capabilities", "issue": "Design assumes Claude Code API supports multi-agent dispatch with specific tool access controls (Read/WebFetch). No validation that these capabilities exist in production API, no fallback if they don't.", "why_it_matters": "Entire skill architecture depends on being able to dispatch parallel agents with restricted tool access. If the API doesn't support this or changes, the skill cannot be built as designed.", "suggestion": "Validate Claude Code API capabilities before finalizing design. Document API version requirements as explicit constraint. Define fallback approach if API limitations discovered (e.g., sequential execution, external orchestration)."}
{"type": "finding", "id": "v1-assumption-hunter-002", "title": "Reviewer agent isolation mechanism unspecified", "severity": "Critical", "phase": {"primary": "design", "contributing": null}, "section": "Reviewer Personas", "issue": "FR1.3 requires reviewers operate independently without cross-contamination, but design doesn't specify HOW isolation is achieved. Are these separate Claude sessions? Separate API calls? Thread isolation? Process isolation?", "why_it_matters": "Without knowing the isolation mechanism, cannot validate that FR1.3 is actually achievable. Different mechanisms have different failure modes (shared context leakage, rate limiting, cost implications).", "suggestion": "Specify the technical mechanism for reviewer isolation. Document whether this is native Claude Code behavior or requires custom orchestration. Add constraint about execution environment capabilities."}
{"type": "finding", "id": "v1-assumption-hunter-003", "title": "WebFetch tool assumed available without validation", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Reviewer Capabilities", "issue": "Design mentions reviewers can access public URLs via 'WebFetch' tool (line 252, C1.3), but this tool isn't documented in CLAUDE.md available tools section. Assumption that it exists in Claude Code.", "why_it_matters": "If WebFetch doesn't exist or has restrictions (rate limits, authentication, content-type support), public URL access for design docs won't work. C1.3 explicitly scopes this as MVP capability.", "suggestion": "Validate WebFetch tool exists in Claude Code and document its limitations. If it doesn't exist, either remove public URL support from MVP or specify how to implement it (curl via bash?)."}
{"type": "finding", "id": "v1-assumption-hunter-004", "title": "Pattern extraction assumed to complete within session context", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Cross-Iteration Finding Tracking", "issue": "FR10 runs pattern extraction 'in critical path after synthesis, before finding processing.' For 87 findings (v3 example), this involves LLM semantic analysis. No timeout specified, no handling for context exhaustion if findings + patterns exceed session limits.", "why_it_matters": "Pattern extraction could timeout or fail due to context limits, blocking the entire review from completing. User gets no findings output if synthesis succeeds but pattern extraction fails.", "suggestion": "Define timeout for pattern extraction step. Specify fallback behavior if pattern extraction fails (proceed with findings, skip patterns? abort review?). Consider making pattern extraction optional for >50 finding threshold."}
{"type": "finding", "id": "v1-assumption-hunter-005", "title": "Git repository assumed to be in clean state", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Output Artifacts", "issue": "Design commits review artifacts to git (FR5.3) and uses git diff for changed sections (FR8.4), but doesn't specify what happens if working directory is dirty, detached HEAD, merge conflict, or no git repo exists.", "why_it_matters": "Git operations could fail silently or corrupt state if assumptions about repo cleanliness violated. Design says 'git repo exists AND design doc has prior committed version' but doesn't handle cases where repo exists but is in broken state.", "suggestion": "Add explicit git state validation before review: check for clean working directory, valid HEAD, no merge conflicts. Define behavior for non-git workflows (skip diff, still save artifacts). Document as constraint in C1."}
{"type": "finding", "id": "v1-assumption-hunter-006", "title": "Synthesizer agent capability assumptions unstated", "severity": "Critical", "phase": {"primary": "design", "contributing": null}, "section": "Synthesis", "issue": "Synthesizer must deduplicate findings, classify phases, surface contradictions, resolve severity ranges—all requiring semantic understanding of 100+ findings. Design assumes Claude can do this reliably in a single call without hallucination, without dropping findings, without context overflow.", "why_it_matters": "If synthesizer fails or produces incorrect consolidation, entire review output is untrustworthy. NFR1.2 says 'synthesizer processes 100+ findings without timeout' but doesn't validate this is possible. v3 had 83 findings—what if next review has 200?", "suggestion": "Validate synthesizer capacity with realistic test (process 100+ findings from v3 review). Define maximum finding count the synthesizer can handle. Specify fallback for reviews exceeding capacity (batch synthesis? user-driven consolidation?)."}
{"type": "finding", "id": "v1-assumption-hunter-007", "title": "Prompt caching effectiveness assumed without empirical validation", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Reviewer Prompt Architecture", "issue": "FR8.1 and NFR2.3 assume prompt caching achieves 90% cost reduction. This is a vendor-provided theoretical number, not validated for this specific use case (reviewer prompts, synthesis prompts, pattern extraction).", "why_it_matters": "Budget projections depend on 90% savings assumption. If cache hit rates are lower in practice (due to prompt evolution, cache eviction, session boundaries), costs could be 10x higher than projected. $1/review becomes $10/review, blowing monthly budget.", "suggestion": "Add NFR requirement to measure actual cache hit rates in eval framework. Budget for worst case (no caching) in early prototyping. Document cache effectiveness as assumption needing validation, not fact."}
{"type": "finding", "id": "v1-assumption-hunter-008", "title": "Parallel agent success threshold arbitrary", "severity": "Minor", "phase": {"primary": "design", "contributing": null}, "section": "Parallel Agent Failure Handling", "issue": "FR7.1 states 'proceed if minimum threshold met (4/6 agents)' with no rationale for 4/6. Why not 3/6? 5/6? How does this interact with persona diversity requirement (FR1.2: <30% overlap)?", "why_it_matters": "If 4/6 is arbitrary, could be wrong threshold. If Prior Art Scout and First Principles both fail (2/6), review misses entire classes of findings. Design doesn't analyze which combinations of reviewer failures are tolerable.", "suggestion": "Justify 4/6 threshold with rationale or mark as 'initial hypothesis to validate via eval.' Consider whether certain reviewer combinations are critical (e.g., Requirement Auditor + Assumption Hunter must both succeed)."}
{"type": "finding", "id": "v1-assumption-hunter-009", "title": "Interactive finding processing assumes synchronous human availability", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "UX Flow", "issue": "Step 6 presents findings 'one at a time' for accept/reject. Design assumes human is available for entire session to process potentially 50 findings. What if human needs to pause, switch context, resume later?", "why_it_matters": "FR4.2 claims 'async-first workflow' but interactive processing is synchronous by design. If human must leave mid-session, no way to resume from finding #23. Contradiction between async-first claim and interactive implementation.", "suggestion": "Either make interactive processing truly resumable (save progress, restart from checkpoint) or acknowledge it's synchronous within a session. Clarify 'async-first' means artifacts to disk, NOT interactive flow is resumable."}
{"type": "finding", "id": "v1-assumption-hunter-010", "title": "JSONL schema validation assumed to catch all malformed outputs", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Parallel Agent Failure Handling", "issue": "FR7.5 says 'validate reviewer JSONL output before synthesis (schema check, retry on malformed JSON)' but schema validation only catches structural errors. What if reviewer outputs valid JSONL with semantically wrong content (empty findings, nonsense text, hallucinated sections)?", "why_it_matters": "Schema validation gives false confidence. Synthesizer could receive valid JSONL that's garbage data. Design assumes schema validation = quality check, but it's only format check.", "suggestion": "Add semantic validation checks: minimum finding count (non-zero), required fields non-empty, severity values match enum. Define separate handling for 'valid format, invalid content' vs 'invalid format.'"}
{"type": "finding", "id": "v1-assumption-hunter-011", "title": "Reviewer token budget unspecified", "severity": "Important", "phase": {"primary": "plan", "contributing": "design"}, "section": "Reviewer Capabilities", "issue": "Reviewers read design/requirements docs via Read tool (FR8.3). No limit specified on document size. What if design doc is 50k tokens? 100k tokens? Does each reviewer get full document in context?", "why_it_matters": "Large design docs could exhaust reviewer context windows or make review prohibitively expensive. NFR2.1 assumes <$1/review but doesn't account for document size variability. 6 reviewers reading 50k token doc = 300k input tokens = $1+ on input alone.", "suggestion": "Define maximum design doc size for MVP (e.g., 10k tokens). Add validation that rejects oversized docs with clear error. Consider chunking strategy for large docs in post-MVP."}
{"type": "finding", "id": "v1-assumption-hunter-012", "title": "Severity classification assumed to be consistent across reviewers", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Synthesis", "issue": "Design acknowledges severity ranges when reviewers disagree (FR2.4) but assumes this is edge case. What if reviewers systematically rate differently (Assumption Hunter always rates Critical, Feasibility Skeptic always rates Minor)?", "why_it_matters": "FR3.4 uses highest severity for verdict (conservative), but if reviewers are systematically miscalibrated, verdict logic produces false escalations. 'Any Critical finding → revise' becomes meaningless if 50% of findings have Critical in range.", "suggestion": "Add calibration requirement for severity consistency across personas. Track per-reviewer severity distributions in eval framework. Flag reviewers with >2 standard deviation variance for prompt tuning."}
{"type": "finding", "id": "v1-assumption-hunter-013", "title": "Auto-fix deferred but design still references it", "severity": "Minor", "phase": {"primary": "design", "contributing": "calibrate"}, "section": "UX Flow", "issue": "Step 4 describes auto-fix workflow in detail (auto-fixable findings, separate commit, re-review). But Q2 resolution in requirements says 'defer entirely to post-MVP.' Design doc not synced with requirements decision.", "why_it_matters": "Confuses implementers about what to build. If auto-fix is deferred, Step 4 should be removed or clearly marked as future work. Design doc says v4 'synced with requirements v1.2' but this section wasn't updated.", "suggestion": "Remove Step 4 entirely or wrap it in [FUTURE] marker with note 'deferred to post-MVP per Q2 resolution.' Update version to v4.1 to reflect sync."}
{"type": "finding", "id": "v1-assumption-hunter-014", "title": "Pattern extraction cap of 15 patterns arbitrary", "severity": "Minor", "phase": {"primary": "design", "contributing": null}, "section": "Cross-Iteration Finding Tracking", "issue": "FR10.1 caps patterns at 15 per review with label 'sanity limit.' No rationale for why 15 vs 10 or 20. Prototype extracted 12 patterns from 87 findings—what if next review has 200 findings and natural grouping yields 25 patterns?", "why_it_matters": "Arbitrary cap could force artificial pattern merging, losing signal. Or cap could be too high, allowing noisy pattern proliferation. Cap should be justified by human processing capacity or LLM synthesis limits.", "suggestion": "Justify 15 pattern cap with rationale (human can process N patterns, or synthesis quality degrades beyond N) or mark as hypothesis to validate. Consider dynamic cap based on finding count."}
{"type": "finding", "id": "v1-assumption-hunter-015", "title": "Timestamped folder collision handling unspecified", "severity": "Minor", "phase": {"primary": "plan", "contributing": null}, "section": "Output Artifacts", "issue": "FR6.5 says 'timestamped folders when re-running reviews of an existing topic' but doesn't specify timestamp granularity. If user runs two reviews within same second, do they collide? Does timestamp include milliseconds?", "why_it_matters": "Low probability but non-zero. Rapid iteration testing (run, fix, run) could trigger collision. Design doesn't specify error handling.", "suggestion": "Specify timestamp format (ISO 8601 with milliseconds) or use sequence numbers (topic-v1, topic-v2). Add collision detection with clear error message."}
{"type": "finding", "id": "v1-assumption-hunter-016", "title": "Clean reviews assumption conflicts with efficiency goal", "severity": "Important", "phase": {"primary": "design", "contributing": "calibrate"}, "section": "Cross-Iteration Finding Tracking", "issue": "Design mandates clean reviews (FR1.3, no prior context to reviewers) to preserve diversity. But this means reviewers can't focus on unresolved findings from prior review, leading to duplicate work. NFR1.1 wants <5min reviews, but reviewers re-analyze already-addressed issues.", "why_it_matters": "Tension between review quality (clean perspectives) and efficiency (don't waste time on resolved issues). For iterative designs, 80% of findings might repeat across runs if reviewers don't know what changed.", "suggestion": "Acknowledge tradeoff explicitly. Consider providing reviewers with resolved finding count (not content) as signal to adjust depth. Or accept that clean reviews cost more in iteration cycles and adjust time expectation."}
{"type": "finding", "id": "v1-assumption-hunter-017", "title": "Single-user constraint hidden in C1.4", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Constraints", "issue": "C1.4 states 'local filesystem (single-user workflow)' as constraint but never explains implications. What if design doc is being edited during review? What if two team members want to review same doc concurrently?", "why_it_matters": "Single-user constraint is major limitation for team workflows but buried in constraints section. Design doesn't explain why this constraint exists (technical limitation? MVP scope choice?) or how it could be lifted.", "suggestion": "Move single-user constraint to Must-Have/Out-of-Scope section with rationale. Explain whether this is fundamental architectural limitation or just MVP scoping. Document workarounds (e.g., separate git branches per reviewer)."}
{"type": "finding", "id": "v1-assumption-hunter-018", "title": "LLM semantic matching for pattern delta assumed reliable", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Cross-Iteration Finding Tracking", "issue": "FR10.2 uses 'LLM-based semantic matching' to identify resolved/persisting/new patterns across iterations. Assumes LLM can reliably determine equivalence without false positives (claiming pattern is same when it's different) or false negatives (claiming pattern is new when it persisted).", "why_it_matters": "Delta detection accuracy affects iteration tracking value. If LLM falsely claims patterns resolved (false negative), user thinks issue fixed when it's not. If LLM claims patterns new when they persisted (false positive), user thinks review found new issues when it's duplicates.", "suggestion": "Add NFR6.2 acceptance criteria for delta detection accuracy (e.g., >90% precision/recall on planted test cases). Document that semantic matching is heuristic, not perfect. Provide user override mechanism when delta detection is wrong."}
{"type": "finding", "id": "v1-assumption-hunter-019", "title": "Blind spot check assumed to add value without validation", "severity": "Minor", "phase": {"primary": "design", "contributing": null}, "section": "Reviewer Personas", "issue": "FR9.4 requires 'blind spot check per reviewer' for self-error-detection. Design notes this is 'optional — being empirically validated' in persona prompt template but doesn't explain validation criteria or what happens if blind spot checks produce no value.", "why_it_matters": "If blind spot checks are boilerplate filler ('I might have missed edge cases'), they waste tokens and add no value. Open Questions section asks 'whether blind spot checks produce actionable findings or noise' but no plan for how to decide.", "suggestion": "Make blind spot check explicitly experimental in FR9.4. Define success criteria (e.g., ≥30% of blind spot checks identify actionable finding) and plan to remove if criteria not met after 10 reviews."}
{"type": "finding", "id": "v1-assumption-hunter-020", "title": "Verdict escalation assumes upstream phases are idempotent", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Verdict Logic", "issue": "When verdict is 'escalate,' user returns to survey or calibrate phase. Design assumes user can re-do those phases without breaking things. What if survey phase gathered time-sensitive data? What if calibrate phase made decisions that downstream work depends on?", "why_it_matters": "Escalation could be expensive if upstream phases aren't designed for re-execution. Requirements phase might require stakeholder re-engagement. Survey phase might require new research. Design doesn't specify how to handle non-idempotent upstream dependencies.", "suggestion": "Document escalation as potentially expensive operation requiring upstream rework. Add guidance: when escalating to calibrate, specify which requirements need revision. When escalating to survey, specify what research questions are unresolved."}
