{"type":"finding","id":"v1-problem-framer-001","title":"Problem statement missing from design document","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"Overview","issue":"The Overview section describes what parallax:review does (solution) but never states the problem it solves. Line 8-12 jumps directly to 'core hypothesis' and 'core novel contribution' without establishing the pain point or user need. Requirements doc has JTBD section (lines 9-39) but design has no problem statement.","why_it_matters":"Without explicit problem framing, reviewers cannot validate whether the solution addresses root causes versus symptoms. Readers cannot assess if this is the right problem to solve. Requirements-design gap creates risk of solution drift.","suggestion":"Add Problem Statement section before Overview. Reference requirements JTBD but reframe as: 'What pain does this solve? Why does manual review fail? What's the root cause—lack of perspectives, orchestration overhead, or finding classification gaps?' Tie solution directly to problem."}
{"type":"finding","id":"v1-problem-framer-002","title":"Core hypothesis conflates two distinct problems","severity":"Important","phase":{"primary":"calibrate","contributing":"survey"},"section":"Overview","issue":"Line 12 states 'multiple perspectives catch design gaps' (perspective diversity problem) as hypothesis, then line 12 states 'finding classification routes errors back' (pipeline phase routing problem) as novel contribution. These are separate problems with different root causes. Perspective diversity addresses blind spots. Phase routing addresses workflow inefficiency.","why_it_matters":"Conflated problems lead to unclear success criteria. If perspective diversity is the hypothesis, eval framework should measure gap detection rate. If phase routing is novel, eval should measure rework reduction. Currently unclear which problem is primary and which is secondary.","suggestion":"Separate into: (1) Core problem: Single-perspective review misses design gaps (parallax error). (2) Secondary problem: Findings discovered late, traced back to wrong phase. Establish problem hierarchy and validate each independently."}
{"type":"finding","id":"v1-problem-framer-003","title":"Pipeline phase routing solves symptom not root cause","severity":"Critical","phase":{"primary":"calibrate","contributing":"survey"},"section":"Overview, Finding Phase Classification","issue":"Lines 12 and 154-161 frame finding classification as novel contribution, but routing findings back to failed phases treats symptom (late discovery) not root cause (inadequate upstream review). If requirements were properly reviewed, design findings wouldn't trace back to calibrate gaps. Root problem is checkpoint quality, not finding routing.","why_it_matters":"Building elaborate routing infrastructure (systemic detection, contributing phase, escalation logic) optimizes wrong problem. High percentage of calibrate-phase findings in design review signals requirements review failed, not that routing is needed. Adds complexity without addressing why upstream reviews miss issues.","suggestion":"Reframe problem: 'Checkpoints lack adversarial review, passing flawed artifacts downstream.' Solution: Apply parallax:review at ALL checkpoints (requirements, design, plan), not just design. Phase routing becomes diagnostic signal (high escalation rate = upstream checkpoint failed) rather than primary feature."}
{"type":"finding","id":"v1-problem-framer-004","title":"Implicit assumption that design review is primary use case","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Skill Interface, Prototype Scope","issue":"Lines 26 and 324-342 state design stage is MVP scope, requirements/plan stages deferred. But requirements JTBD (lines 18-21) shows calibrate-phase gaps as critical pain point. Problem framing suggests requirements review is higher leverage (prevents downstream issues) but scope prioritizes design review first.","why_it_matters":"Building design review first validates lower-leverage use case. If requirements review prevents design failures (as phase classification hypothesis suggests), prototyping design review wastes effort on downstream symptom treatment. Scope-problem misalignment.","suggestion":"Validate problem hierarchy: Is design review most painful (high manual overhead) or requirements review most valuable (prevents rework)? If requirements review higher leverage, prototype requirements stage first with 4 personas. Design stage second. Match scope to problem severity."}
{"type":"finding","id":"v1-problem-framer-005","title":"Success criteria missing for problem validation","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Jobs-to-Be-Done","issue":"Requirements lines 9-39 list five pain points but no measurable success criteria. 'Eliminate manual overhead' (Job 1) lacks baseline metric (how much time currently spent?). 'Catch design flaws in the phase that caused them' (Job 2) lacks target (what percentage of findings should be routed correctly?). Cannot validate whether problem is solved.","why_it_matters":"Without success metrics, cannot distinguish problem solved from problem reframed. Risk shipping solution that addresses different problem than stated. Requirements have acceptance criteria for implementation (FR1.2, FR2.2) but not for problem validation.","suggestion":"Add success criteria per JTBD: Job 1: Reduce review orchestration time from X to Y minutes (measure baseline in test case). Job 2: Z% of findings correctly classified to causative phase. Job 3: Verdict accuracy >N% (compare to human gold standard). Measure problem severity before building solution."}
{"type":"finding","id":"v1-problem-framer-006","title":"Problem scope unclear: skill vs pipeline vs orchestration","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Overview, Pipeline Integration, Prototype Scope","issue":"Line 10 describes parallax:review as 'skill that dispatches reviewers' (tool scope). Line 312-322 describes it as 'gate between phases' (pipeline scope). Line 326 says 'prototype scope: review skill only' but design includes verdict logic, escalation, phase routing (orchestration concerns). Unclear where review skill ends and orchestrator begins.","why_it_matters":"Scope ambiguity causes scope creep. If skill scope, verdict should be output not decision. If orchestration scope, skill needs state management and phase awareness. Currently design includes orchestration logic (escalate, revise, proceed) but states orchestrator is separate. Problem boundary unclear.","suggestion":"Clarify problem scope: Is parallax:review solving 'get adversarial review on one artifact' (skill) or 'manage review gates across pipeline' (orchestration)? If skill, remove verdict logic and escalation (output findings, let orchestrator decide). If orchestration, admit that and scope accordingly. Likely: split into two problems."}
{"type":"finding","id":"v1-problem-framer-007","title":"Unstated assumption about problem generalizability","severity":"Minor","phase":{"primary":"calibrate","contributing":"survey"},"section":"Jobs-to-Be-Done, Prototype Scope","issue":"Requirements lines 9-11 reference 'real design sessions' in problem-statements/design-orchestrator.md as validation, but design lines 344-346 list 'Second Brain Design test case' and 'real design docs from brainstorming skill' as test cases. Unclear if problem applies to all design docs or just AI-assisted design sessions. Scope assumption unstated.","why_it_matters":"If problem is specific to AI-assisted design (verbose outputs, implicit assumptions), solution may not generalize to human-authored designs. If problem is general, AI-specific test cases may miss edge cases. Generalizability impacts market size and solution applicability.","suggestion":"State problem scope explicitly: Does manual review overhead apply to all software design or specifically AI-generated design artifacts? If AI-specific, list why (token volume, implicit context). If general, include human-authored design in test cases to validate problem holds."}
{"type":"finding","id":"v1-problem-framer-008","title":"Root cause framing inverted: tool problem or process problem","severity":"Critical","phase":{"primary":"calibrate","contributing":"survey"},"section":"Jobs-to-Be-Done","issue":"Requirements frame problem as tool gap (Job 1: 'manual prompting overhead', Job 4: 'no way to track progress') but root cause may be process gap (lack of design review discipline). If teams don't do adversarial review because it's tedious, tool removes friction. If teams don't do it because they don't know how, tool doesn't teach method. Problem diagnosis unclear.","why_it_matters":"Tool problem → build parallax:review. Process problem → document review methodology, tool becomes secondary. If root cause is skill gap (teams don't know how to do adversarial review), automating it hides the skill development opportunity. May be solving wrong layer of problem.","suggestion":"Validate root cause: Interview users who attempted manual multi-perspective review. Was pain point tooling overhead or lack of review framework? If framework, provide methodology first (checklist, personas as mental model). If tooling, current design valid. Test: Can team do effective review manually with persona checklist? If yes, tool problem. If no, skill problem."}
{"type":"finding","id":"v1-problem-framer-009","title":"Problem-solution fit unclear for 'finding classification' contribution","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Overview, Finding Phase Classification","issue":"Line 12 claims 'finding classification routes errors back to pipeline phase' as novel contribution, but no stated problem in JTBD requires this. Job 2 (lines 18-21) says 'design flaws discovered late, traced back to missing requirements' but solution is better requirements review, not finding routing. Classification solves unstated problem.","why_it_matters":"Novel contribution that doesn't map to stated problem signals solution-first thinking. Risk of building feature that doesn't address user pain. Requirements show systemic detection (FR2.7) uses classification, but why is classification valuable if not tied to pain point?","suggestion":"Either: (1) Add JTBD that classification solves: 'Job 6: Determine which phase failed so team knows where to fix process gaps.' Or (2) Reframe classification as diagnostic feature, not core contribution. Novel contribution should directly address top pain point (manual overhead or perspective diversity)."}
{"type":"finding","id":"v1-problem-framer-010","title":"Blind spot check: Problem Framer perspective","severity":"Minor","phase":{"primary":"calibrate","contributing":null},"section":"Meta","issue":"Problem Framer focus on problem statement and root causes may miss: (1) Implicit problem framing embedded in design decisions (e.g., persona selection assumes coverage-based review is right approach). (2) Problems that are well-framed but not validated with users. (3) Whether stated problems reflect designer's assumptions vs actual user pain.","why_it_matters":"Focusing only on explicit problem statements misses implicit problem framing in solution choices. Design may encode problem assumptions that deserve interrogation (e.g., why 6 personas, why parallel dispatch).","suggestion":"Cross-check with User Research perspective: Are stated problems validated with real users or inferred from designer experience? Check with First Principles: Do solution constraints (parallel agents, JSONL output) reveal unstated problem assumptions?"}
