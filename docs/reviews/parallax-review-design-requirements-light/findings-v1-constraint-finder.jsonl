{"type": "finding", "id": "v1-constraint-finder-001", "title": "Timeout values arbitrary without performance justification", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Parallel Agent Failure Handling", "issue": "Timeout of 60-120s per agent lacks justification. No stated basis for this range - no performance benchmarks, no reasoning about review complexity or document size.", "why_it_matters": "Wrong timeout values cause false failures (too short) or wasteful blocking (too long). Timeout choice depends on document size, model speed, persona complexity - none documented.", "suggestion": "State the assumptions behind timeout values: expected document size range, expected finding count, model speed baseline. Add timeout as configurable parameter with documented defaults and scaling guidance."}
{"type": "finding", "id": "v1-constraint-finder-002", "title": "Minimum reviewer threshold lacks rationale", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Parallel Agent Failure Handling", "issue": "Partial results proceed if 4/6 agents succeed. No justification for 67% threshold. What if the missing 2 are the only ones that would have caught a Critical finding?", "why_it_matters": "Arbitrary threshold risks shipping broken designs. The missing reviewers might hold the only perspective that catches the fatal flaw. Coverage gaps aren't evenly distributed across personas.", "suggestion": "Define minimum threshold based on persona coverage, not raw count. Require at least one reviewer from each critical perspective category (assumption, edge case, requirement, feasibility). Document rationale for partial-review acceptance criteria."}
{"type": "finding", "id": "v1-constraint-finder-003", "title": "Cost constraints missing entirely", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "Prototype Scope", "issue": "No cost constraints documented. Design uses 4-9 parallel LLM agents per review with full document context, pattern extraction, delta detection. CLAUDE.md mentions $2000/month budget but no per-review cost limits or model tiering strategy in this design.", "why_it_matters": "Unbounded cost per review makes this unusable. A single design review with 6 agents reading full context could cost $5-20 depending on document size. Pattern extraction and delta detection add more LLM calls. Without cost constraints, first real-world use could blow the monthly budget.", "suggestion": "Document per-review cost ceiling (e.g., $2 for MVP, $10 for production). Specify model tiering strategy per persona (Haiku for simple reviewers, Sonnet for complex). Add cost estimation before dispatch with user approval gate."}
{"type": "finding", "id": "v1-constraint-finder-004", "title": "Document size limits unstated", "severity": "Critical", "phase": {"primary": "design", "contributing": null}, "section": "Skill Interface", "issue": "No maximum document size specified. Design assumes reviewers read full design and requirements docs via Read tool. No limits on token count, file size, or number of sections.", "why_it_matters": "Large documents break the design. A 50-page design doc exceeds context windows, causes review failures, makes pattern extraction infeasible. Read tool has 2000-line limit per call - documents exceeding this require chunking strategy not described in design.", "suggestion": "Define maximum supported document size (e.g., 10,000 tokens for design + requirements combined). Document chunking strategy for documents exceeding limits. Add pre-flight validation that rejects oversized inputs with clear error message."}
{"type": "finding", "id": "v1-constraint-finder-005", "title": "Pattern extraction cap lacks justification", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Cross-Iteration Finding Tracking", "issue": "Pattern extraction capped at 15 patterns per review with no reasoning. Why 15? What happens if there are 20 meaningful patterns? How does the cap interact with the 50-finding processing threshold?", "why_it_matters": "Arbitrary cap risks information loss. If real findings cluster into 18 patterns but cap is 15, which 3 get dropped? Cap should derive from cognitive load limits or tool constraints, not arbitrary choice.", "suggestion": "Justify the 15-pattern cap with reasoning (human cognitive load, UI constraints, etc.). Document pattern prioritization strategy when cap is exceeded. Consider making cap configurable based on review complexity."}
{"type": "finding", "id": "v1-constraint-finder-006", "title": "50-finding threshold for interactive processing arbitrary", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Cross-Iteration Finding Tracking", "issue": "Reviews with >50 findings skip interactive processing and write to disk only. No justification for 50 as the cutoff. Why not 30? Why not 100?", "why_it_matters": "Wrong threshold degrades UX. Too low: users forced into async mode for moderately complex reviews. Too high: interactive mode becomes unusable cognitive overload. Threshold should derive from session time limits or human attention span research.", "suggestion": "Base threshold on realistic session constraints (e.g., 5 minutes per finding = 50 findings = 4+ hour session). Make threshold configurable. Add finding-count estimate before review dispatch so users know which mode they'll get."}
{"type": "finding", "id": "v1-constraint-finder-007", "title": "Systemic issue detection threshold lacks empirical basis", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Synthesis", "issue": "Systemic issue flagged when >30% of findings with contributing phase share same contributing phase. No justification for 30%. Why not 25%? Why not 40%? Seems borrowed from v3 review calibrate failure rate but not validated for general use.", "why_it_matters": "Wrong threshold causes false positives (flagging normal variation as systemic) or false negatives (missing real systemic failures). Threshold should derive from empirical data across multiple reviews, not single-case observation.", "suggestion": "Document origin of 30% threshold and acknowledge it as hypothesis pending validation. Add threshold as configurable parameter. Track systemic detection accuracy in eval framework to calibrate threshold empirically."}
{"type": "finding", "id": "v1-constraint-finder-008", "title": "Retry strategy incomplete", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Parallel Agent Failure Handling", "issue": "Retry strategy says '1 retry with exponential backoff' but exponential backoff requires multiple retries to be exponential. Single retry means fixed delay. Also no backoff duration specified.", "why_it_matters": "Vague retry strategy causes implementation ambiguity. Single retry with fixed delay might not resolve transient rate limits (need several seconds). Exponential backoff with only one retry is contradictory.", "suggestion": "Clarify retry strategy: either single retry with fixed delay (e.g., 5s) or multiple retries with exponential backoff (e.g., 2s, 4s, 8s). Specify max total retry time budget per agent to bound worst-case latency."}
{"type": "finding", "id": "v1-constraint-finder-009", "title": "Concurrency limits unstated", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "UX Flow", "issue": "Design dispatches 4-6 reviewer agents in parallel but no concurrency constraints documented. API rate limits, local resource limits, and token-per-minute quotas could block parallel dispatch.", "why_it_matters": "Unbounded parallelism hits rate limits immediately. Anthropic API has per-account rate limits (e.g., 50 requests/min for Claude Sonnet). Dispatching 6 agents simultaneously could exhaust quota, causing cascading failures.", "suggestion": "Document expected concurrency limits based on API tier. Add rate-limit-aware dispatch queue that spaces requests to stay within quota. Consider serial dispatch as fallback when parallel dispatch fails."}
{"type": "finding", "id": "v1-constraint-finder-010", "title": "Git dependency assumption unstated", "severity": "Important", "phase": {"primary": "design", "contributing": "calibrate"}, "section": "Cross-Iteration Finding Tracking", "issue": "Design assumes git repo exists for diff-based change tracking ('when git diff is available'). Non-git workflows unsupported but this constraint not stated up-front in requirements or limitations section.", "why_it_matters": "Git assumption blocks non-git users (Dropbox, Google Drive, file-based workflows). Changed section highlighting and iteration tracking degrade silently without git. Users don't discover limitation until mid-review.", "suggestion": "State git requirement explicitly in Skill Interface section. Document degraded behavior for non-git workflows (no diff highlighting, pattern-only delta detection). Consider file-hash-based change detection as git-free alternative."}
{"type": "finding", "id": "v1-constraint-finder-011", "title": "Auto-fix scope unbounded", "severity": "Critical", "phase": {"primary": "design", "contributing": null}, "section": "UX Flow", "issue": "Auto-fix step applies fixes automatically but criteria are vague ('typos in markdown, missing file extensions, broken internal links'). No constraint on which file types can be auto-fixed, whether auto-fix can modify code, or blast radius limits.", "why_it_matters": "Unbounded auto-fix is dangerous. What if synthesizer misclassifies a semantic change as a typo? What if auto-fix breaks code formatting? User gets surprise commits they didn't review. Trust in tool collapses after first bad auto-fix.", "suggestion": "Define strict whitelist of auto-fixable changes (e.g., markdown typos only, no code changes). Add max-changes-per-auto-fix limit (e.g., 5). Require explicit user confirmation before applying auto-fixes, showing diff preview."}
{"type": "finding", "id": "v1-constraint-finder-012", "title": "Pattern extraction in critical path without timeout", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Cross-Iteration Finding Tracking", "issue": "Pattern extraction runs in critical path after synthesis before finding processing. No timeout specified. If pattern extraction hangs or takes 10 minutes, entire review workflow blocks.", "why_it_matters": "Critical path operations need timeout bounds. Pattern extraction is an LLM call - unpredictable latency. Users waiting for interactive processing don't know why they're stuck if pattern extraction hangs.", "suggestion": "Add timeout to pattern extraction (e.g., 30s). On timeout, skip pattern extraction and proceed to finding processing with degraded delta detection. Log timeout event for eval framework to track failure rate."}
{"type": "finding", "id": "v1-constraint-finder-013", "title": "Reviewer tool access unspecified", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Reviewer Capabilities", "issue": "Section says 'tool access boundaries specified in stable prompt prefix per persona' but doesn't actually specify them. Which reviewers get Read tool? Which get Grep? Which get web search?", "why_it_matters": "Undefined tool access blocks implementation. Prior Art Scout needs web search to find existing solutions. Assumption Hunter might need Grep to search codebase for unstated dependencies. Without tool specifications, prompt engineering can't start.", "suggestion": "Create tool access matrix showing which tools each persona gets (Read, Grep, Glob, web search). Start conservative (Read only) and expand based on observed reviewer needs during prototyping as stated."}
{"type": "finding", "id": "v1-constraint-finder-014", "title": "Prompt caching savings assumption unvalidated", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Reviewer Prompt Architecture", "issue": "Design assumes '90% input cost reduction on cache hits' from prompt caching. This is Anthropic's advertised rate, not validated for this use case. Cache hit rate depends on stable prefix size, request volume, cache TTL - none analyzed.", "why_it_matters": "Overestimating cache savings leads to cost overruns. If stable prefix is small relative to variable suffix, cache savings are minimal. If reviews are infrequent (days apart), cache expires between uses. Actual savings might be 20%, not 90%.", "suggestion": "State cache savings as hypothesis pending validation. Document cache invalidation triggers (stable prefix changes, TTL expiry). Track actual cache hit rates in JSONL cost logging to validate assumptions."}
{"type": "finding", "id": "v1-constraint-finder-015", "title": "Multi-file design support claimed but unspecified", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Reviewer Prompt Architecture", "issue": "Document access section says Read tool 'supports multi-file designs' but no specification of how multi-file designs are passed to skill or how reviewers access multiple files.", "why_it_matters": "Multi-file support is complex. Does user pass array of file paths? How do reviewers know which files exist? Do all reviewers read all files or selective reading based on persona? Missing specification blocks implementation.", "suggestion": "Specify multi-file input format (array of paths vs directory path). Document reviewer strategy for multi-file designs (read all, read selectively based on file type). Defer multi-file support to post-MVP if specification is complex."}
{"type": "finding", "id": "v1-constraint-finder-016", "title": "Schema validation blocks auto-fix but schema undefined", "severity": "Critical", "phase": {"primary": "design", "contributing": "calibrate"}, "section": "Parallel Agent Failure Handling", "issue": "Schema validation of reviewer output is 'blocked on JSONL schema definition' but auto-fix step (Step 4) depends on structured finding classification. Auto-fix can't work without schema. Circular dependency.", "why_it_matters": "Auto-fix is in critical path but blocked by missing schema. Design can't be implemented as written. Either schema must be defined before implementation starts or auto-fix must be deferred.", "suggestion": "Acknowledge schema definition as prerequisite for implementation. Reference FR7.5 completion (JSONL schema exists per MEMORY.md). Update design to reference existing schema or defer auto-fix to post-schema-validation milestone."}
{"type": "finding", "id": "v1-constraint-finder-017", "title": "Authenticated document sources deferred without workaround", "severity": "Important", "phase": {"primary": "design", "contributing": "calibrate"}, "section": "Reviewer Prompt Architecture", "issue": "Authenticated sources (Confluence, Google Docs, Notion) deferred to MCP integration. Many real-world design docs live in these systems. No workaround specified for MVP.", "why_it_matters": "Deferring authenticated sources limits real-world usability. Users must copy-paste docs to local files, losing version history and update tracking. Friction reduces adoption.", "suggestion": "Document manual workaround for MVP (export to markdown, save locally). Add authenticated source support to post-MVP roadmap with priority based on user feedback. Consider read-only API access as MCP-free alternative."}
{"type": "finding", "id": "v1-constraint-finder-018", "title": "Per-review cost logging JSONL format unspecified", "severity": "Minor", "phase": {"primary": "design", "contributing": null}, "section": "Prototype Scope", "issue": "Cost logging per review run in JSONL output mentioned but format not specified. What fields are logged? Token counts? Model used? Cache hit rate? Latency?", "why_it_matters": "Unspecified log format blocks cost analysis. Can't evaluate model tiering effectiveness without consistent cost data structure. Can't track cache hit rates without defined schema.", "suggestion": "Define JSONL cost log schema: timestamp, persona, model, input_tokens, output_tokens, cache_hit, cost_usd, latency_ms. Reference FR7.5 schema work or create new cost-specific schema."}
{"type": "finding", "id": "v1-constraint-finder-019", "title": "Review iteration limit unstated", "severity": "Minor", "phase": {"primary": "design", "contributing": null}, "section": "Cross-Iteration Finding Tracking", "issue": "Design supports multiple review iterations (v1, v2, v3) with delta detection but no limit on iteration count. Unbounded iteration could indicate design thrash or reviewer miscalibration.", "why_it_matters": "Unbounded iteration is process smell. If review reaches v5, either the design is fundamentally broken or reviewers are producing noise. Iteration limit forces escalation decision.", "suggestion": "Add recommended iteration limit (e.g., 3 iterations max). After limit, escalate to human design review or reset requirements. Track iteration count in summary.md as process health metric."}
{"type": "finding", "id": "v1-constraint-finder-020", "title": "Selective re-run of failed reviewers requires state tracking", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Parallel Agent Failure Handling", "issue": "Design allows re-running individual failed reviewers without redoing successful ones. Requires tracking which reviewers completed successfully, which failed, and preserving successful reviewer outputs across re-runs. State management complexity not addressed.", "why_it_matters": "Selective re-run without state tracking causes duplicate work or lost results. If state isn't persisted, user can't resume after session ends. If state isn't cleaned up, stale results contaminate new runs.", "suggestion": "Define state tracking mechanism for partial reviews (e.g., .parallax-review-state.json with completed reviewer list, timestamps, output file paths). Document cleanup strategy for abandoned partial reviews."}
{"type": "finding", "id": "v1-constraint-finder-021", "title": "Changed section focus requires git history depth", "severity": "Minor", "phase": {"primary": "design", "contributing": null}, "section": "Cross-Iteration Finding Tracking", "issue": "Changed section highlighting via git diff requires prior committed version of design doc. New repos or docs with no git history have no diff. Shallow clones may not have full history for comparison.", "why_it_matters": "Git history constraints block changed section highlighting in common scenarios (new project, shallow clone, squashed commits). Reviewers lose focus signal without warning.", "suggestion": "Document git history requirements (full clone, design doc previously committed). Gracefully degrade when requirements not met (log warning, proceed without diff). Consider alternative change detection (file modification time, explicit change markers)."}
{"type": "finding", "id": "v1-constraint-finder-022", "title": "Contradiction resolution blocks proceed verdict", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Synthesis", "issue": "When reviewers disagree, synthesizer presents both positions for user resolution. But verdict logic is automated (Critical → revise/escalate). What if the contradiction IS the Critical finding? Does automated verdict fire before user resolves contradiction?", "why_it_matters": "Contradiction handling conflicts with automated verdict. If Assumption Hunter flags Critical but Feasibility Skeptic flags Minor for same issue, highest severity determines verdict (Critical → revise). But user hasn't resolved the disagreement yet. Automated action without human judgment.", "suggestion": "Add contradiction resolution gate before verdict computation when contradictions involve Critical findings. User must resolve severity disagreements before automated verdict logic applies. Document this flow in verdict logic section."}
{"type": "finding", "id": "v1-constraint-finder-023", "title": "No fallback when LLM semantic matching fails", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Cross-Iteration Finding Tracking", "issue": "Delta detection uses LLM-based semantic matching to compare patterns across iterations. No fallback specified when LLM call fails (rate limit, timeout, malformed output).", "why_it_matters": "Delta detection failure blocks multi-iteration reviews. If semantic matching fails on v2 vs v1 comparison, user can't see what changed or persisted. Review workflow stalls.", "suggestion": "Add fallback delta detection strategy when LLM semantic matching fails (e.g., exact text matching, pattern ID matching, skip delta and show all patterns). Log LLM failures for eval framework to track reliability."}
{"type": "finding", "id": "v1-constraint-finder-024", "title": "Blind spot check output format unspecified", "severity": "Minor", "phase": {"primary": "design", "contributing": null}, "section": "Per-Reviewer Output Format", "issue": "Each reviewer produces blind spot check ('what might I have missed?'). Format shows it in markdown but not clear if it's a finding with severity/phase or just freeform reflection. How does synthesizer process blind spot checks?", "why_it_matters": "Unstructured blind spot checks can't be processed systematically. If they're just prose, synthesizer must parse freeform text to extract actionable insights. If they're findings, they need severity/phase classification.", "suggestion": "Clarify blind spot check format: either structured finding with severity/phase or separate reflection section excluded from finding count. Document how synthesizer handles blind spot checks (include in summary, flag meta-concerns, ignore)."}
{"type": "finding", "id": "v1-constraint-finder-025", "title": "Topic label collision handling incomplete", "severity": "Minor", "phase": {"primary": "design", "contributing": null}, "section": "Skill Interface", "issue": "Topic label collision handled via timestamped folders for iteration separation. But what if user runs same review twice in same second? What if user wants to retry failed review with same topic label?", "why_it_matters": "Timestamp collision is rare but possible (automated test suites, rapid retries). Collision handling needs deterministic behavior, not hope timestamps differ.", "suggestion": "Add collision resolution strategy (append UUID suffix, fail with clear error, prompt user for new label). Define retry behavior (reuse existing folder vs create new timestamped folder)."}
{"type": "finding", "id": "v1-constraint-finder-999", "title": "Blind spot check: Constraint Finder perspective", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Meta", "issue": "What might I have missed by focusing on constraints? Possible blind spots: (1) Assumed budget constraint from CLAUDE.md applies to this design without checking if design explicitly states different limits. (2) Focused on missing constraints, might have missed contradictions between stated constraints. (3) Evaluated feasibility based on MVP scope but didn't check if MVP scope itself is feasible given stated timeline/resources (no timeline documented). (4) Assumed certain technical constraints (API rate limits) without checking if they're actually relevant to target deployment environment.", "why_it_matters": "Blind spot awareness helps catch gaps in the review process. Design might have implicit constraints I missed by pattern-matching to similar systems. Constraint contradictions might be more critical than missing constraints.", "suggestion": "Cross-check findings against actual requirements doc to see if constraints are stated there (not just missing from design). Look for contradictory constraints (e.g., low cost + high parallelism). Verify MVP scope feasibility given all identified constraints."}
