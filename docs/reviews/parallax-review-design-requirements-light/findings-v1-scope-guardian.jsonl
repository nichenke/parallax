{"type":"finding","id":"v1-scope-guardian-001","title":"MVP boundary contradicts prototype scope claim","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"Prototype Scope","issue":"Section states 'This design is phase 1 of the orchestrator problem statement. Scope boundary: review skill only.' But 'Build now' includes cross-iteration finding tracking (pattern extraction, delta detection), auto-fix workflow with separate git commits, and JSONL output infrastructure—none of which are mentioned in requirements v1.2. The MVP is significantly larger than stated.","why_it_matters":"Implementation team cannot plan work without knowing which features are actually in scope for v1. The gap between 'review skill only' and the actual feature list blocks timeline estimation and increases delivery risk.","suggestion":"Explicitly state whether pattern extraction, delta detection, auto-fix, and JSONL output are MVP or post-MVP. If MVP, update requirements v1.2 to include functional requirements for these features. If post-MVP, remove from 'Build now' section and move to 'Build later'."}
{"type":"finding","id":"v1-scope-guardian-002","title":"Auto-fix scope undefined with conservative promise","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"UX Flow - Step 4: Auto-Fix","issue":"Auto-fix is in 'Build now' scope but definition is circular: 'Define auto-fixable criteria conservatively in MVP and expand based on eval data.' No specific criteria provided. Examples given (typos, missing file extensions, broken links) suggest scope, but 'conservatively' leaves implementation team without actionable boundaries.","why_it_matters":"Auto-fix involves write operations to design artifacts and separate git commits. Without explicit inclusion/exclusion criteria, implementers cannot build the feature or testers cannot validate it. Conservative promise without definition creates scope creep risk as edge cases emerge.","suggestion":"Either provide explicit auto-fix rules for MVP (e.g., 'markdown lint violations only: whitespace, link syntax, heading structure') or move auto-fix entirely to post-MVP. If keeping in MVP, add functional requirement to requirements v1.2 with testable acceptance criteria."}
{"type":"finding","id":"v1-scope-guardian-003","title":"Requirements and plan stage personas listed but excluded from MVP","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Reviewer Personas","issue":"Design includes complete persona definitions for requirements stage (5 personas) and plan stage (5 personas) with detailed responsibilities and activation matrix. But 'Build later' explicitly defers these stages. This front-loads design work for deferred scope.","why_it_matters":"Including detailed designs for out-of-scope features adds cognitive load and increases risk of accidental implementation. If these personas evolve based on design-stage learnings, this upfront design may be wasted effort.","suggestion":"Move requirements and plan stage persona details to a 'Future Extensions' appendix or separate design doc. Keep only the activation matrix and high-level concept in main design to show intended evolution path without committing to specific implementations."}
{"type":"finding","id":"v1-scope-guardian-004","title":"JSONL output in MVP conflicts with markdown-first strategy","severity":"Important","phase":{"primary":"calibrate","contributing":"design"},"section":"Prototype Scope","issue":"'Build now' includes 'JSONL output enables jq-based filtering by severity/persona/phase without LLM tokens.' But Output Artifacts section shows only markdown files. JSONL schema is mentioned in failure handling ('Blocked on JSONL schema definition') but no JSONL output structure is defined in this design.","why_it_matters":"If JSONL output is MVP, the design is incomplete—missing schema definition, file naming, relationship to markdown artifacts. If JSONL is post-MVP, removing it from 'Build now' prevents scope creep. Requirements v1.2 FR7 specifies JSONL schema exists but doesn't specify when parallax:review adopts it.","suggestion":"Clarify JSONL scope: (1) If MVP, add JSONL schema section to design showing structure, files, and markdown-JSONL relationship. (2) If post-MVP, remove JSONL from 'Build now' and note it as v2 enhancement. (3) Add explicit requirement to requirements v1.2 stating when JSONL adoption happens."}
{"type":"finding","id":"v1-scope-guardian-005","title":"Cross-iteration tracking adds significant complexity to stated MVP","severity":"Critical","phase":{"primary":"calibrate","contributing":"design"},"section":"Cross-Iteration Finding Tracking","issue":"Pattern extraction (cap 15 patterns, JSON output), delta detection (LLM-based semantic matching), and finding processing thresholds (≤50 findings) are described in detail as part of synthesis workflow. But this represents a major feature addition beyond 'review skill only' scope claim. Pattern extraction runs in critical path, adds LLM cost, and requires semantic grouping logic.","why_it_matters":"Pattern extraction and delta detection are separate features with independent failure modes, cost implications, and validation requirements. Including them in MVP without explicit requirements triples the surface area of the review skill. This is the highest scope creep risk in the design.","suggestion":"Move cross-iteration tracking entirely to post-MVP unless there is explicit requirement driving it. If pattern extraction is essential for MVP (e.g., required for systemic issue detection), create dedicated functional requirements in requirements v1.2 with acceptance criteria, cost budgets, and eval metrics."}
{"type":"finding","id":"v1-scope-guardian-006","title":"Systemic issue detection denominator clarified but detection itself not scoped","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Synthesis - Finding Phase Classification","issue":"Systemic issue detection logic is defined: '>30% of findings with contributing phase share the same contributing phase.' But no specification of what happens when systemic issue is detected beyond 'synthesizer flags' it. Is this informational? Does it block the verdict? Does it trigger auto-escalation?","why_it_matters":"Systemic detection is mentioned as a key pattern extraction benefit and is part of synthesis workflow. Without action specification, implementers cannot build the feature and users cannot predict behavior when systemic issues are found.","suggestion":"Either specify systemic issue detection action (e.g., 'add warning banner to summary, do not change verdict') or move systemic detection to post-MVP observability features. If keeping in MVP, add functional requirement to requirements v1.2."}
{"type":"finding","id":"v1-scope-guardian-007","title":"Reviewer tool capabilities deferred to eval without MVP baseline","severity":"Important","phase":{"primary":"design","contributing":"calibrate"},"section":"Reviewer Capabilities","issue":"Section states 'Specific tool assignments are an empirical question for the eval framework—start with baseline access and expand based on observed reviewer needs.' But no baseline is defined. Without baseline tool access, reviewers cannot function (they need at minimum Read tool to access design/requirements docs per Document Access section).","why_it_matters":"Implementation cannot proceed without knowing which tools each persona gets. Deferring to eval framework means MVP cannot be built. This is a design incompleteness blocking implementation.","suggestion":"Define minimum viable tool set for MVP (likely: Read tool for all personas, Grep/Glob for Prior Art Scout, Bash for feasibility checks). Document this as baseline in design. Defer expansion/restriction to eval phase, but specify starting point now."}
{"type":"finding","id":"v1-scope-guardian-008","title":"Validation test cases reference inaccessible Second Brain artifact","severity":"Minor","phase":{"primary":"design","contributing":"survey"},"section":"Prototype Scope - Validate with","issue":"'Second Brain Design test case (3 reviews, 40+ findings in the original session)' is listed as primary validation artifact. But MEMORY.md Session 15 notes Second Brain is from openclaw repo which is missing from current machine ('Accessibility trumps quality: Second Brain theoretically better test case but inaccessible').","why_it_matters":"Cannot validate MVP against stated test case if test artifact is unavailable. This creates risk that MVP is validated against different criteria than design intended. Parallax self-test is noted as viable alternative but not mentioned in this design.","suggestion":"Update validation test cases to reflect available artifacts: 'Parallax design doc self-test (design doc + v3 findings as ground truth)' or obtain Second Brain artifact before implementation begins. Remove references to inaccessible test cases."}
{"type":"finding","id":"v1-scope-guardian-009","title":"Changed section focus requires git integration but git assumptions not validated","severity":"Important","phase":{"primary":"design","contributing":"calibrate"},"section":"Cross-Iteration Finding Tracking - Changed section focus","issue":"Design assumes git repo exists and design doc has prior committed version to generate diff. But input contract accepts 'docs as file paths or inline context.' No handling specified for non-git scenarios (e.g., Confluence pages, Google Docs, first-time reviews of uncommitted docs).","why_it_matters":"If git diff is used for reviewer focus, non-git scenarios silently degrade. If it's optional, the design should state fallback behavior. Requirements v1.2 doesn't specify git as a constraint, creating assumption gap.","suggestion":"Add explicit git requirement to input contract if diff-based focus is essential, OR specify graceful degradation: 'When git diff unavailable (non-git docs, first review), reviewers perform full document review without change highlighting.' Add assumption to requirements v1.2."}
{"type":"finding","id":"v1-scope-guardian-010","title":"Prompt caching optimization described but versioning strategy incomplete","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Reviewer Prompt Architecture","issue":"Stable prefix is cacheable with version number tracked, calibration rules are not cached with separate version. But no specification of how version numbers are assigned, incremented, or stored. Section notes 'two version numbers tracked' but doesn't define versioning scheme or storage location.","why_it_matters":"Prompt versioning is part of FR8 (cost optimization via caching) and affects reproducibility. Without versioning scheme, cannot correlate review results with prompt versions during eval. Low severity because this is optimization infrastructure, not core functionality.","suggestion":"Either define versioning scheme in design (e.g., 'semantic versioning in prompt header comment, logged in summary.md metadata') or defer prompt versioning to post-MVP observability work. If deferring, remove version tracking from MVP scope."}
{"type":"finding","id":"v1-scope-guardian-011","title":"Partial failure handling specifies minimums but no retry budget","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Parallel Agent Failure Handling","issue":"Timeout (60-120s), retry count (1 retry), and minimum threshold (4/6 agents) are specified. But no total retry budget specified. If 3 agents fail, does skill retry all 3 simultaneously? Sequentially? What's the maximum wall-clock time before complete failure?","why_it_matters":"Without retry budget, skill could spend unbounded time retrying failed agents. User experience degrades if review takes >5 minutes due to cascading retries. Cost implications if multiple retries consume tokens.","suggestion":"Add maximum total review time (e.g., 5 minutes wall-clock including retries) or maximum retry budget (e.g., 6 total agent invocations = 6 initial + 0 retries OR 4 success + 2 retries). Document timeout behavior in UX Flow section."}
{"type":"finding","id":"v1-scope-guardian-012","title":"MCP integration mentioned for authenticated sources but deferred without boundary","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Reviewer Prompt Architecture - Document access","issue":"'Authenticated sources (Confluence, Google Docs, Notion) deferred to MCP integration.' But no specification of when MCP integration happens (post-MVP? v2? v3?) or what MVP behavior is when user provides authenticated URL.","why_it_matters":"If user provides Confluence URL as design doc path, skill must fail gracefully with clear error. Without error message specification, user gets confusing failure. Affects UX and adoption if users attempt unsupported workflows.","suggestion":"Add explicit error handling to input validation: 'MVP supports local files and public URLs only. Return clear error if authenticated URL provided: ERROR: Authenticated sources not supported in v1. Please export to markdown.' Specify MCP integration timeline if known."}
{"type":"finding","id":"v1-scope-guardian-013","title":"Reviewer count optimization deferred to eval but cost planning requires estimate","severity":"Important","phase":{"primary":"calibrate","contributing":"survey"},"section":"Open Questions","issue":"'Optimal number of personas per stage (starting hypothesis: 4-6)' listed as open question for eval framework. But MVP defines exactly 6 personas for design stage. If 6 is starting hypothesis, what's the success criteria for changing it? If 6 is fixed for MVP, it's not an open question.","why_it_matters":"Cost planning for MVP requires knowing exact persona count. 4-6 range represents 33% cost variance. If persona count is fixed at 6 for MVP eval, remove from open questions. If it's actually variable during MVP (test 4 vs 5 vs 6), specify eval design.","suggestion":"Clarify MVP scope: (1) If 6 personas fixed for MVP, remove from open questions and document as 'eval framework will test 4-8 personas post-MVP.' (2) If persona count is variable in MVP, define A/B test structure and move to eval design, not review design. (3) Add cost estimate for 6-persona review to requirements."}
{"type":"finding","id":"v1-scope-guardian-014","title":"Contradiction resolution deferred to user but synthesis role claims judgment authority","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Synthesis","issue":"Synthesizer 'surfaces contradictions' and 'suggests tie-breaking criteria' but explicitly 'does NOT pick winners in contradictions.' Yet earlier section states synthesizer 'exercises editorial judgment transparently.' Scope of synthesizer judgment is ambiguous.","why_it_matters":"If synthesizer cannot resolve contradictions, suggesting tie-breaking criteria exceeds stated authority. If synthesizer can exercise judgment, the 'does NOT' list is incomplete. Affects prompt design and user expectations during finding processing.","suggestion":"Clarify synthesizer judgment boundary: 'Synthesizer exercises judgment for deduplication and phase classification only. For contradictions, synthesizer describes the tension and lists resolution options (not recommendations) for user decision.' Distinguish descriptive analysis from prescriptive recommendation."}
{"type":"finding","id":"v1-scope-guardian-015","title":"Discuss mode cut from MVP without eval justification","severity":"Minor","phase":{"primary":"calibrate","contributing":null},"section":"UX Flow - Step 6: Process Findings","issue":"'Discuss mode cut from MVP—evaluate adding in v2 if eval data shows rejected findings aren't being addressed.' This is a scope decision based on predicted future data, not current evidence. The reason for cutting (complexity? cost? timeline?) is not stated.","why_it_matters":"Scope cuts should be justified by current constraints, not hypothetical future data. If discuss mode is valuable, cutting it delays value delivery. If it's not valuable, the v2 re-evaluation criterion is misaligned. Affects user workflow if they need to discuss ambiguous findings.","suggestion":"State actual reason for discuss mode cut (e.g., 'Discuss mode deferred to v2 to reduce MVP scope and accelerate delivery'). If rejection notes are sufficient for calibration feedback, state that explicitly. Remove eval-conditional language."}
{"type":"finding","id":"v1-scope-guardian-016","title":"Clean reviews prevent reviewer anchoring but increase duplicate effort","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Cross-Iteration Finding Tracking","issue":"'Reviewers do NOT receive prior review context (avoids anchoring bias, preserves perspective diversity).' But pattern extraction and delta detection run after synthesis to detect resolved/persisting findings. This means reviewers re-analyze the same issues every iteration without awareness.","why_it_matters":"Clean reviews are a quality tradeoff: better perspective diversity but higher cost and longer review time. If delta detection shows 80% of findings persist across iterations, the benefit of clean reviews may not justify the cost. Eval framework should measure this, but it's not listed in Open Questions.","suggestion":"Add to Open Questions: 'Whether clean reviews (no prior context) provide sufficient quality benefit to justify duplicate analysis cost. Eval should measure: finding persistence rate across iterations, new findings per iteration after v1, cost multiplier vs context-aware reviews.' This frames the tradeoff for eval validation."}
{"type":"finding","id":"v1-scope-guardian-999","title":"Blind spot check: Scope Guardian perspective","severity":"Minor","phase":{"primary":"calibrate","contributing":null},"section":"Meta","issue":"Focusing on scope boundaries may have missed implicit scope expansion through dependency chains. Pattern extraction is in critical path after synthesis—does this block finding processing? Auto-fix requires separate git commits—does this assume git CLI availability and user git configuration? JSONL output mentioned but schema location not specified—is it in parallax repo or external? Changed section focus assumes design doc is a single file—what about multi-file designs? Several features reference 'eval framework' without specifying what's in eval scope vs review scope.","why_it_matters":"Implicit scope in design decisions can expand MVP without explicit acknowledgment. Dependency assumptions (git CLI, single-file designs, JSONL schema existence) should be surfaced as requirements or constraints.","suggestion":"Audit design for dependency assumptions: document git CLI requirement, multi-file design handling, JSONL schema source. Clarify eval framework boundary: what's being evaluated (review quality) vs what's being built (review skill). Consider whether pattern extraction and delta detection are post-synthesis analysis tools (out of review scope) or synthesis features (in scope)."}