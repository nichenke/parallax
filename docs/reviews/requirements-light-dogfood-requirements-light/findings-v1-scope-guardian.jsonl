{"type":"finding","id":"v1-scope-guardian-001","title":"Synthesis logic scope undefined - reuse vs rebuild decision unmade","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Implementation Notes > Reusable Components","issue":"Document states 'Synthesis logic (if we build a synthesizer)' but doesn't clarify whether MVP includes synthesis or not. Reusable Components section mentions synthesis but MVP Scope section doesn't list it in 'Build First' items.","why_it_matters":"Unclear whether summary.md generation is in/out of MVP scope. Affects implementation effort estimate and success criteria validation (summary.md is listed as output artifact but building it isn't explicitly scoped).","suggestion":"Explicitly state in MVP Scope whether summary.md synthesis is included or deferred. If included, add 'Summary synthesis' to Build First list. If deferred, remove summary.md from Output artifacts section or mark as 'Post-MVP'."}
{"type":"finding","id":"v1-scope-guardian-002","title":"Test case scope ambiguous - 3-5 past brainstorms undefined","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"MVP Scope > Test Cases","issue":"States 'Test on 3-5 past brainstorms' but lists 4 specific test cases with uncertain accessibility (2 marked 'if accessible'). Unclear if MVP validation requires all 4, any 3-5, or specific subset.","why_it_matters":"MVP success criteria depend on test case validation ('catches 3-5 real gaps per test case'). If test cases aren't accessible, MVP validation might fail for logistical reasons rather than design quality. Implementation could stall waiting for inaccessible test data.","suggestion":"Specify minimum viable test set: e.g., 'MVP requires 3 accessible test cases minimum. Priority order: (1) Parallax own design doc (always accessible), (2) Parallax v1/v2 problem statements, (3) others if accessible.' Document fallback if openclaw/claude-ai-customize unavailable."}
{"type":"finding","id":"v1-scope-guardian-003","title":"Persona prompt implementation detail scope unclear","severity":"Minor","phase":{"primary":"calibrate","contributing":null},"section":"Reviewer Personas","issue":"Personas defined with focus, blind spot checks, and 'looks for' criteria, but no specification of prompt structure, length limits, or format. Unclear if each persona gets identical prompt template with different focus sections or custom prompts.","why_it_matters":"Affects implementation consistency and maintenance. Custom prompts per persona = more effort, harder to maintain. Templated prompts = faster implementation but might miss persona-specific nuances. Scope boundary unclear.","suggestion":"Add to Implementation Notes: specify whether personas use (a) shared prompt template with persona-specific focus sections (faster MVP), or (b) custom prompts per persona (higher quality, more effort). Document choice and rationale."}
{"type":"finding","id":"v1-scope-guardian-004","title":"Finding ID format specification incomplete","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"Output Format > JSONL Findings","issue":"Example shows 'v1-problem-framer-001' format but doesn't specify: (1) whether NNN counter is per-persona or global, (2) how to handle >999 findings, (3) whether v1 refers to review iteration or schema version.","why_it_matters":"Blocks implementation of finding ID generation logic. Ambiguous ID scheme could cause collisions if multiple personas generate finding 001, or confusion if v1 refers to schema version (already at v1.0.0) vs review iteration.","suggestion":"Add ID format spec to Output Format section: 'Format: v{iteration}-{persona}-{NNN} where iteration=review run number (v1, v2...), persona=lowercase-hyphenated role name, NNN=zero-padded sequential number per persona. Example: v1-scope-guardian-001, v1-scope-guardian-002, v2-scope-guardian-001 (second review run).'"}
{"type":"finding","id":"v1-scope-guardian-005","title":"Deep mode scope boundary vague - what changes post-light?","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Overview","issue":"Document states 'This design covers --light mode only. Deep mode deferred until light mode proves valuable' but doesn't define what deep mode would include/exclude differently. Section 'Why 5 personas' mentions 'Deep mode will have different personas' but no details.","why_it_matters":"Risk of scope creep: without clear light/deep boundary, features might leak into light mode ('just one more check'). Also affects evaluation: if deep mode scope undefined, can't validate whether light mode successfully handles its intended scope vs accidentally covering deep concerns.","suggestion":"Add section 'Light vs Deep Mode Scope Boundary' defining: (1) Light = post-brainstorm checkpoint (problem framing, scope, constraints), (2) Deep = post-design checkpoint (specification completeness, edge cases, implementation details). State explicitly what light mode does NOT validate (e.g., API design completeness, error handling specs, performance requirements beyond success criteria)."}
{"type":"finding","id":"v1-scope-guardian-006","title":"Validation metrics precision/recall ground truth undefined","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"Implementation Notes > Testing Strategy","issue":"Success threshold specifies 'Precision >70%' and 'Recall >60%' but doesn't define ground truth source. How to determine 'real gaps' vs false positives? Document states 'Compare to actual rework that happened' but past brainstorms may not have documented rework.","why_it_matters":"Blocks MVP validation. Can't measure precision/recall without ground truth. If relying on 'actual rework', test cases must have documented iteration history showing what gaps caused rework. Parallax own design doc might have this (v1/v2 reviews documented) but other test cases uncertain.","suggestion":"Add to Testing Strategy: specify ground truth approach per test case. For Parallax design doc: use v3 review findings as ground truth (calibrate-phase findings = requirement gaps that light mode should have caught). For other test cases: manual expert review to label findings as true positive / false positive. Document minimum ground truth quality needed for validation."}
{"type":"finding","id":"v1-scope-guardian-007","title":"Output directory naming collision risk with design review","severity":"Minor","phase":{"primary":"calibrate","contributing":null},"section":"Output Format > Output Location","issue":"Design states 'Separate folder prevents collision with design review outputs (<topic>-review-v1/)' but doesn't address scenario where same topic gets both requirements review AND design review. If both output to docs/reviews/<topic>-*/, could cause confusion.","why_it_matters":"Low immediate risk (MVP is manual, users control invocation) but affects workflow clarity. User might invoke both /requirements --light and /review on same topic, creating two similar-looking output folders. Unclear which findings to address first or whether they reference same design doc version.","suggestion":"Add to Output Location section: document expected workflow for topics with both requirements and design review: 'If both reviews run on same topic, folder structure is: docs/reviews/<topic>-requirements-light/ (early findings) and docs/reviews/<topic>-review-v1/ (post-revision findings). Requirements findings should be addressed before design review to avoid duplicate findings across reviews.'"}
{"type":"finding","id":"v1-scope-guardian-008","title":"Contributing phase attribution scope unclear for requirements issues","severity":"Minor","phase":{"primary":"calibrate","contributing":"survey"},"section":"Output Format > JSONL Findings","issue":"Phase classification specifies 'contributing: survey if research gap caused it' but doesn't define how reviewer determines causation. Is this reviewer judgment call or prescribed criteria? What counts as 'research gap caused it' vs just a requirement gap?","why_it_matters":"Affects finding classification consistency across personas. If each persona uses different judgment for survey attribution, findings won't be comparable. Could inflate/deflate survey phase issue counts in systemic analysis.","suggestion":"Add guideline to Output Format: 'Attribute contributing: survey only when finding explicitly identifies missing research/prior art as root cause (e.g., problem statement assumes approach without evaluating alternatives). Default to contributing: null for pure requirement gaps (unclear scope, missing constraints, unstated assumptions).'"}
