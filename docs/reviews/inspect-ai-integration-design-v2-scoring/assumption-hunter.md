{"type": "finding", "id": "v1-assumption-hunter-001", "title": "Judge impartiality assumes Haiku has no systematic bias toward leniency", "severity": "Critical", "phase": {"primary": "design", "contributing": "calibrate"}, "section": "Scoring Strategy (ADR-007) — Tier 1: reverse_judge_precision", "issue": "The design assumes Haiku will reliably detect false positives without systematic leniency bias. Haiku is optimized for speed and cost, not rigorous critique. At T=0 (deterministic) with a leniency-prone model, the judge could consistently return 'genuine' for borderline findings — producing inflated precision scores that mask reviewer quality problems. No calibration step verifies Haiku's false-positive detection rate against known false positives before the judge is trusted.", "why_it_matters": "If Haiku is systematically lenient, precision scores cluster near 1.0 regardless of reviewer quality. The primary quality signal becomes meaningless. A broken reviewer that hallucinates findings gets the same score as a high-quality one. The false positive list in the judge prompt mitigates this partially but does not verify Haiku's actual behavior against the encoded categories.", "suggestion": "Before relying on reverse_judge_precision as a quality signal, run Haiku against 5–10 known false positives from the encoded list (hallucinated requirements, implementation details, style preferences). Confirm Haiku marks them as NOT genuine. Document the calibration run. Gate Phase 2 on this calibration passing."}
{"type": "finding", "id": "v1-assumption-hunter-002", "title": "must_find.jsonl curation assumes human curator can distinguish doc-visible from context-dependent findings", "severity": "Critical", "phase": {"primary": "design", "contributing": "calibrate"}, "section": "Scoring Strategy (ADR-007) — Tier 2: must_find_recall", "issue": "The curation rule 'only include findings discoverable from the frozen document content alone' assumes the human curator reliably distinguishes document-visible findings from context-dependent ones. This is the exact same distinction that caused recall=0.0 in the previous scorer. The design provides one example (Bedrock IAM vs direct API env vars in MEMORY.md) but no systematic test for whether a given finding requires context. A curator who incorrectly includes a context-dependent finding creates a recall floor that can never be reached — a permanent false regression.", "why_it_matters": "must_find_recall is the regression guard. If it contains uncuratable findings (context-dependent, accidentally included), the baseline recall can never reach the min_recall threshold, and every eval run looks like a regression. This is a systematic false negative that erodes trust in the entire eval framework.", "suggestion": "Add a curation test protocol: for each proposed must_find candidate, ask 'Can a model with only the frozen document content produce this finding?' Run a single zero-shot check — pass only the frozen document to Haiku and ask it to find this specific issue. If Haiku cannot locate it from the document alone, the finding is context-dependent and goes to context_dependent_findings.jsonl. Document this protocol in the dataset curation guide."}
{"type": "finding", "id": "v1-assumption-hunter-003", "title": "Genuineness criterion 'materially affects whether the design can succeed' assumes shared understanding of success", "severity": "Important", "phase": {"primary": "design", "contributing": "calibrate"}, "section": "Scoring Strategy (ADR-007) — Tier 1, Encoded criteria — GENUINE", "issue": "The criterion 'materially affects whether the design/plan can succeed' is not anchored to a stated success definition in the reviewed document. Different judges will disagree on what 'success' means for an eval framework: shipping Phase 1? Achieving 90% recall? Becoming the canonical eval approach for parallax? A finding that blocks shipping but not eventual success could be rated either way. The phrase 'including logic bombs that make success improbable until resolved' adds specificity for one subcategory but leaves the base criterion underspecified.", "why_it_matters": "The judge is deterministic (T=0) but the prompt's definition of genuineness is ambiguous. Two identical Haiku runs on the same finding with the same document would produce the same output — but two human curators reading the prompt might encode the criterion differently. The result is prompt-version-sensitive precision scores that don't travel across judge prompt versions.", "suggestion": "Add a concrete example of a 'marginal genuineness' case directly in the judge prompt: 'A finding that the success metrics section lacks specific numeric thresholds IS genuine — it materially blocks whether success can be evaluated. A finding that the document would benefit from an executive summary is NOT genuine — the design can succeed without it.' This grounds the criterion in the document type without requiring a success definition."}
{"type": "finding", "id": "v1-assumption-hunter-004", "title": "Phase 2 prerequisites assume Phase 1 non-zero precision is achievable without judge calibration", "severity": "Important", "phase": {"primary": "design", "contributing": "design"}, "section": "Phase 2 Design Prerequisites (not in Phase 1)", "issue": "Prerequisite 1 states 'Phase 1 produces non-zero reverse_judge_precision on at least one reviewer task.' This assumes the reverse judge scorer is implemented and running in Phase 1. But the design later says 'Full implementation of reverse_judge_precision and must_find_recall scorers (prototype in scorers/reverse_judge_scorer.py is throwaway)' is a Phase 2 prerequisite. The Phase 1 definition and Phase 2 prerequisites are internally contradictory: Phase 2 requires non-zero Phase 1 precision, but the precision scorer is Phase 2 work.", "why_it_matters": "This circular prerequisite creates an unresolvable dependency. If the prototype scorer is throwaway, Phase 1 cannot produce a trustworthy precision signal. If Phase 1 must produce non-zero precision, the prototype must be production-quality. Either way, the current language blocks Phase 2 gate indefinitely.", "suggestion": "Clarify the phase boundary: either (a) the prototype reverse_judge_scorer is 'good enough to gate Phase 2' and Phase 1 delivers it as a working prototype, or (b) Phase 2 prerequisite 1 is replaced with 'Phase 1 produces non-zero parseable JSONL findings on at least one reviewer task' — which is already captured in Phase 1 success criteria. Eliminate the circular dependency explicitly."}
{"type": "finding", "id": "v1-assumption-hunter-005", "title": "False positive category 'duplicates another finding' assumes reviewer produces multiple findings per run", "severity": "Minor", "phase": {"primary": "design", "contributing": null}, "section": "Scoring Strategy (ADR-007) — Tier 1, Encoded false positives — NOT genuine", "issue": "The category 'Duplicates another finding from a different angle without adding new information' assumes the judge has access to the full set of findings from the same reviewer run. But reverse_judge_precision is scored per-finding — the judge sees one finding and the document, not the set. A finding that is genuinely redundant given three other findings from the same run looks isolated and non-redundant to the judge.", "why_it_matters": "Redundant findings inflate the total_findings denominator, lowering precision artificially. But the judge cannot detect this redundancy without the full set. The false positive category as written is not operationalizable in the per-finding judge architecture.", "suggestion": "Either (a) remove this category from the encoded false positive list — the per-finding judge cannot evaluate it — and handle deduplication as a post-processing step on the reviewer output before precision scoring; or (b) pass the full finding set to the judge alongside the individual finding, with instructions to check for redundancy. Note the token cost implication of option (b)."}
{"type": "finding", "id": "v1-assumption-hunter-006", "title": "Reverse judge assumes frozen document is always complete and self-contained", "severity": "Minor", "phase": {"primary": "design", "contributing": null}, "section": "Scoring Strategy (ADR-007) — Tier 1: reverse_judge_precision", "issue": "The design instructs to 'pass full document to judge — do not truncate.' This assumes the frozen document is semantically complete — that it contains all the context a judge needs to evaluate whether a finding is genuine. Design documents routinely reference external ADRs, requirements, or prior decisions by reference. A finding that correctly identifies a gap between the design and ADR-006 requires both documents. The judge with only the design doc may rate it NOT genuine (no external context required) when it is genuinely grounded in external constraints.", "why_it_matters": "Findings that correctly identify cross-document gaps get rated as false positives (referencing external context). Precision score underestimates reviewer quality. Reviewers who spot cross-document inconsistencies are penalized, not rewarded.", "suggestion": "For the initial implementation, add a clause to the judge prompt: 'If the finding references an external document or decision (ADR, requirements doc), treat this as a signal the finding may be genuine — do not rate as NOT genuine solely because the referenced document is not present. Rate based on whether the gap named is plausible given the design document content.' Flag this as a known limitation of single-document judge evaluation."}
{"type": "finding", "id": "v1-assumption-hunter-999", "title": "Blind spot check: Assumption Hunter perspective", "severity": "Minor", "phase": {"primary": "design", "contributing": null}, "section": "Meta", "issue": "This review focused on unstated assumptions in the scoring strategy design. I may have underweighted: (1) operational assumptions about who runs make reviewer-eval and how often — the design assumes a disciplined solo operator but says nothing about team workflows or CI; (2) assumptions about Inspect AI version stability — the design pins to current Inspect AI API patterns but does not state a version constraint; (3) the assumption that two scorers (precision + recall) are independent quality signals — if the same Haiku judge is used for both, systematic judge biases affect both metrics in correlated ways.", "why_it_matters": "Blind spot awareness helps catch gaps in the review process", "suggestion": "Consider: (1) Add a note about intended usage frequency and operator discipline. (2) Pin Inspect AI version in pyproject.toml. (3) Note that precision and recall share the same judge — correlated errors are possible."}
