# First Principles Review

## Problem Reframe
The core problem is: how do you know if a reviewer agent is getting better or worse across prompt iterations? From first principles, the answer requires two things: (1) a way to measure output quality that does not depend on capturing one specific run's findings as ground truth, and (2) a signal that is stable enough to distinguish signal from noise across independent runs. ADR-007 attempts to solve (1) with reverse_judge_precision and (2) with must_find_recall. The framing is directionally correct, but the implementation encodes assumptions about judge reliability and curation fidelity that deserve scrutiny from scratch.

## Findings

### Finding 1: The "logic bomb" framing conflates two independent quality dimensions
- **Severity:** Critical
- **Phase:** calibrate (primary), design (contributing)
- **Section:** Scoring Strategy (ADR-007) — Tier 1, Encoded criteria — GENUINE
- **Issue:** The genuineness criterion "materially affects whether the design/plan can succeed — including logic bombs that make success improbable until resolved" combines two different quality tests into one criterion: (A) does this finding identify a real structural gap? and (B) is this gap severe enough to threaten success? A finding can be a genuine structural gap (honest, discoverable, non-hallucinated) but only affect a non-critical component — genuine but not material. By requiring both genuine AND material in the same criterion, the judge rejects findings that are honest and correct but about lower-stakes design choices. This biases reverse_judge_precision toward only detecting Critical-severity findings, which is not what precision measures in a design review context.
- **Why it matters:** Conflating genuine (real, non-hallucinated) with material (threatens success) means Important and Minor findings are systematically rated NOT genuine by the judge — not because they are false, but because they are not Critical. The precision score becomes 'fraction of Critical findings' not 'fraction of real findings.' A reviewer that correctly identifies only Important issues scores near 0.0 on precision — a false signal that rewards false urgency over accurate calibration.
- **Suggestion:** Split the genuineness criterion into two independent checks: (A) Is this finding real? (Discoverable from document, not hallucinated, not a duplicate.) This gates precision — any finding passing A is genuine. (B) Is this finding material? (Materially affects success.) This gates severity classification but is not a precondition for genuineness. A finding can be genuine (A=true) without being material (B=false) — the reviewer should note it as Minor, not hallucinate it.

### Finding 2: Reverse judge precision solves the wrong version of the genuine-difference problem
- **Severity:** Critical
- **Phase:** calibrate (primary)
- **Section:** Scoring Strategy (ADR-007) — Why cross-run matching fails
- **Issue:** The design diagnoses two root causes for accuracy=0.000: context contamination and genuine run-to-run variance. ADR-007's reverse_judge_precision addresses context contamination (judge evaluates the finding against the frozen document, no external context). It does not address genuine run-to-run variance. If the reviewer correctly finds different-but-valid flaws each run, all runs will score high on reverse_judge_precision (all findings are genuine) — but you cannot compare run A to run B to detect regression. A reviewer that degrades from finding 10 real flaws to finding 2 real flaws but generating 8 hallucinated flaws could maintain 100% precision (if 2 genuine findings remain) while delivering 80% degradation in coverage.
- **Why it matters:** The design frames reverse_judge_precision as the "primary quality signal." But it cannot detect coverage regression — only hallucination regression. A reviewer that stops finding genuine flaws but never hallucinates is undetectable by precision alone. must_find_recall was supposed to catch this, but its Phase 2 deferral means Phase 1 has no coverage regression detector at all.
- **Suggestion:** Name the limitation explicitly in the design: 'reverse_judge_precision detects hallucination regression (producing false findings). must_find_recall detects coverage regression (missing genuine findings). Both are needed for complete quality measurement. Phase 1 delivers only hallucination detection.' This reframing prevents over-interpreting Phase 1 precision as a complete quality signal and sets correct expectations for Phase 2.

### Finding 3: The false positive list encodes reviewer evaluation heuristics, not epistemology
- **Severity:** Important
- **Phase:** calibrate (primary), design (contributing)
- **Section:** Scoring Strategy (ADR-007) — Tier 1, Encoded false positives — NOT genuine
- **Issue:** The false positive list ('implementation detail rather than a requirement/design gap,' 'hypothetical future concern,' etc.) is a list of common reviewer failure modes specific to design/requirements review context. It is not derived from a first-principles definition of what makes a finding genuine — it is a curated list of things that parallel review typically gets wrong. This is pragmatically valuable but structurally fragile: as the parallax skill evolves (plan review, execution review), the false positive list will need domain-specific updates that are not architecturally connected to the genuineness definition. A false positive for design review ('implementation detail') may be a valid genuine finding for plan review.
- **Why it matters:** The judge prompt will be reused across multiple skill types (requirements, design, plan) as parallax expands. Encoding design-review-specific false positives in the base judge creates a leakage problem: plan-review findings flagged as 'implementation detail' are correctly rejected by the design judge but incorrectly rejected by the same judge applied to plan review. The false positive list needs to be parameterized by review type, not global.
- **Suggestion:** Add a parameter to the judge prompt: 'Review type: [requirements|design|plan].' Include a review-type-specific section in the false positive list. For Phase 1 (design review only), this is a single entry — but the architecture is extensible. Document this as a known limitation: 'Current false positive list is calibrated for design review. Plan and requirements review require type-specific additions.'

### Finding 4: Two tiers of scoring may be solving a problem that simpler N-run sampling would solve
- **Severity:** Minor
- **Phase:** design (primary)
- **Section:** Scoring Strategy (ADR-007) — Overall architecture
- **Issue:** The original genuine-difference problem was: 'the model finds legitimately valid-but-different flaws each run.' The design's response is a two-tier scoring system with a new dataset artifact (must_find.jsonl), a new judge architecture, and Phase 2 implementation work. An alternative from first principles: run the reviewer N=5 times and ask 'which ground truth findings appear in at least K runs?' This union-based detection approach requires no new scorer infrastructure — just running the existing eval N times and aggregating results by finding title similarity. The design considered N-run approaches but deferred them to Phase 2.5 — but N-run aggregation may resolve the genuine-difference problem more directly than a two-tier precision/recall split.
- **Why it matters:** If N-run union sampling resolves the genuine-difference problem, the must_find.jsonl curation step is unnecessary — the union across N runs self-curates. This would eliminate the highest-complexity element of ADR-007 (curation protocol) while delivering equivalent recall signal. The design should explicitly compare N-run aggregation against must_find curation on cost, complexity, and signal quality before committing to two-tier architecture.
- **Suggestion:** Add a Phase 1.5 experiment: run the reviewer 5 times on the same document and measure what fraction of ground truth findings appear in at least 3 of 5 runs. If this fraction exceeds 70%, N-run aggregation is a viable alternative to must_find curation. Document this as a decision point before Phase 2 implementation begins.

## Blind Spot Check
I focused on whether the problem framing and solution architecture are correct from first principles. I may have underweighted: (1) the practical constraint that running N=5 reviewer tasks per eval cycle has significant cost and time implications that make N-run aggregation less appealing despite its conceptual simplicity; (2) the organizational constraint that the design must be implementable by a solo operator — two-tier scoring with explicit curation may be more tractable than N-run statistics for a single developer managing the eval framework.
