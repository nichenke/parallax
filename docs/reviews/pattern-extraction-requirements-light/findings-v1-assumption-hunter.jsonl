{"type": "finding", "id": "v1-assumption-hunter-001", "title": "LLM pattern extraction assumes semantic grouping succeeds without validation", "severity": "Critical", "phase": {"primary": "design", "contributing": "calibrate"}, "section": "Pattern Extraction Logic", "issue": "Design specifies LLM-driven semantic grouping where Claude identifies patterns via prompt, but provides no validation mechanism to verify patterns are actually semantic or stable across runs. Assumes Claude will group findings by root cause/theme without testing if groupings are consistent or meaningful.", "why_it_matters": "If pattern extraction produces inconsistent groupings (same findings â†’ different patterns on re-run), entire delta detection (FR10.2) fails. Bad patterns could hide real issues or create false systemic signals. No way to verify extraction quality until after implementation.", "suggestion": "Add validation step: Run pattern extraction twice on same sample with different random seeds. Patterns should be >80% consistent (same findings grouped together). If consistency <80%, add explicit pattern definition criteria to prompt or use few-shot examples to stabilize output."}
{"type": "finding", "id": "v1-assumption-hunter-002", "title": "Sample size representativeness not justified - assumes 20% captures full distribution", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Sample Finding Selection", "issue": "Design assumes 15-20 findings (20% of 83) will be representative of full v3 review without empirical basis. Selection criteria ensure diversity but don't guarantee sample captures rare patterns or long-tail distributions. Assumes stratified sampling (severity, reviewer, phase) produces representative results.", "why_it_matters": "If v3 has long-tail distribution (e.g., rare edge cases appearing once), small sample misses them. Pattern extraction validated on unrepresentative sample means full-scale deployment could produce different pattern counts or types. Requirements update (FR2.7/FR10) based on flawed sample validation.", "suggestion": "Add representativeness check: After sample selection, compare sample distribution to full v3 (severity ratio, phase distribution, reviewer coverage). If sample deviates >10% from population on any dimension, re-select. Consider stratified random sampling algorithm instead of manual cherry-picking to reduce bias."}
{"type": "finding", "id": "v1-assumption-hunter-003", "title": "Assumes v3 JSONL data exists and is valid - no verification step", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Architecture", "issue": "Design references 'findings from v3 markdown' and 'full v3 conversion' but doesn't verify v3 source data exists or is in usable format. Assumes v3 findings are documented in parseable markdown structure. No validation that v3 data quality is sufficient for pattern extraction testing.", "why_it_matters": "If v3 markdown is incomplete, inconsistently formatted, or missing key fields, sample JSONL inherits these flaws. Prototype would validate pattern extraction on garbage data. 'Garbage in, garbage out' means validation results are meaningless.", "suggestion": "Add prerequisite verification: Before sample creation, validate v3 source data: (1) confirm docs/reviews/parallax-review-v3/ exists and contains complete findings, (2) spot-check 5 random findings for required fields (title, severity, issue, suggestion), (3) verify finding count matches expected 83. If data quality issues found, defer prototype until v3 data is cleaned."}
{"type": "finding", "id": "v1-assumption-hunter-004", "title": "Manual vs automated conversion tradeoff not examined", "severity": "Minor", "phase": {"primary": "design", "contributing": null}, "section": "Key Decisions", "issue": "Decision #2 states 'Manual sample creation (not full markdown conversion)' justified by 'Prototype goal is validating pattern logic, not conversion tooling.' Assumes manual approach is always faster/better without examining automation benefits. For 15-20 findings, manual may be faster. For 83 findings (full conversion), assumption may not hold.", "why_it_matters": "If manual conversion takes >2 hours due to error-prone JSON formatting, LLM-assisted conversion could be faster and more accurate. Assumption that 'manual = simpler' may be false. Affects scalability to full v3 conversion.", "suggestion": "Add tradeoff analysis: Estimate manual conversion time (5-10 min per finding = 1.5-3 hours for 15-20). Compare to LLM-assisted approach (prompt Claude to convert markdown finding to JSONL, validate output). If manual effort >2 hours, revisit automation decision. Document actual time spent for post-prototype analysis."}
{"type": "finding", "id": "v1-assumption-hunter-005", "title": "Pattern count prediction (8-12 from 15-20 findings) has no justification", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Pattern Extraction Logic", "issue": "Design expects '8-12 patterns from 15-20 findings' but provides no rationale. Is this based on v3 summary analysis? Prior experience? Intuition? Assumes ~1.5-2.5 findings per pattern, but no evidence this clustering ratio is correct.", "why_it_matters": "If expectation is wrong (e.g., only 3-5 patterns emerge, or 15+ patterns), affects interpretation of results. Success criteria reference this range but it's an assumption, not a validated target.", "suggestion": "Add rationale: 'Expected pattern count based on v3 summary identifying 5 critical themes (architectural gaps, assumption violations, edge cases, auto-fix risks, prior art). Sample (24% of findings) should yield 60-80% of themes (3-4 patterns minimum). 8-12 range allows for sub-themes and cross-cutting patterns. If <8 patterns, may indicate over-clustering. If >12, may indicate under-clustering or new themes.'"}
{"type": "finding", "id": "v1-assumption-hunter-006", "title": "Single-pass extraction assumes success - no validation-retry loop", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "Pattern Extraction Logic", "issue": "Design states 'single pass extraction (no iterative refinement)' which differs from FR7.5 requirement that reviewers retry validation failures. Assumes first extraction attempt produces valid, schema-compliant output. No retry mechanism if LLM produces malformed JSON or violates schema.", "why_it_matters": "If pattern extraction produces invalid JSON (e.g., missing required fields, wrong enum values), no recovery path specified. Must manually edit output or re-run extraction? This breaks 'skill-driven workflow' and makes prototype success dependent on perfect first-try LLM output.", "suggestion": "Add validation-retry loop: After pattern extraction, validate output against schema. If validation fails, inspect error, provide error feedback to LLM, retry extraction with clarified prompt (max 3 attempts). This aligns with FR7.5 validation workflow and improves robustness. Document retry count in issue #17 to inform production implementation."}
{"type": "finding", "id": "v1-assumption-hunter-007", "title": "Systemic detection denominator ambiguity - assumes contributing_phase findings only", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "Systemic Detection (Pattern-Based)", "issue": "Design states '>30% of total findings' (line 123) but example calculation shows 5 findings as 33.3% when there are 15 in sample. Math: 5/15 = 33.3%. However, FR2.7 explicitly states 'findings with contributing_phase set to that phase'. This means denominator should be findings with contributing_phase, not all findings. Design doesn't clarify which interpretation is correct.", "why_it_matters": "If prototype uses wrong denominator, it won't validate FR2.7 correctly. Systemic detection results will differ between prototype and production. Requirements update could specify wrong algorithm if based on incorrect prototype implementation.", "suggestion": "Clarify in Systemic Detection section: 'Denominator for systemic threshold is findings with contributing_phase set (per FR2.7), not all findings. Example: if 5 findings have contributing_phase=calibrate in a 15-finding sample, percentage is 5/(findings with contributing_phase) not 5/15. Validate this interpretation against FR2.7 before implementation.'"}
{"type": "finding", "id": "v1-assumption-hunter-008", "title": "Assumes Python + jsonschema installed in execution environment", "severity": "Minor", "phase": {"primary": "design", "contributing": null}, "section": "Schema Validation", "issue": "Design specifies validation using existing 'scripts/validate-schemas.py' which requires Python and jsonschema library. Assumes these are installed and working in Claude Code execution environment. No verification step or fallback if dependencies missing.", "why_it_matters": "If Python or jsonschema not available, schema validation step fails. Blocks prototype completion. Minor severity because CLAUDE.md indicates project has working Python environment, but assumption should be verified.", "suggestion": "Add prerequisite check: Before running validation, verify Python and jsonschema available: 'python --version && python -c \"import jsonschema\"'. If check fails, document dependency installation steps or use alternative validation (jq for basic JSON syntax, online schema validator)."}
{"type": "finding", "id": "v1-assumption-hunter-009", "title": "Assumes Issue #17 provides necessary context for findings interpretation", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Next Steps", "issue": "Design states 'Document findings in issue #17' and 'Summary comment in issue #17 with findings and next steps' but doesn't verify Issue #17 exists or contains relevant context. Assumes issue thread provides problem statement, acceptance criteria, or other context needed to interpret prototype results.", "why_it_matters": "If Issue #17 is minimal (just title 'Pattern extraction prototype') or lacks context, findings documentation will need to be self-contained. Affects how much background to include in summary. However, severity is Minor because issue existence is verifiable before work starts.", "suggestion": "Add step 0 to Next Steps: 'Verify Issue #17 exists and contains: (1) problem statement or link to requirements, (2) acceptance criteria or definition of done, (3) any constraints or assumptions from issue creation. If context missing, add Problem Statement and Success Criteria to issue before starting implementation.'"}
{"type": "finding", "id": "v1-assumption-hunter-999", "title": "Blind spot check: Assumption Hunter perspective", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Meta", "issue": "What might I have missed by focusing on assumptions? Potential blind spots: (1) May have assumed technical assumptions (LLM behavior, data quality) are more important than process assumptions (team workflow, review practices), (2) Focused on implicit assumptions but may have missed explicit assumptions that are wrong (e.g., 'Skill-driven workflow' assumption may not align with actual execution environment limitations), (3) Didn't assess whether some assumptions are reasonable to make in prototype context (e.g., assuming Python available is probably fine for this team).", "why_it_matters": "Blind spot awareness helps catch gaps in the review process. Assumption hunting can over-index on technical risks and miss organizational or process assumptions that are equally important.", "suggestion": "Consider: (1) Are there process or organizational assumptions (e.g., 'user will have time to do manual JSONL conversion', 'team agrees on pattern definition semantics')? (2) Are any explicitly stated assumptions actually wrong (e.g., claims about FR2.7 that don't match requirements)? (3) Which assumptions are reasonable for prototype scope vs production?"}
