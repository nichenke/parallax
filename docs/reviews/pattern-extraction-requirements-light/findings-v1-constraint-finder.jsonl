{"type": "finding", "id": "v1-constraint-finder-001", "title": "No token/cost budget for LLM-driven pattern extraction", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Pattern Extraction Logic", "issue": "LLM-driven pattern extraction has no stated token budget or cost constraint. Project has $2000/month budget (~$200-500 actual spend) but this prototype could consume significant API costs with Claude analyzing 15-20 findings multiple times (input validation + pattern extraction + systemic detection).", "why_it_matters": "Without cost bounds, prototype could exceed budget or require manual intervention. LLM calls for pattern extraction are unbounded - no fallback if first attempt produces poor patterns, no limit on retry attempts.", "suggestion": "Add constraint: 'Pattern extraction limited to single Claude API call per sample set, estimated <$5 per run with prompt caching. Budget: $20 total for prototype (4 iterations max).'"}
{"type": "finding", "id": "v1-constraint-finder-002", "title": "Sample size selection lacks statistical justification", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Sample Finding Selection", "issue": "Design specifies '15-20 findings' but provides no statistical rationale for this range. Why not 10? Why not 30? Selection criteria (severity distribution, reviewer diversity, phase coverage) are qualitative - no formula for determining adequate sample size.", "why_it_matters": "Small samples may miss rare but important patterns. Large samples increase manual conversion effort without proven benefit. No way to validate if 15-20 is sufficient until after prototype completes.", "suggestion": "Document constraint rationale: 'Sample size 15-20 represents 18-24% of 83 total v3 findings. Minimum 15 ensures 2-3 findings per reviewer (6 reviewers). Maximum 20 limits manual JSONL conversion effort to <2 hours. Statistical validity deferred to post-prototype analysis.'"}
{"type": "finding", "id": "v1-constraint-finder-003", "title": "Manual JSONL conversion has unbounded effort", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Architecture", "issue": "Design requires 'handcrafted sample' JSONL creation by manually cherry-picking and converting 15-20 findings from markdown. No time estimate provided, no automation constraint, no quality validation process beyond schema compliance.", "why_it_matters": "Manual conversion is error-prone (typos, invalid JSON, schema violations). Could take hours if findings are complex. No constraint on how much time to spend fixing validation errors. Blocks pattern extraction until sample is perfect.", "suggestion": "Add constraints: 'Manual JSONL conversion estimated <2 hours. If validation fails after 3 attempts, reduce sample size to minimum (15 findings). Consider LLM-assisted conversion if manual effort exceeds 3 hours.'"}
{"type": "finding", "id": "v1-constraint-finder-004", "title": "Schema evolution could invalidate prototype work", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": "design"}, "section": "Schema Validation", "issue": "Prototype depends on two schemas (reviewer-findings v1.0.0, pattern-extraction v1.0.0) but provides no versioning constraints or migration strategy. If schemas change mid-prototype (e.g., findings from requirement updates), all JSONL files become invalid.", "why_it_matters": "Breaking schema changes would require re-doing manual JSONL conversion and re-running validation. No documented constraint on schema stability during prototype window. Risk of wasted work if parallel development touches schema files.", "suggestion": "Add constraint: 'Schema freeze during prototype execution (Issue #17 branch only). No schema changes merged to main until prototype completes. If schema updates required, document migration path in ADR before merging.'"}
{"type": "finding", "id": "v1-constraint-finder-005", "title": "No failure recovery or rollback constraints", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Testing & Success Criteria", "issue": "Success criteria list 6 checkboxes but no failure handling. What if pattern extraction produces 0 patterns? What if systemic detection flags 10 systemic issues? What if schema validation fails? No documented constraints on acceptable failure modes or rollback strategy.", "why_it_matters": "Prototype could produce unexpected results (e.g., all findings collapse into 2 mega-patterns, or 15 single-finding patterns). No decision criteria for 'this didn't work, restart with different approach' vs 'this is unexpected but valid'. Wastes time if failure threshold isn't predefined.", "suggestion": "Add constraints: 'Pattern extraction must produce 4-15 patterns (too few = under-clustering, too many = over-fitting). Systemic detection must flag 0-4 issues (>4 = threshold too low). Schema validation failures: max 3 retry attempts before reducing sample size or abandoning prototype.'"}
{"type": "finding", "id": "v1-constraint-finder-006", "title": "Single-pass implies no concurrent processing constraint", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Pattern Extraction Logic", "issue": "Design states 'single pass extraction (no iterative refinement)' but doesn't clarify if this precludes parallel processing. Could pattern extraction run concurrently for different finding subsets? Is sequential processing required?", "why_it_matters": "If sequential processing is required, it's a constraint on future scaling. If parallelization is allowed, it changes token cost estimates (multiple concurrent API calls). Ambiguity could lead to incorrect assumptions in eval framework.", "suggestion": "Clarify constraint: 'Single-pass = one analysis attempt per finding set (no human-in-loop refinement). Sequential processing required for prototype (one LLM call). Parallelization deferred to post-MVP optimization.'"}
{"type": "finding", "id": "v1-constraint-finder-007", "title": "No context window constraints for full v3 conversion", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "Architecture", "issue": "Prototype uses 15-20 finding sample, but doc mentions 'full conversion can be done later via Opus subagent once approach is validated.' No analysis of whether 83 findings fit in Claude context window, or if chunking strategy is needed.", "why_it_matters": "Claude Opus context window is 200k tokens. 83 findings at ~500 tokens each = ~41.5k tokens (input) + pattern extraction logic + output. Likely fits, but no documented constraint or fallback if it doesn't. Full conversion could fail silently if context limits hit.", "suggestion": "Add constraint: '83 findings estimated <50k input tokens, fits Claude Opus 200k window with margin. If full conversion exceeds context limits, implement chunking strategy (3 batches of ~28 findings, merge patterns post-extraction). Validate token count before full conversion.'"}
{"type": "finding", "id": "v1-constraint-finder-008", "title": "Python schema validation has no performance constraints", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Schema Validation", "issue": "Design uses existing `scripts/validate-schemas.py` but provides no performance constraint. What if JSONL file is large (1000+ findings in future)? Does validator load entire file into memory? Any timeout constraints?", "why_it_matters": "For prototype (15-20 findings), performance is irrelevant. For full v3 conversion (83 findings), likely still fast. But if pattern extraction scales to 500+ findings across multiple reviews, validator could become bottleneck. No documented constraint on acceptable validation time.", "suggestion": "Add constraint (future-looking): 'Schema validation must complete <5 seconds for files up to 1000 findings. Prototype scale (15-83 findings) has no performance constraint. Document validation approach (streaming vs in-memory) if file size becomes concern.'"}
{"type": "finding", "id": "v1-constraint-finder-999", "title": "Blind spot check: Constraint Finder perspective", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Meta", "issue": "What might I have missed by focusing on constraints? Possible blind spots: (1) Assumed project budget from CLAUDE.md applies to this prototype without explicit allocation, (2) Assumed schema versioning is managed elsewhere without checking if v1.0.0 is stable, (3) Focused on technical constraints (cost, performance, data size) but may have missed regulatory constraints (are design findings considered sensitive data?), (4) Assumed manual process constraints matter without checking if user prefers speed over cost optimization.", "why_it_matters": "Constraint reviews can over-index on blocking concerns and miss enabling constraints (e.g., 'must complete in 1 day' vs 'no time pressure'). May have flagged issues that don't matter for prototype scope.", "suggestion": "Consider: Did I assume constraints from domain knowledge? Did I miss non-obvious limits? For prototype scope, time-to-value may outweigh cost optimization. Validate assumption that $20 budget allocation is meaningful vs '$2000/month means prototype cost is negligible'."}
