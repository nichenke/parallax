{"type": "finding", "id": "v1-success-validator-001", "title": "Pattern quality validation relies on unmeasurable 'manual review'", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "Testing & Success Criteria", "issue": "Success criterion 'Patterns identified match expected v3 themes (manual review)' has no objective rubric. What constitutes a 'match'? How many patterns must match? What if some patterns are missing and new ones emerge?", "why_it_matters": "This is the primary validation criterion for whether pattern extraction works, but without measurable criteria, different reviewers could reach opposite conclusions. Blocks objective MVP approval decision.", "suggestion": "Define measurable criteria: 'At least 75% of extracted patterns map to v3 summary themes (architectural gaps, assumption violations, edge cases, auto-fix risks). Each pattern must contain 2+ findings addressing same root cause. Zero patterns with single-finding clusters.'"}
{"type": "finding", "id": "v1-success-validator-002", "title": "Definition of done missing for prototype", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "Next Steps", "issue": "Next steps list 'if validated' and 'if issues found' paths but don't define what 'validated' means. Which success criteria must pass for approval? Can we proceed with 5/6 criteria passing?", "why_it_matters": "Without explicit definition of done, risk of ambiguous validation results leading to iteration loops or premature requirements update. Stakeholders need clear completion criteria.", "suggestion": "Add definition of done: 'Prototype validated when: (1) all 6 checkmark criteria pass, (2) manual pattern review confirms 4/5 v3 critical themes captured, (3) at least 1 systemic issue detected with >30% threshold, (4) zero schema validation errors. Any Critical finding from validation triggers design revision.'"}
{"type": "finding", "id": "v1-success-validator-003", "title": "Systemic detection validation lacks concrete success metrics", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Testing & Success Criteria", "issue": "Success criterion 'Systemic detection flags 1-2 systemic issues' provides a target count but no validation criteria. How do we verify the flagged issues are actually systemic? What if detection flags 0 or 5 issues?", "why_it_matters": "Systemic detection is a core feature (FR2.7). If prototype flags issues that aren't actually systemic, or misses obvious clustering, the algorithm needs revision. Need criteria to distinguish success from failure.", "suggestion": "Add validation criteria: 'For each flagged systemic issue: (1) verify finding_count ≥ 4, (2) confirm findings share same root cause (manual review), (3) verify percentage calculation correct. If 0 issues flagged but v3 had JSONL schema gap (5 findings), detection failed. If >3 issues flagged, review threshold tuning.'"}
{"type": "finding", "id": "v1-success-validator-004", "title": "Pattern extraction success defined by absence of errors, not quality", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Testing & Success Criteria", "issue": "Success criterion 'Pattern extraction completes without errors' only validates technical execution, not output quality. Extraction could complete successfully but produce nonsensical patterns, duplicate patterns, or miss obvious themes.", "why_it_matters": "Prototype goal is validating pattern logic and prompt quality, not just technical execution. Weak success criterion could pass MVP despite poor pattern quality, leading to requirements update based on flawed approach.", "suggestion": "Replace with quality criteria: 'Pattern extraction produces 8-12 patterns with: (1) no duplicate themes, (2) each pattern contains 2+ findings, (3) pattern titles are distinct and descriptive, (4) actionable_next_step is concrete (not vague like 'consider addressing'), (5) phase attribution aligns with finding phases.'"}
{"type": "finding", "id": "v1-success-validator-005", "title": "Expected pattern count range lacks validation threshold", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Pattern Extraction Logic", "issue": "Design expects '8-12 patterns from 15-20 findings' but success criteria don't specify what happens if extraction produces 5 patterns or 15 patterns. Is this validation failure or acceptable variance?", "why_it_matters": "Pattern count is a key quality signal. Too few patterns suggests over-clustering (losing granularity). Too many patterns suggests under-clustering (not identifying systemic themes). Need thresholds to interpret results.", "suggestion": "Add pattern count validation: 'Accept 6-14 patterns (expected 8-12). If <6 patterns, over-clustering detected—review for false pattern merging. If >14 patterns, under-clustering detected—many patterns likely have single finding. Either case triggers prompt revision.'"}
{"type": "finding", "id": "v1-success-validator-006", "title": "Schema validation success criterion has no failure mode handling", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Testing & Success Criteria", "issue": "Success criteria require 'Output validates against pattern-extraction schema' but don't specify what happens if validation fails. Is this a design bug, schema bug, or prompt bug? What's the fix path?", "why_it_matters": "Schema validation failures could indicate multiple root causes. Without diagnostic guidance, could waste time fixing wrong layer (e.g., editing schema when prompt is the issue).", "suggestion": "Add failure mode handling: 'If schema validation fails: (1) check for missing required fields (prompt bug), (2) check for invalid enum values (prompt bug), (3) check for pattern count >15 (logic bug). Schema changes require ADR documentation—default assumption is prompt needs fixing.'"}
{"type": "finding", "id": "v1-success-validator-007", "title": "Sample finding selection criteria lack validation checkpoint", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Sample Finding Selection", "issue": "Selection criteria specify severity distribution, reviewer diversity, and phase coverage but no validation step to confirm sample meets criteria before extraction. Risk of discovering non-representative sample after pattern extraction completes.", "why_it_matters": "If sample violates selection criteria (e.g., only 1 reviewer, no calibrate phase findings), pattern extraction results may not validate the approach. Discovering this post-extraction wastes time.", "suggestion": "Add sample validation checkpoint: 'After creating sample-findings-v3.jsonl, validate: (1) severity distribution within ±2 of target (5-6C, 10-12I, 2-3M), (2) all 6 reviewers represented, (3) at least 3 phases present, (4) 5+ findings with contributing phases. If criteria not met, re-select findings before extraction.'"}
{"type": "finding", "id": "v1-success-validator-008", "title": "Deliverables section missing acceptance criteria", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Testing & Success Criteria", "issue": "Deliverables list 3 artifacts (sample JSONL, patterns JSON, issue comment) but no quality criteria for each. What makes the issue comment 'done'? What level of detail is expected?", "why_it_matters": "Clear deliverable acceptance criteria prevent rework. Without them, issue comment could be 2 sentences or 2 pages, both technically meeting 'summary comment' requirement.", "suggestion": "Add deliverable criteria: 'Sample JSONL: 15-20 findings, passes schema validation. Patterns JSON: 8-12 patterns, passes schema validation. Issue comment: 200-400 words covering: pattern count, systemic issues detected, validation results (pass/fail per criterion), recommended next step (update requirements vs iterate design).'"}
{"type": "finding", "id": "v1-success-validator-009", "title": "Testing workflow lacks post-failure iteration plan", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Testing & Success Criteria", "issue": "Testing workflow describes 4 testing steps but doesn't specify what happens if any step fails. Do we iterate on the prompt? Revise the algorithm? Change the schema? How many iterations are acceptable before abandoning approach?", "why_it_matters": "Prototypes often require iteration. Without an iteration plan, team could spin indefinitely on prompt tuning or prematurely abandon a viable approach after first failure. Need guardrails.", "suggestion": "Add iteration policy: 'If pattern extraction fails: (1) first iteration—revise prompt clarity, (2) second iteration—adjust sample selection, (3) third iteration—revisit pattern definition in design. If 3 iterations don't resolve, flag as design issue requiring requirements reconsideration. Document learnings in ADR regardless of outcome.'"}
{"type": "finding", "id": "v1-success-validator-999", "title": "Blind spot check: Success Validator perspective", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Meta", "issue": "What might I have missed by focusing on success criteria? Potential blind spots: (1) assumed success criteria should be in Testing section—might have missed goals stated elsewhere, (2) focused on measurability but may have missed qualitative success dimensions like 'learnings captured' or 'team alignment', (3) pattern extraction is exploratory research—rigid success criteria could stifle useful negative results.", "why_it_matters": "Blind spot awareness helps catch gaps in the review process. Overly rigid success criteria on a prototype could prevent valuable learnings from unexpected results.", "suggestion": "Consider: (1) Are there qualitative success outcomes beyond the 6 checkmarks? (2) Does prototype need escape hatch for 'validation failed but learned X' success case? (3) Should exploratory prototypes have different success criteria than production features?"}
