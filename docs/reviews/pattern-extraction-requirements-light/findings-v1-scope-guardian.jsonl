{"type":"finding","id":"v1-scope-guardian-001","title":"MVP boundary undefined - no explicit statement of deliverable scope","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"Overview","issue":"Document references 'MVP' multiple times (lines 111, 232) but never explicitly states 'The MVP is...' or lists what constitutes the minimal viable deliverable. Multiple features marked 'post-MVP' or 'deferred' but no positive definition of what IS included.","why_it_matters":"Cannot determine if scope is appropriate without knowing what the MVP actually includes. Implementers must infer MVP from scattered context. Risk of scope confusion: is schema validation part of MVP? Is systemic detection? Is documentation? All mentioned but MVP boundary unclear.","suggestion":"Add 'MVP Definition' section before Architecture stating: 'MVP deliverable is: (1) sample-findings-v3.jsonl (15-20 cherry-picked findings), (2) patterns-v3.json (extracted patterns with systemic detection), (3) schema validation passing, (4) issue #17 summary. Explicitly out of MVP: full v3 conversion, iterative refinement, automated conversion tooling, integration into parallax:review skill.'"}
{"type":"finding","id":"v1-scope-guardian-002","title":"Sample creation scope ambiguous - handcrafted vs cherry-pick effort undefined","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Architecture","issue":"Line 26 states 'Handcrafted sample' but line 50 says 'Cherry-pick 15-20 findings'. Lines 183, 222 repeat 'cherry-pick'. Unclear if 'handcrafted' means manual JSONL writing from scratch or copy-paste from markdown with manual reformatting. Effort estimate differs 10x between approaches.","why_it_matters":"Affects implementation time and skill requirements. Full handcrafting (writing JSON by hand) = high error risk, slow. Cherry-picking with reformatting = faster but requires markdown-to-JSONL conversion logic. Scope boundary unclear: is conversion logic in scope or just manual copying?","suggestion":"Clarify in 'Sample Finding Selection' section: specify exact process. Recommended: 'Cherry-pick means: (1) identify 15-20 findings from v3 markdown by reading docs/reviews/parallax-review-v3.md, (2) manually convert each to JSONL format per reviewer-findings schema, (3) validate each line before aggregating into sample-findings-v3.jsonl. No automated conversion tooling in MVP scope - pure manual copy-edit-validate workflow.'"}
{"type":"finding","id":"v1-scope-guardian-003","title":"Out-of-scope not explicitly stated - what are we NOT doing?","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"(Missing)","issue":"Document lacks 'Out of Scope' or 'Non-Goals' section. Mentions deferrals scattered throughout (iterative refinement line 232, full conversion line 222, requirements update line 149) but no consolidated list of what's explicitly excluded from this effort.","why_it_matters":"Scope creep risk: without explicit out-of-scope list, features might leak in ('while we're at it, let's also...'). Stakeholder confusion: unclear if full v3 JSONL conversion is expected or deferred. Success criteria ambiguity: if automated tooling isn't built, is that a gap or intentional?","suggestion":"Add 'Out of Scope' section after Overview listing: (1) Full v3 conversion to JSONL (all 83 findings) - deferred to post-prototype, (2) Automated markdown-to-JSONL conversion tooling, (3) Iterative pattern refinement based on user feedback, (4) Integration into parallax:review skill as automated synthesis step, (5) Requirements update (FR2.7/FR10 changes) - contingent on prototype validation, (6) Pattern quality metrics (precision/recall of pattern extraction)."}
{"type":"finding","id":"v1-scope-guardian-004","title":"Validation success criterion allows unbounded iteration - manual review risk","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Testing & Success Criteria","issue":"Line 206 success criterion: 'Patterns identified match expected v3 themes (manual review)'. No definition of 'match' threshold, no process for handling mismatches. If patterns don't match, is that failure or iteration trigger? Opens door to unbounded 'let me try again' cycles.","why_it_matters":"Scope creep via iteration: vague success criterion could cause prototype to expand into multi-day refinement effort. Without mismatch handling protocol, unclear if one iteration is MVP or multiple iterations expected. Affects timeline and effort estimate.","suggestion":"Tighten success criterion: replace '(manual review)' with objective threshold. Recommended: 'At least 4 of 5 expected v3 themes (architectural gaps, assumption violations, edge cases, auto-fix risks) appear in extracted patterns. If <4 themes present, document gap in issue #17 as design limitation, do NOT iterate - prototype validates approach even if coverage incomplete. Full coverage expected at scale, not in 15-20 sample.'"}
{"type":"finding","id":"v1-scope-guardian-005","title":"Schema dependency scope impact undefined - what if schemas are broken?","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"Schema Validation","issue":"Lines 157-174 specify validation against existing schemas (reviewer-findings-v1.0.0, pattern-extraction-v1.0.0) but don't state scope impact if schemas are broken, missing fields, or incompatible with v3 data. Assumes schemas are correct but MEMORY.md notes schemas implemented in Session 13 - may not have been tested with v3 findings yet.","why_it_matters":"Blocking dependency risk: if schemas don't validate v3 data structure, prototype could stall. Unclear if schema fixes are in scope or require separate effort. Affects timeline if schema iteration needed before prototype can proceed.","suggestion":"Add to Schema Validation section: 'If schema validation fails due to schema issues (not data issues), schema fixes are IN SCOPE for this prototype - update schemas/reviewer-findings-v1.0.0.schema.json as needed to accommodate v3 finding structure. Document any schema changes in issue #17 and update schema version if breaking changes required (v1.0.0 â†’ v1.1.0).'"}
{"type":"finding","id":"v1-scope-guardian-006","title":"Success criteria mix technical and subjective measures without weighting","severity":"Minor","phase":{"primary":"calibrate","contributing":null},"section":"Testing & Success Criteria","issue":"Lines 202-208 list 6 success criteria mixing objective (schema validation passes, extraction completes) and subjective (patterns match themes, systemic detection flags 1-2 issues). Unclear if all criteria required or best-effort. No prioritization: is schema validation more critical than theme matching?","why_it_matters":"Ambiguous pass/fail determination: if 5 of 6 criteria pass, is prototype successful? Could cause disagreement on whether to proceed to requirements update or iterate. Affects decision gate clarity.","suggestion":"Add criterion prioritization: 'MUST PASS: sample JSONL validates, extraction completes, output validates, documentation updated. BEST EFFORT: pattern-theme matching (document gaps if incomplete), systemic detection count (validate threshold logic works, exact count not critical). Prototype succeeds if all MUST PASS criteria met, proceed to requirements update even if best-effort criteria show gaps.'"}
{"type":"finding","id":"v1-scope-guardian-007","title":"Systemic detection threshold scope unclear - testing vs tuning boundary","severity":"Minor","phase":{"primary":"calibrate","contributing":null},"section":"Systemic Detection (Pattern-Based)","issue":"Lines 119-147 specify systemic detection algorithm with >30% threshold (line 123) but 'Open Questions' (line 252) asks if threshold should be adjusted. Unclear if threshold tuning is in prototype scope or deferred. Testing workflow (line 193) says 'confirm threshold logic works' but doesn't specify tuning scope.","why_it_matters":"Scope boundary ambiguity: if prototype finds 0 systemic issues (or 5+), unclear if that triggers threshold adjustment within this effort or deferred to later. Could expand scope into parameter tuning exercise beyond validation.","suggestion":"Clarify in Systemic Detection section: 'Threshold (>30%, 4+ findings) is fixed for MVP prototype - no tuning in scope. If results show 0 or excessive systemic flags, document in issue #17 as tuning recommendation for future work. Prototype validates threshold LOGIC (computation works correctly), not threshold VALUE (optimal number).'"}
{"type":"finding","id":"v1-scope-guardian-008","title":"Requirements update scope conditional but no decision gate defined","severity":"Minor","phase":{"primary":"calibrate","contributing":null},"section":"Next Steps","issue":"Line 242 states 'If validated: Update FR2.7/FR10 in requirements' but doesn't define validation gate. Line 243 offers alternative 'If issues found: Iterate on design' but no criteria for which path to take. Success criteria (lines 202-208) don't map to this decision.","why_it_matters":"Decision ambiguity: unclear who decides if prototype 'validated' or 'found issues'. Could cause scope expansion if 'iterate on design' becomes default path. Affects whether requirements update is in this effort's scope or separate follow-up.","suggestion":"Add to Next Steps: define decision gate. Recommended: 'Validation gate: if all MUST PASS criteria met (see Testing & Success Criteria), proceed to requirements update within this effort. Requirements update includes: (1) ADR documenting pattern-based systemic detection approach, (2) FR2.7/FR10 clarification changes. If MUST PASS criteria fail, requirements update deferred - close issue #17 documenting what failed and recommended design changes for future attempt.'"}
{"type":"finding","id":"v1-scope-guardian-999","title":"Blind spot check: Scope Guardian perspective","severity":"Minor","phase":{"primary":"calibrate","contributing":null},"section":"Meta","issue":"Focusing on scope boundaries may miss: (1) implicit scope in technical decisions (e.g., 'LLM-driven extraction' line 228 implies prompt engineering in scope but not stated), (2) scope assumptions from context (MEMORY.md mentions Session 14 prototype but this doc doesn't reference that prior work - unclear if lessons learned inform this scope), (3) temporal scope (timeline/effort estimate completely absent - is this 2-hour task or 2-day task?).","why_it_matters":"Blind spot awareness: may have over-focused on explicit scope sections and missed implicit scope embedded in technical approach. Absence of timeline in scope definition could cause misalignment on effort expectations.","suggestion":"Consider adding: (1) Effort estimate section (e.g., 'Expected timeline: 4-6 hours for sample creation + extraction + validation + documentation'), (2) Reference to Session 14 learnings if applicable (or state 'This is first pattern extraction attempt'), (3) Explicit list of in-scope activities in Key Decisions or Architecture (e.g., 'Prompt engineering for pattern extraction IS in scope - expect 1-2 iterations on extraction prompt to achieve theme matching')."}
