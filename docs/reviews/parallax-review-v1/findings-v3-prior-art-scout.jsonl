{"type":"finding","id":"v3-prior-art-scout-001","title":"Design Doc Sync Addresses V2 Documentation Debt But Misses Architectural Gaps","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Entire design document","issue":"V2 identified 10 accepted findings from iteration 1 not reflected in design doc. User backported 23 dispositions (per MEMORY.md), addressing documentation debt. However, architectural specification gaps flagged as Critical in V2 remain unaddressed: (1) JSONL output format (V2 Finding 9, flagged by 4 reviewers), (2) Auto-fix mechanism (V2 Finding 2), (3) Cross-iteration finding tracking (V2 Finding 3, flagged by 3 reviewers), (4) Prompt caching architecture (V2 Finding 4). These are structural decisions affecting storage, parsing, and implementation substrate—not cosmetic documentation updates.","why_it_matters":"V2 verdict was \"revise\" specifically because \"these aren't new discoveries requiring redesign—they're accepted decisions not yet documented.\" Design doc sync task addressed disposition notes (decisions) but not architectural specifications (how those decisions manifest in the system). Without JSONL schema, auto-fix workflow, finding ID mechanism, and prompt structure specifications, the design remains incomplete for implementation. This is the difference between \"we decided to do X\" (documented) and \"here's how X works\" (still missing).","suggestion":"Add four architectural specification sections to design: (1) \"Output Format Architecture\" specifying JSONL schema, field definitions, markdown rendering relationship, and migration path. (2) \"Auto-Fix Workflow\" specifying classification criteria, agent responsibilities, validation, and commit strategy. (3) \"Finding Persistence Mechanism\" specifying ID generation algorithm (semantic matching via LLM per V2 Finding 7 suggestion, not brittle hashing), status tracking, and cross-iteration diff format. (4) \"Reviewer Prompt Architecture\" specifying stable prefix structure (persona + methodology + format + voice), variable suffix structure (artifacts + iteration context), cache boundary, and versioning strategy. These specifications enable implementation; disposition notes alone do not."}
{"type":"finding","id":"v3-prior-art-scout-002","title":"Build-vs-Leverage Evaluation Still Deferred to \"When Limits Hit\"","severity":"Critical","phase":{"primary":"calibrate","contributing":"design"},"section":"Implicit throughout (no leverage evaluation section in design)","issue":"CLAUDE.md states \"BUILD adversarial review (novel), LEVERAGE LangGraph + Inspect AI + Claude Code Swarms (mature).\" V2 Prior Art Scout flagged Inspect AI (V2 Finding 13, Critical) and LangGraph (V2 Finding 14, Critical) as solving 80% of custom-built infrastructure. Design doc states \"LangGraph: Start native, evaluate when limits hit\" and has no Inspect AI evaluation. This defers leverage decisions to implementation phase after custom orchestration is already built. Standard practice: evaluate build-vs-leverage during design, not after building.","why_it_matters":"Custom-building parallel reviewer dispatch, finding consolidation, retry logic, timeout handling, result aggregation, state management, and human-in-the-loop gates is 40-60% of implementation surface area. If Inspect AI or LangGraph provides these as battle-tested primitives, parallax becomes domain-specific prompt engineering (20% of work, 100% of novel contribution) rather than infrastructure + prompts (80% plumbing + 20% novel). \"Evaluate when limits hit\" means building custom infrastructure first, discovering it's harder than expected, then evaluating alternatives when already committed. This is backwards—prototyping on existing frameworks validates feasibility before custom work.","suggestion":"Add \"Implementation Substrate Evaluation\" section to design specifying: (1) Prototype reviewer dispatch using Inspect AI's `multi_agent` pattern (1-2 hour spike), validate whether it supports parallel execution, retry/timeout, result collection. If insufficient, document specific gaps. (2) Prototype finding processing using LangGraph state graphs (1-2 hour spike), validate whether `interrupt` for human gates and `state` for disposition tracking meet requirements. If insufficient, document gaps. (3) Decision matrix: if both frameworks meet 80%+ of requirements, use them (reduce maintenance burden). If critical gaps exist, custom-build only the missing 20% as extensions. (4) Document evaluation results in design before committing to implementation approach. This follows CLAUDE.md's \"prototype-first\" philosophy—build to understand, don't design to death. Two 2-hour spikes de-risk the entire architecture."}
{"type":"finding","id":"v3-prior-art-scout-003","title":"Inspect AI Multi-Agent Patterns Are Production-Grade (Not Experimental)","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"V2 Finding 13 disposition, overall architecture","issue":"V2 Prior Art Scout flagged Inspect AI as solving orchestration (V2 Finding 13, Critical). Design doc doesn't reference Inspect AI. Web search reveals Inspect AI (2026) is production infrastructure: UK AI Safety Institute project, supports agent evaluations with flexible built-in agents, multi-agent primitives with handoff, arbitrary external agent support (Claude Code, Codex CLI), comprehensive trace analysis. 89% of organizations have implemented observability for agents, 62% have detailed tracing. Inspect provides exactly what parallax:review needs: multi-agent dispatch, tool-use evaluation, trajectory scoring, state tracking. This is not experimental—it's industry standard for agent evaluation in 2026.","why_it_matters":"Treating Inspect AI as \"future consideration\" ignores that evaluation frameworks have matured beyond simple metrics to comprehensive systems capturing reasoning quality, tool selection, and multi-agent coordination. Parallax:review IS an evaluation task—it evaluates design quality using multi-agent patterns. Inspect AI was designed for this exact use case. Not evaluating it before custom-building orchestration is building blindfolded when a standard solution exists.","suggestion":"Run Inspect AI prototype immediately (before finalizing design). Minimal test: implement 2 reviewer personas (Assumption Hunter, Edge Case Prober) as Inspect solvers, dispatch them in parallel against parallax's own design doc, collect findings via Inspect's trace system. Validate: (1) Does `multi_agent` pattern support parallel dispatch with progress tracking? (2) Does retry/timeout work as expected? (3) Can finding consolidation use Inspect's scorer API? (4) Does cost tracking integrate with Inspect's built-in telemetry? If yes to 3+, adopt Inspect as substrate. If no, document specific blockers and architect around them. Budget: 3-4 hours for spike. Payoff: de-risks 40-60% of implementation."}
{"type":"finding","id":"v3-prior-art-scout-004","title":"LangGraph Human-in-the-Loop Is First-Class (Not Afterthought)","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"UX Flow (Step 5: Process Findings), V2 Finding 14 disposition","issue":"V2 Prior Art Scout flagged LangGraph for stateful workflow and human gates (V2 Finding 14, Critical). Design doc states \"evaluate LangGraph when limits hit\" but doesn't specify what \"limits\" would trigger evaluation. Web search reveals LangGraph (2026) treats human-in-the-loop as first-class: workflows pause for manual oversight while retaining full context, persistence saves state after every step, `interrupt` feature pauses graphs waiting to be resumed, checkpointers save StateSnapshots at every super-step. This is exactly what parallax:review's interactive finding processing requires: pause for human review, persist disposition state, resume from exact position after interruption.","why_it_matters":"V2 identified state management gaps: Finding 21 (Important, crash/interruption loses progress), Finding 18 (Important, no resumption specification for async mode), Finding 19 (Important, timestamped folders break git diff workflow), Finding 28 (Important, single-user assumption, no multi-user support). All four findings are state management problems. LangGraph solves state management as its core value proposition—central memory that every step reads/updates, persistence across interruptions, explicit checkpointing. Custom-building this means reimplementing what LangGraph provides as production-grade primitives.","suggestion":"Same as Finding 2—prototype LangGraph for finding processing workflow. Minimal test: model review process as state graph (dispatch → synthesize → process findings loop → wrap up). Validate: (1) Does `state` properly track finding dispositions across processing loop? (2) Does `interrupt` pause at human gates without losing context? (3) Can user close terminal and resume processing later? (4) Does persistence enable multi-user workflows (Alice processes findings 1-5, Bob continues from 6)? If yes to 3+, use LangGraph for control flow. Budget: 2-3 hours. Payoff: solves 4 Important findings from V2."}
{"type":"finding","id":"v3-prior-art-scout-005","title":"JSONL Format Decision Needs Schema Specification (Not Just Intent)","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Output Artifacts, V2 Finding 9","issue":"V2 Finding 9 (Critical, 4 reviewers) flagged JSONL format missing from design. MEMORY.md states \"JSONL as canonical output format—decided but not yet implemented.\" Design doc sync (23 dispositions) didn't add JSONL specification. JSONL is not a future enhancement—it's foundational architecture affecting: (1) how finding IDs are generated and stored (JSON `id` field vs text parsing), (2) how dispositions are tracked (structured `status`/`disposition` fields vs markdown prose), (3) how tools consume output (jq filtering vs grep/awk), (4) how cross-iteration tracking works (JSON diffing vs LLM parsing). Deciding \"we'll use JSONL\" without specifying schema defers the hard design questions.","why_it_matters":"Current design commits to markdown output format throughout. If JSONL is the intended canonical format, every section describing output format is specifying transitional scaffolding, not the actual system. Prototyping in markdown then migrating to JSONL later requires either (1) rewriting all file I/O and parsing, or (2) maintaining two output formats indefinitely. Schema decisions (field names, nesting structure, relationship encoding) affect how findings are deduplicated, how severity ranges are represented, how phase classification is stored (single-label vs multi-label), and how cross-iteration diffs are computed.","suggestion":"Add \"Output Format Architecture\" section specifying JSONL schema even if implementation is deferred. Minimal schema: one JSON object per finding with fields `{id, iteration, reviewer, severity, phase_primary, phase_contributing, section, issue, why_it_matters, suggestion, disposition, disposition_note, timestamp}`. Specify: (1) Finding ID generation (semantic hash or LLM-based matching—see Finding 7 for hash brittleness), (2) Severity range encoding (array `[\"Critical\", \"Important\"]` when reviewers disagree), (3) Phase classification encoding (separate fields for primary/contributing or nested object), (4) Cross-iteration status (`open`/`addressed`/`rejected`/`partial`), (5) Markdown rendering relationship (JSONL is storage, markdown is human-readable view generated from JSONL, not separate format). Clarify whether reviewers output JSONL (affects prompt complexity) or synthesizer converts markdown to JSONL (affects parsing logic)."}
{"type":"finding","id":"v3-prior-art-scout-006","title":"Auto-Fix Criteria Conservative But Implementation Workflow Unsafe","severity":"Important","phase":{"primary":"design","contributing":null},"section":"V2 Finding 2 (auto-fix requirement), Synthesis section (should include auto-fix step)","issue":"V2 Finding 2 (Critical) flagged auto-fix requirement missing. Design doc sync didn't add auto-fix specification. Auto-fix was accepted in V2 with disposition: \"Auto-fix step between synthesis and human processing. Git history must show auto-fixes as separate commit.\" V2 Finding 20 (Important) flagged git safety concern: auto-fix modifies design doc before user sees findings, conflicts with Git Safety Protocol (\"NEVER commit changes unless user explicitly asks\"). These findings are in tension—auto-fix before human review enables automatic application but violates consent; auto-fix after human review requires user to pre-approve fixes without seeing them applied.","why_it_matters":"Auto-fix is high-value for obvious corrections (typos, broken internal links, path corrections) but dangerous if applied without user visibility. Standard pattern for auto-fix: show user proposed changes as diff/patch, get approval, then apply. This requires specification in design: (1) how fixes are presented (inline diff, separate file, patch format), (2) when user approves (before or after seeing other findings), (3) how approval is captured (interactive prompt, config flag, explicit command), (4) how fixes are validated before committing (subset of reviewers re-check auto-fixed sections).","suggestion":"Add \"Auto-Fix Workflow\" section to design specifying conservative MVP approach: (1) Synthesizer classifies findings as auto-fixable using strict criteria (typos in prose only—no code, paths, or structural changes), (2) Auto-fixable findings presented to user as \"can be auto-fixed\" flag in summary, (3) User explicitly approves auto-fix set before application (\"Apply 7 auto-fixes? [y/n]\"), (4) Fixes applied to design doc copy (not original), diff shown for review, (5) User confirms, then fixes committed as separate git commit with message listing fixed finding IDs, (6) Re-review triggered automatically on auto-fixed sections only (subset of reviewers, focused scope). This balances automation value with git safety and user consent. Expand auto-fix criteria based on eval data showing false-positive rate is acceptable."}
{"type":"finding","id":"v3-prior-art-scout-007","title":"Cross-Iteration Finding Tracking Needs LLM-Based Matching (Hashing Insufficient)","severity":"Important","phase":{"primary":"design","contributing":null},"section":"V2 Finding 3 (cross-iteration tracking), V2 Finding 7 (text-hash brittleness)","issue":"V2 Finding 3 (Critical, 3 reviewers) flagged cross-iteration tracking mechanism incomplete. V2 Finding 7 (Critical) flagged that hash-based finding IDs break when finding text is refined (\"Parallel agent dispatch has no retry logic\" in iteration 1 becomes \"Parallel agent retry logic lacks exponential backoff\" in iteration 2—same concern evolving, but different hashes). Design doc sync didn't add finding persistence specification. Text hashing is brittle for iterative review because reviewers naturally rephrase findings as design improves. Exact text matching fails precisely when iteration tracking is most valuable.","why_it_matters":"Finding persistence enables two high-value features: (1) showing user which prior findings are resolved vs still open vs new (iteration diff), (2) focusing reviewer scrutiny on sections that changed (via git diff). Without stable IDs, \"12 new findings\" is ambiguous—are these truly new concerns or rephrased versions of prior findings? User wastes time cross-referencing manually. Hash-based IDs optimize for implementation simplicity at cost of UX quality.","suggestion":"Add \"Finding Persistence Mechanism\" section specifying LLM-based semantic matching (not hashing). Process: (1) On re-review, synthesizer receives prior summary with all findings from iteration N, (2) For each new finding from iteration N+1, synthesizer asks \"does this semantically match any prior finding?\" using LLM comparison, (3) If match found, assign same finding ID and mark status (`addressed` if issue resolved, `persists` if still present, `refined` if partially addressed), (4) If no match, assign new finding ID, (5) Cross-iteration diff in summary shows: \"Iteration 2 resolved Findings 1, 3, 5 from iteration 1. Findings 2, 4 persist with updated details. 8 new findings introduced.\" This is more token-expensive than hashing but provides correct semantics. Alternatively, use hybrid: hash for exact matches (fast path), LLM matching for non-exact (quality path). Validate approach during Second Brain test case (known ground truth of which findings should match across iterations)."}
{"type":"finding","id":"v3-prior-art-scout-008","title":"Prompt Caching Architecture Underspecified (Affects Cost Model)","severity":"Important","phase":{"primary":"design","contributing":null},"section":"V2 Finding 4 (prompt caching missing), Reviewer Prompt Architecture (should exist)","issue":"V2 Finding 4 (Critical) flagged prompt caching architecture not addressed. Requirements doc states \"This is an architectural convention, not a feature—90% input cost reduction on cache hits.\" Design doc sync didn't add prompt architecture section. Design doc has one paragraph in \"Reviewer Prompt Architecture\" stating prompts are structured for future caching with stable prefix (persona + methodology + format + voice) and variable suffix (artifacts + iteration context), but defers optimization \"until prompts stabilize post-prototype.\" This treats caching as performance optimization when requirements frame it as cost strategy.","why_it_matters":"Prompt caching is Anthropic-specific feature requiring specific prompt structure. If prompts aren't architected for caching from start, refactoring later invalidates eval data (changing prompts changes review quality). CLAUDE.md cost strategy explicitly relies on prompt caching for 90% cost reduction—this is not optional for budget viability. Additionally, prompt versioning affects cache invalidation: minor tweaks to stable prefix invalidate cache and spike costs. Without explicit cache boundary specification in design, implementers won't know what belongs in prefix (cacheable, changes rarely) vs suffix (per-review, never cached).","suggestion":"Expand \"Reviewer Prompt Architecture\" section to specify: (1) Stable prefix exact structure (in order: persona identity and mandate, review methodology steps, output format rules with examples, voice guidelines, blind spot check instructions), (2) Variable suffix exact structure (design artifact, requirements artifact, prior review summary if re-review, iteration number, git diff if available), (3) Cache boundary delimiter (explicit marker showing where prefix ends and suffix begins—required by Anthropic caching API), (4) Prompt versioning (semantic versioning for prefix: major version = methodology change, minor version = wording refinement, patch = typo fix; major/minor changes invalidate cache), (5) Version tracking in output (summary.md records prompt version per reviewer to correlate finding quality with prompt changes). Note: this specification enables eval framework to measure cache hit rate and cost-per-review accurately."}
{"type":"finding","id":"v3-prior-art-scout-009","title":"LangSmith Annotation UI Is Standard Solution for Finding Processing","severity":"Important","phase":{"primary":"design","contributing":"calibrate"},"section":"Step 5: Process Findings, V2 Finding 44 (LangSmith for annotation)","issue":"V2 Prior Art Scout Finding 44 (Important) flagged LangSmith annotation UI as solving finding processing UX. Design doc sync didn't add LangSmith evaluation. Design custom-builds CLI-based interactive finding processing (accept/reject one-at-a-time) with state stored in markdown. LangSmith (already in tooling budget, 5k traces/month free tier) provides production annotation queues with web UI for human feedback, finding tagging by severity/phase, accept/reject/comment workflows, team collaboration, historical disposition tracking. This is standard solution for human-in-the-loop LLM workflows.","why_it_matters":"V2 identified UX limitations in custom CLI processing: no async mode (Finding 42), no resumption spec (Finding 18), state loss on crash (Finding 21), single-user assumption (Finding 28), orphaned findings in Critical-first mode (Finding 25). All five findings are UX problems stemming from CLI-based stateful interaction. LangSmith solves these as core features: web UI enables async (user processes findings hours later), persistent state enables resumption, database storage prevents loss, team features enable multi-user, filtering enables partial processing without orphaning. Adopting LangSmith shifts finding processing from custom implementation to configuration.","suggestion":"Prototype LangSmith for finding processing. Test: (1) Each reviewer run becomes LangSmith trace with metadata (reviewer name, iteration, timestamp), (2) Each finding becomes annotation attached to trace with tags (severity, phase, section), (3) User processes findings in LangSmith web UI: accept/reject with notes, filter by severity/reviewer, bulk operations, (4) Skill reads back dispositions from LangSmith API to update summary.md. Validate: does workflow feel natural? Does web UI add friction vs CLI? Does team collaboration work (multiple people processing same review)? If LangSmith UX fits, adopt it and eliminate custom processing UI. If not, document specific gaps and build custom. Budget: 2-3 hours to prototype. Payoff: solves 5 Important findings from V2."}
{"type":"finding","id":"v3-prior-art-scout-010","title":"Braintrust Scorers Are Standard Solution for Synthesis Tasks","severity":"Important","phase":{"primary":"design","contributing":"calibrate"},"section":"Synthesis responsibilities, V2 Finding 45 (Braintrust for severity normalization)","issue":"V2 Prior Art Scout Finding 45 (Important) flagged Braintrust LLM-as-judge for severity normalization and deduplication. Design doc sync didn't add Braintrust evaluation. Design treats synthesizer as custom prompt engineering problem: deduplicate findings (semantic similarity task), reconcile severity ranges (consensus scoring task), classify by phase (multi-class classification task). Braintrust (already in tooling budget, 1M spans/10k scores free tier) is specifically designed for LLM-as-judge evaluation with autoevals library, chain-of-thought reasoning for assessments, and configurable scorers.","why_it_matters":"Synthesizer responsibilities from design: (1) judge if two findings are \"the same issue\" (requires semantic comparison + similarity threshold), (2) reconcile severity when reviewers disagree (requires consensus logic), (3) assign phase classification (requires multi-label classification with reasoning), (4) detect systemic patterns (requires clustering/aggregation). All four are LLM-as-judge tasks. Custom implementation means writing scorer prompts, managing model calls, handling edge cases, tracking confidence. Braintrust provides battle-tested scorers as reusable components.","suggestion":"Prototype Braintrust for synthesis. Test: (1) Each reviewer output becomes Braintrust trace, (2) Deduplication scorer compares pairs of findings using semantic similarity (cosine similarity on embeddings or LLM-based comparison), threshold at 0.8+ for \"same issue\", (3) Severity scorer reconciles ranges using majority vote or conservative max, (4) Phase classification scorer assigns primary + contributing phases using chain-of-thought, (5) Synthesizer aggregates scorer results into summary.md. Validate: does scorer API provide needed flexibility? Are built-in scorers sufficient or do custom scorers need development? Does reasoning transparency (chain-of-thought) help debug synthesis errors? Budget: 3-4 hours. Payoff: synthesis becomes configuration of scorers rather than custom prompt engineering."}
{"type":"finding","id":"v3-prior-art-scout-011","title":"TRiSM Frameworks Provide Adversarial Resilience Standards","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Reviewer personas (security/adversarial focus)","issue":"2026 Trust, Risk, and Security Management (TRiSM) frameworks (Wiley review, January 2026) provide five foundational pillars for AI lifecycle: explainability with drift monitoring, ModelOps governance, application security, data protection/privacy, and adversarial resilience. Design doc specifies 6-9 reviewer personas but doesn't map to industry-standard adversarial testing frameworks. Parallax:review reviews design artifacts, not LLM outputs, but the adversarial lens is transferable—both seek to find flaws before production.","why_it_matters":"TRiSM frameworks represent industry consensus on what constitutes comprehensive adversarial review in 2026. Parallax's persona selection (Assumption Hunter, Edge Case Prober, etc.) was derived ad-hoc. If TRiSM pillars map cleanly to design review concerns, adopting standard taxonomy improves persona coverage and enables cross-industry comparison. Example mapping: explainability → Assumption Hunter (unstated assumptions), ModelOps governance → Feasibility Skeptic (implementation complexity), application security → (missing persona for security review at design stage), data protection → (missing persona for privacy/compliance), adversarial resilience → Edge Case Prober (failure modes).","suggestion":"Add TRiSM mapping to \"Reviewer Personas\" rationale section. For each pillar, identify which personas cover it and whether gaps exist. If security and privacy personas are missing from design-stage review, add them or document explicit choice to defer to plan-stage review. This grounds persona selection in industry standards rather than intuition."}
{"type":"finding","id":"v3-prior-art-scout-012","title":"Cloud Security Alliance Agentic AI Controls Apply to Parallax Architecture","severity":"Important","phase":{"primary":"design","contributing":"plan"},"section":"Reviewer capabilities, security boundaries","issue":"Cloud Security Alliance Singapore published (2026) draft addendum to Securing AI Systems guide targeting agentic AI with practical controls: risk assessment hardening, asset and secret hygiene, authentication/authorization, limiting agency, segmentation, secure inter-agent communication, monitoring/logging, human-in-loop, and vulnerability disclosure. Parallax:review is an agentic AI system (6+ agents with tool access orchestrated to review designs). Design doc has one sentence on reviewer capabilities (\"read-only, no write access\") but doesn't address CSA controls.","why_it_matters":"Security controls for agentic AI are emerging industry standards. Parallax agents have tool access (file reading, potentially `gh`, `curl`, `git`), inter-agent communication (findings passed to synthesizer), and human-in-loop gates. Without explicit security design: (1) reviewers could access files outside repo (no segmentation), (2) malicious input in design doc could prompt-inject reviewers (no input validation), (3) agents could exfiltrate data via tool calls (no monitoring), (4) credentials could leak in findings (no secret hygiene). These are standard agentic AI risks.","suggestion":"Add \"Security Architecture\" section to design addressing CSA controls relevant to parallax:review: (1) Limiting agency: reviewers have read-only file access scoped to repo, no network access except Prior Art Scout (least privilege), (2) Segmentation: reviewers cannot access parent directories, credentials files, or system paths outside workspace, (3) Secret hygiene: findings must not include content from .env, secrets.json, or credential files (filtering layer in synthesis), (4) Monitoring: all tool calls logged with parameters for audit trail, (5) Human-in-loop: findings are reviewed before any action taken (already in design), (6) Input validation: design/requirements artifacts scanned for prompt injection patterns before feeding to reviewers. Many of these controls may already exist in Claude Code's MCP server boundaries—document explicit reliance on platform security vs parallax-specific controls."}
{"type":"finding","id":"v3-prior-art-scout-013","title":"Multi-Agent Observability Is Industry Standard (62% Have Detailed Tracing)","severity":"Important","phase":{"primary":"design","contributing":"plan"},"section":"Cost tracking, parallel agent failure handling","issue":"Web search reveals 89% of organizations have implemented observability for agents, 62% have detailed tracing inspecting individual agent steps and tool calls (2026 industry data). Parallax:review dispatches 6 agents in parallel, consolidates findings, and processes them iteratively. Design doc specifies timeout/retry/partial results handling but has no observability specification beyond \"cost logging per review run in JSONL output.\"","why_it_matters":"Without detailed tracing, debugging parallax:review failures is opaque. If Assumption Hunter produces low-quality findings, is it prompt issue, model issue, or input artifact issue? If synthesizer misclassifies findings by phase, which step in synthesis logic failed? If review takes 8 minutes instead of expected 3 minutes, which reviewer was slow? Observability enables: (1) debugging (trace why finding was flagged), (2) optimization (identify bottleneck agents), (3) cost attribution (which reviewer consumed most tokens), (4) quality analysis (correlate finding quality with reviewer reasoning traces). Inspect AI and Braintrust both provide trace collection as built-in features. If parallax custom-builds orchestration without observability, it lacks production-grade debugging that frameworks provide for free. LangSmith provides trace visualization with nested spans showing agent interactions, tool calls, and decision points.","suggestion":"Add \"Observability and Tracing\" section to design specifying: (1) Each reviewer execution emits structured trace (start time, input tokens, output tokens, elapsed time, tool calls made, findings generated, errors encountered), (2) Synthesizer execution emits trace (deduplication decisions with reasoning, severity reconciliation with input/output, phase classification with justification), (3) Review run produces trace collection in JSONL (enables post-hoc analysis), (4) If using Inspect AI or Braintrust, rely on their tracing; if custom, implement minimal structured logging. (5) Eval framework uses traces to measure: reviewer coverage overlap, synthesis accuracy, cost per finding, time per reviewer. This is standard practice per 2026 industry data—not optional."}
{"type":"finding","id":"v3-prior-art-scout-014","title":"Mitsubishi Adversarial Debate Validates Stance-Based Review (Not Just Coverage)","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"V2 Finding 11 (adversarial pairs), Reviewer personas","issue":"Mitsubishi Electric announced (January 2026) multi-agent adversarial debate AI using argumentation framework where expert AI agents compete to derive better conclusions through debate. This validates parallax's adversarial review hypothesis but highlights gap: Mitsubishi's agents have incompatible worldviews forced to reconcile (true adversarial debate), whereas parallax's personas are coverage-based (inspecting different domains, not opposing each other). V2 Finding 11 (Critical) flagged this: \"adversarial review is misnamed—this is comprehensive coverage, not opposition.\" Design doc sync didn't address this.","why_it_matters":"If adversarial tension (forcing incompatible positions to reconcile) produces better decisions than comprehensive coverage (more inspectors examining different aspects), parallax's persona design is suboptimal. Mitsubishi's approach: agents argue FOR incompatible conclusions, synthesis must reconcile. Parallax's approach: agents examine different aspects, all can be right simultaneously. The former surfaces deep tensions, the latter improves breadth. Both are valuable but different mechanisms.","suggestion":"Run empirical test during Second Brain validation: (1) Coverage-based review (current design): 6 personas examine different aspects, (2) Stance-based review (Mitsubishi pattern): 3 pairs of opposing personas argue incompatible positions (Optimizer vs Hardener, User-Centric vs Operator-Centric, Ship-Fast vs Ship-Safe), synthesizer reconciles debates. Compare: which approach catches more design flaws? Which produces more actionable findings? Which is more expensive (token cost)? If stance-based review produces significantly better results, redesign personas. If comparable, document that coverage-based is simpler and equally effective."}
{"type":"finding","id":"v3-prior-art-scout-015","title":"Compound Engineering Learning Loop Is Exact Prior Art (Should Study Before Building)","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"V2 Finding 47 (Compound Engineering), calibration feedback loop","issue":"V2 Prior Art Scout Finding 47 (Important) flagged Compound Engineering (EveryInc, 8.9k stars) as exact prior art for learning loops where review findings feed back to improve prompts. Design doc sync didn't address this. Requirements doc describes \"correction compounding\" where false negatives/positives become permanent calibration rules in reviewer prompts. Compound Engineering implements exactly this: Plan → Work → Review → Compound → Repeat, where Review captures learnings and Compound feeds them back.","why_it_matters":"Compound Engineering demonstrates learning loops are viable and valuable in production. They've solved implementation problems parallax will encounter: how to structure calibration data, how to version prompts when learnings accumulate, how to measure whether learnings improve review quality, how to prevent prompt bloat as rules accumulate. Not studying their implementation before building parallax's calibration means reinventing solutions to known problems.","suggestion":"Study Compound Engineering's learning loop implementation before prototyping parallax calibration. Key research questions: (1) How do they capture learnings from review cycles (structured format, manual curation, LLM extraction)? (2) Where do learnings go in prompt (stable prefix, separate knowledge base, examples)? (3) How do they version prompts as learnings accumulate (prevent prompt bloat)? (4) How do they measure whether learnings improved quality (regression testing, A/B comparison)? (5) What's their prompt update workflow (manual review, automated injection, git-based versioning)? Document answers in design. If Compound Engineering already solved this well, consider: propose collaboration where parallax contributes design-stage reviewers to their ecosystem rather than building competing infrastructure."}
{"type":"finding","id":"v3-prior-art-scout-016","title":"adversarial-spec Multi-LLM Debate Pattern Requires Iteration Design","severity":"Important","phase":{"primary":"design","contributing":null},"section":"V2 Finding 48 (adversarial-spec study), Iteration loops","issue":"V2 Prior Art Scout Finding 48 (Important) flagged adversarial-spec (zscole, 487 stars) as demonstrating multi-LLM debate pattern where models critique each other iteratively until consensus. Design doc sync didn't address this. Current parallax design: reviewers examine artifact once in parallel, findings are consolidated, user processes findings. No iteration between reviewers, no debate, no critique-of-critique. This is single-pass review, not debate.","why_it_matters":"adversarial-spec's key insight: debate surfaces gaps better than independent inspection. Their process: Claude drafts spec → opponents critique → synthesize → revise → repeat. Multiple debate rounds force deeper thinking. Parallax's single-pass review means reviewers don't see each other's findings—Assumption Hunter might flag something Edge Case Prober would realize is actually fine if they debated. True adversarial review requires reviewers to respond to each other's findings, not just to the design.","suggestion":"Add \"Multi-Round Review (Post-MVP)\" section to design documenting debate pattern as future enhancement. Minimal design: (1) Round 1: reviewers examine design independently (current design), (2) Round 2: reviewers receive each other's findings and write responses (\"Assumption Hunter flagged X, but I think Y because...\"), (3) Synthesizer reconciles debates and flags unresolved tensions for human decision. Note implementation cost: 2x token usage (two rounds of review), 2x latency (sequential rounds). Defer to post-MVP pending eval data on whether single-pass review quality is sufficient. If Second Brain test case shows missed findings that multi-round would catch, prioritize this enhancement."}
