{"type":"finding","id":"v3-first-principles-001","title":"Documentation Sync Treated as Design Revision Conflates Two Different Actions","severity":"Critical","phase":{"primary":"calibrate","contributing":"design"},"section":"Iteration 2 verdict reasoning, documentation debt pattern","issue":"Iteration 2 verdict is \"revise\" based on 15 Critical findings. 10 of those are \"accepted iteration 1 findings not yet reflected in design doc\" (67% of Requirement Auditor findings flagged as documentation debt). These aren't design flaws requiring rethink—they're accepted decisions requiring documentation updates. User accepted the design decisions in iteration 1, implemented them in code/prompts (Task 8), didn't update design doc. Iteration 2 re-flags same issues as Critical. Treating documentation sync as design revision conflates \"design needs improvement\" (substantive rework) with \"design doc needs updates to match implemented design\" (writing).","why_it_matters":"Documentation debt is process failure (design doc not kept in sync with implementation), not design failure (wrong architecture, missed edge case). Requiring design revision when actual action is \"update docs to match code\" wastes iteration cycles. Iteration 2 summary correctly diagnoses this as systemic issue (\"design doc treated as static artifact rather than living spec\") but still routes to \"revise\" verdict. This creates false signal—design is good (evidenced by Task 8 implementing accepted decisions), documentation is stale.","suggestion":"Add documentation_sync verdict type distinct from revise. If majority of Critical findings are \"accepted decision not yet documented,\" verdict should be \"sync documentation\" with estimated time (1-2 hours writing), not \"revise design\" (implying rearchitecture). Alternatively: build documentation sync into the review workflow—when user accepts findings and implementation updates occur, prompt to sync design doc in same commit. Prevents documentation drift from becoming systemic debt. Iteration 2 flagged this pattern—iteration 3 should test whether design doc sync workflow prevents recurrence."}
{"type":"finding","id":"v3-first-principles-002","title":"Testing Orchestration Infrastructure by Building Design Review First Inverts Risk","severity":"Critical","phase":{"primary":"calibrate","contributing":"design"},"section":"Prototype Scope, problem statement framing","issue":"Problem statement identifies \"No structured requirement refinement\" as root cause of design failures, explicitly states \"requirement refinement has been the single biggest design quality lever in practice,\" then prototypes design-stage review to \"validate orchestration mechanics.\" Requirements review is deferred to eval phase. This validates infrastructure (parallel agent dispatch, finding consolidation) using a phase acknowledged to have lower leverage than the deferred phase. Iteration 2 prior review flagged this (Finding 10, Finding 12, Finding 27)—all noted as \"valid but exploratory\" in dispositions. Design doc sync (this iteration) still doesn't address the inversion.","why_it_matters":"If requirements review is actually the highest-leverage intervention (as stated), weeks spent tuning design-stage personas prove orchestration works but not that the right thing is being orchestrated. When you eventually build requirements review, you may discover it needs different persona types (Product Strategist added to requirements stage but not tested), different output format (success criteria vs design tradeoffs), different finding classification (missing requirement vs wrong architecture). This invalidates design-stage infrastructure decisions. The prototype tests \"can we dispatch agents in parallel?\" (yes, superpowers already does this) instead of \"does multi-perspective requirements review prevent downstream design failures?\" (unknown, highest value).","suggestion":"Flip the prototype order. Build requirements-stage review first with 4 requirement-focused personas (Product Strategist, Assumption Hunter, Requirement Auditor, First Principles). Run it against a real historical project where bad requirements led to design failures (Second Brain candidate—problem statement says \"review findings revealed design flaws that should have been caught during research phase\"). Compare review findings to actual downstream failures. If requirements review catches 60%+ of issues that became design problems, you've validated the value hypothesis and can confidently build design/plan review infrastructure. If it catches <40%, the hypothesis is wrong and orchestration doesn't matter. Alternatively: accept this is an infrastructure validation prototype (not value validation) and reframe scope explicitly."}
{"type":"finding","id":"v3-first-principles-003","title":"\"Adversarial Review\" is Coverage-Based Inspection, Not Adversarial Debate","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"Reviewer Personas, core hypothesis, overview","issue":"Design claims \"adversarial multi-agent design review\" as differentiator. The 6 design personas (Assumption Hunter, Edge Case Prober, Requirement Auditor, Feasibility Skeptic, First Principles, Prior Art Scout) are scoped by domain coverage—what part of the design to inspect. They can all be simultaneously correct because they're examining different surfaces. True adversarial review requires incompatible incentives forced to reconcile: \"ship fast, prove in production\" vs \"ship safe, prove upfront.\" Mitsubishi's adversarial debate AI (Jan 2026 validation) uses opposing models arguing for contradictory conclusions. Design is comprehensive checklisted inspection, not adversarial debate. Iteration 2 prior review flagged this (Finding 11: \"Adversarial review is misnamed\"), disposition said \"valid critique of problem framing, evaluate empirically in prototype.\"","why_it_matters":"If core hypothesis is \"adversarial tension surfaces design blind spots better than single-perspective review,\" your persona architecture doesn't test it. You're testing \"more inspectors find more issues\" (obviously true, not novel). The finding consolidation synthesizer deduplicates parallel findings—additive coverage. Adversarial debate synthesizer would reconcile opposing positions—forcing design to satisfy contradictory constraints reveals tradeoffs. Design doc sync (this iteration) added 23 accepted dispositions but didn't change persona architecture to address this.","suggestion":"Either (a) rename to \"comprehensive multi-perspective design review\" and acknowledge adversarial tension is deferred, OR (b) redesign 2-3 personas as stance-based adversaries with opposing success criteria. Example pairs: Optimizer (minimize complexity, defer edge cases, ship fast) vs Hardener (demand robustness, block on unknowns). User-Centric (prioritize UX even if implementation cost high) vs Operator-Centric (prioritize maintainability even if UX suffers). Require reconciliation with tradeoff justification. Force the designer to choose between contradictory goods, not accumulate independent findings. Alternatively: test both architectures in eval (coverage-based vs stance-based) and compare which surfaces more valuable findings. This was flagged in iteration 2 Finding 11 as exploratory—iteration 3 design doc still doesn't acknowledge naming mismatch."}
{"type":"finding","id":"v3-first-principles-004","title":"Solving \"Lack of Review Automation\" When Problem is \"Skipping Requirements Phase\"","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"Problem statement, desired workflow","issue":"Observed pain points trace to \"no structured requirement refinement—jumped from idea to design without prioritizing.\" Desired workflow shows calibrate phase with MoSCoW, anti-goals, success criteria, human checkpoint. All pain points are upstream of design (dimension mismatch should have been caught in requirements, open questions should have been resolved before design, dropped features indicate wrong prioritization). Design builds review automation to catch design flaws adversarially. This treats symptom (designs have flaws) not cause (teams skip requirements because ROI isn't obvious). Iteration 2 prior review Finding 12 flagged this—disposition: \"valid but exploratory.\"","why_it_matters":"Automating design review doesn't solve \"teams skip requirements.\" It moves problem detection downstream. You'll catch design flaws adversarially but requirement-level errors still compound into implementation failures. If real problem is \"requirements phase is skipped because value isn't obvious until later when errors surface,\" you need to make requirements refinement's value obvious immediately. Building calibrate skill that takes 5 minutes and produces MoSCoW + anti-goals + testable success criteria, then demonstrating it reduces downstream design errors, proves its own value and solves the skipping problem.","suggestion":"Reframe problem as \"requirement refinement is skipped because ROI isn't obvious\" and build parallax:calibrate first. Make it fast (5-minute interaction), high-value (prevents known failure modes), and self-demonstrating (run calibrate on this project, compare requirements doc produced to historical designs without it). If calibrate prevents 60%+ of observed design failures, you've proven value. Then build design review as validation layer (catching what calibrate missed), not primary intervention. This inverts current architecture but aligns with stated problem framing."}
{"type":"finding","id":"v3-first-principles-005","title":"Phase Classification Routing Logic Assumes Linear Causality","severity":"Critical","phase":{"primary":"design","contributing":"calibrate"},"section":"Finding Phase Classification, Verdict Logic, Synthesis","issue":"Reviewers classify findings by primary phase (survey/calibrate/design/plan) with optional contributing phase. Verdict logic routes on primary phase only: calibrate gap → escalate to requirements, design flaw → revise design. Real design failures are multi-causal: missing research (survey) leads to unstated assumption (calibrate gap) enables flawed design (design flaw) that's hard to implement (plan concern). Forcing single-phase classification loses multi-causal reality. Design doc now includes primary+contributing classification (accepted from iteration 1 Finding 7 disposition, synced in this iteration) but verdict logic still routes on primary phase only. Contributing phase is metadata, not operationalized in routing.","why_it_matters":"If finding is \"design flaw (primary) caused by calibrate gap (contributing),\" verdict routes to design revision when it should escalate to requirements. Fixing design without fixing upstream calibrate gap produces local patch, not systemic fix. Iteration 2 summary shows 7 calibrate gaps flagged (some contributing, some primary) but verdict is \"revise\" (design-level action). Design doc sync added primary+contributing to Finding Phase Classification section but didn't update Verdict Logic to use contributing phases for escalation triggers. If contributing phases aren't escalation signals, classification is decorative.","suggestion":"Revise verdict logic to treat contributing phases as escalation signals. Rule: If any finding has \"calibrate gap (contributing)\" or \"survey gap (contributing),\" verdict includes \"address upstream issue\" action—not just \"revise design.\" Synthesizer should aggregate: if >30% of findings share same contributing phase, that phase failed systemically and requires rework regardless of primary classifications. User must acknowledge systemic upstream issue before proceeding. Design doc now documents primary+contributing but doesn't operationalize it—iteration 3 should validate whether synced design doc reflects intended verdict behavior."}
{"type":"finding","id":"v3-first-principles-006","title":"JSONL Format Decided But Design Specifies Markdown Throughout","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Output Artifacts, Per-Reviewer Output Format, Summary Format","issue":"Memory states \"JSONL as canonical output format—decided but not yet implemented.\" Multiple iteration 1 disposition notes reference JSONL enabling features (Finding 14: \"JSONL format enables this naturally—jq filters by severity/persona/phase\"). Design document specifies markdown output format throughout with zero JSONL specification. Iteration 2 flagged this (Finding 9: 4 reviewers consensus). Design doc sync (this iteration) accepted 23 dispositions but didn't add JSONL schema. This isn't \"deferred to post-MVP\" (acceptable)—it's architectural decision (canonical format) not reflected in design.","why_it_matters":"If JSONL is decided canonical format, markdown specification in design is throwaway scaffolding. Building markdown-first then migrating to JSONL requires rewriting file I/O, parsing logic, finding ID generation, disposition tracking. JSONL vs markdown affects finding ID stability (hash vs structured ID field), disposition tracking (structured fields vs prose), tool consumption (jq vs grep), cross-iteration diffing (structured comparison vs text parsing). Deferring implementation is YAGNI (good). Deferring design of decided format means markdown format may bake in assumptions that conflict with JSONL schema (bad). Four reviewers flagged this in iteration 2—iteration 3 design doc sync still doesn't address it.","suggestion":"Either (a) add JSONL schema specification to design (finding format, summary format, disposition schema) with migration path from markdown, OR (b) reverse the decision—acknowledge markdown is MVP and JSONL is speculative v2 enhancement pending eval data. Clarify \"canonical format\" means long-term target (design now, implement later) vs \"current format\" means markdown sufficient (JSONL only if eval proves value). If JSONL is decided, design must specify schema even if implementation deferred—schema design affects current markdown format choices. Iteration 2 flagged this as highest-consensus finding (4 reviewers)—iteration 3 should resolve the ambiguity."}
{"type":"finding","id":"v3-first-principles-007","title":"Design-After-Implementation Creates Rationalization Risk","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Entire design document, prototype scope","issue":"Memory context shows 11 commits on feature/parallax-review branch before this design review (implementation exists). Design document describes system already built. Standard practice: design review before implementation. Post-implementation design docs rationalize what was built (glossing over implementation complexity, ideal-case framing) rather than specify buildable reality. This review examines idealized description, not actual system. Iteration 2 Feasibility Skeptic Finding 1 flagged this. Design doc sync (this iteration) synced 23 accepted dispositions but doesn't address post-hoc nature. If implementation diverged during development, findings are academic.","why_it_matters":"Reviewing a design written after implementation validates documentation quality, not design quality. The test that matters is \"does implementation match spec?\" which is inverted—spec was written to match implementation. Iteration 2 documentation debt finding (67% of Requirement Auditor issues) confirms: design doc is lagging indicator of implementation decisions, not leading blueprint. Three review iterations have now occurred post-implementation—validating design doc completeness, not design feasibility. This is acceptable for dogfooding (testing review skill on itself) but doesn't validate core hypothesis (multi-perspective review prevents design failures).","suggestion":"Acknowledge this is design extraction (documenting what was built for future reference) not design specification (blueprint for implementation). Review for completeness and accuracy, not feasibility. Include implementation artifacts (actual agent prompts, code snippets, error handling logic, test results) in next review iteration so findings reference reality. Better: establish design-before-implementation discipline for future phases (requirements review, plan review) where you can validate \"is this buildable?\" before building. Use iterations 1-3 as calibration for what post-hoc design review can/can't validate. Mark design doc status as \"post-implementation documentation\" not \"pre-implementation specification.\""}
{"type":"finding","id":"v3-first-principles-008","title":"Requirements Versioning Untracked Creates False Resolution Signals","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Skill Interface, Cross-Iteration Finding Tracking","issue":"All reviewer prompts reference requirements document as immutable input. Cross-iteration tracking (added to design doc in this iteration, accepted from iteration 1 Finding 5 disposition) detects when findings are resolved between iterations. In real design workflows, adversarial review findings trigger requirement clarification (\"we need to specify what happens at scale\" → requirement gets added to requirements doc). Next review cycle operates against updated requirements. Reviewers can't distinguish \"design improved to satisfy requirement\" from \"requirement was lowered/changed to match design.\" This produces false \"finding resolved\" signals when requirement moved, not design fixed. Iteration 2 Assumption Hunter Finding 1 flagged this.","why_it_matters":"Finding classification routes errors to pipeline phase that failed. If design finding from iteration 1 is \"resolved\" in iteration 2 because requirement was relaxed (not design improved), that's calibrate gap disguised as design improvement. Reviewer sees design now satisfies requirement X, doesn't see requirement X was rewritten. Should escalate to calibrate (requirement changed between iterations), not mark as design-resolved. Cross-iteration tracking (synced in this iteration) without requirement versioning can't detect this. Design doc now includes cross-iteration tracking mechanism but doesn't specify requirement versioning.","suggestion":"Add requirement versioning to Cross-Iteration Finding Tracking section. Timestamp or hash requirements doc, reviewers note which version they reviewed against. Synthesizer flags requirement changes across iterations. Pass git diff of requirements to reviewers on re-review (\"requirements changed: added failure mode section, relaxed latency constraint\"). Alternatively: require explicit user confirmation when re-reviewing with modified requirements—warn that finding resolution may be due to requirement changes, not design improvements. This was flagged in iteration 2—design doc sync should have included it if cross-iteration tracking was added."}
{"type":"finding","id":"v3-first-principles-009","title":"Building Custom Infrastructure When Mature Frameworks Exist","severity":"Critical","phase":{"primary":"calibrate","contributing":"design"},"section":"Entire design—Reviewer dispatch, Synthesis, UX Flow, Pipeline Integration","issue":"Design custom-builds parallel reviewer dispatch, finding consolidation, retry logic, timeout handling, state management for finding processing, cross-iteration tracking, and verdict computation. Iteration 2 Prior Art Scout identified: Inspect AI provides multi-agent patterns with handoff primitives and retry/timeout (Finding 13), LangGraph solves stateful workflows and human-in-the-loop gates (Finding 14), LangSmith provides annotation UI for finding processing (Finding 44), Braintrust provides LLM-as-judge for severity normalization (Finding 45). All are in tooling budget, MIT licensed, production-grade. Design builds 80% custom infrastructure for 20% novel contribution (persona prompts, phase classification). Design doc sync (this iteration) accepted these findings as \"evaluate during implementation\" but didn't update design to reflect evaluation results.","why_it_matters":"CLAUDE.md explicitly states \"BUILD adversarial review (novel), LEVERAGE LangGraph + Inspect AI + Claude Code Swarms (mature).\" Design inverts this—custom-builds infrastructure (orchestration, state, UI) and treats persona prompts as configuration. Maintaining custom agent dispatch, result collection, retries, timeout handling, progress tracking, cost estimation, failure recovery is 40-60% of implementation surface area. Inspect AI is purpose-built for multi-agent LLM evaluation (UK AI Safety Institute project, actively maintained). Using Inspect positions this as domain-specific prompt engineering (the actual novel contribution) rather than infrastructure work. Iteration 2 flagged this with 4 Critical findings from Prior Art Scout—iteration 3 design doc sync doesn't address build-vs-leverage decision.","suggestion":"Evaluate Inspect AI as implementation substrate before continuing custom orchestration development. Prototype reviewer personas as Inspect solvers with custom system prompts. Use Inspect's multi_agent pattern for parallel dispatch, scorer API for synthesis, trace collection for cost tracking. Reserve custom orchestration only if Inspect proves insufficient after prototyping. If Inspect works, 60% of design surface area becomes \"use Inspect's patterns\" and focus shifts to persona prompt quality (the novel work). This is build-vs-leverage decision that should have been explicit in calibrate phase—iteration 2 identified prior art exists, iteration 3 should evaluate before committing to custom build."}
{"type":"finding","id":"v3-first-principles-010","title":"Verdict Computed Before User Processes Findings Wastes Cycles","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Verdict Logic, UX Flow Step 4-6","issue":"UX flow presents verdict to user in Step 4 (after synthesis, before finding processing). Verdict is computed from all findings including those with incorrect severity (false positives). User processes findings in Step 5, rejects 3 Critical findings as invalid. Verdict presented in Step 4 was \"revise\" (based on 5 Critical findings). After rejecting 4 Critical findings as false positives, should have been \"proceed.\" Design doesn't specify whether verdict is recomputed after processing or remains unchanged. Iteration 2 Feasibility Skeptic Finding 43 flagged this.","why_it_matters":"Presenting verdict before user validates findings wastes cycles. User sees \"revise\" verdict, spends 20 minutes processing 41 findings, rejects 4 Critical findings as invalid (wrong assumptions, out of scope, misread design). Correct verdict was \"proceed with noted improvements.\" If verdict is recomputed post-processing, Step 4 verdict is provisional (confusing). If verdict is not recomputed, it's based on unvalidated findings (incorrect). Design doc doesn't specify verdict recomputation logic.","suggestion":"Eliminate provisional verdict or recompute post-processing. Option 1: Show finding counts and severity distribution in Step 4, compute verdict in Step 6 after user processing (based on accepted findings only). Option 2: Show provisional verdict in Step 4 with \"pending validation\" status, recompute final verdict in Step 6. Option 3: Keep current flow but label verdict as \"based on unprocessed findings—may change after review.\" This affects UX flow specification—iteration 3 should clarify verdict timing."}
{"type":"finding","id":"v3-first-principles-011","title":"Async-First Architecture is File-Based Output, Not Background Execution","severity":"Important","phase":{"primary":"design","contributing":null},"section":"UX Flow, iteration 1 Finding 2 disposition, Output Contract","issue":"Iteration 1 Finding 2 disposition states \"Async is the default—review always writes artifacts to disk. Interactive mode is convenience layer.\" Design doc now includes this in Output Contract (synced this iteration). This describes file-based output (disk artifacts vs in-memory), not async execution (background processing). True async requires: review runs without blocking terminal, user can close session and resume later, finding processing state persists across sessions. Current design: sync execution (terminal occupied during review), file-based output (artifacts on disk), optional interactive processing (same session). Background automation (CLAUDE.md track 6) requires true async.","why_it_matters":"If review takes 5 minutes, that's 5 minutes terminal occupancy. User can't work on other tasks during review. Calling this \"async-first\" is misleading—it's \"disk-first\" (good for persistence) but not \"background-capable\" (required for automation). File-based output enables async processing (process findings hours later) but execution is still blocking. True async would support --background flag, session-independent finding processing, in-progress review status checks. Design doc sync added \"async-first\" language but doesn't distinguish execution mode from output mode.","suggestion":"Distinguish execution mode (sync/async) from output mode (interactive/file-based). Current design: sync execution, file-based output with optional interactive processing. For true async: support --background flag (review runs detached), session-independent finding processing (resume on any terminal/machine), in-progress review status. Defer true async to post-MVP but document distinction—current architecture enables async finding processing (good first step), not async review execution (required for automation track). Update Output Contract to clarify \"async-first\" means file-based artifacts enabling deferred processing, not background execution."}
{"type":"finding","id":"v3-first-principles-012","title":"Minimum Viable Reviewer Set is Empirical Question Built Into Design","severity":"Important","phase":{"primary":"calibrate","contributing":"design"},"section":"Reviewer Personas, Open Questions","issue":"Design specifies 6 reviewers for design stage, acknowledges \"optimal number is empirical question for eval framework,\" but builds 6-reviewer architecture into skill. No coverage analysis, overlap measurement, or diminishing returns calculation. Prior Art Scout and First Principles Challenger have overlap (both question whether design should exist). Edge Case Prober and Feasibility Skeptic both examine \"what could go wrong.\" Iteration 2 Feasibility Skeptic Finding 39 flagged this. Design doc sync (this iteration) didn't change reviewer count or add empirical validation plan.","why_it_matters":"Each reviewer adds cost and coordination overhead. Design treats 6 reviewers as baseline when it's an untested hypothesis. Should be: start with 2-3 core reviewers, add one at a time, measure incremental value. Problem statement says \"optimal N is empirical\"—design should enable empirical testing, not hardcode 6. If first 3 reviewers catch 80% of findings, running all 6 is low ROI. Three review iterations have now occurred—coverage analysis data should exist but isn't referenced in design.","suggestion":"Build reviewer set as configurable parameter, not hardcoded architecture. Enable coverage analysis during prototyping: for each finding, tag which reviewer(s) flagged it. After iterations 1-3 (data now exists), analyze: reviewers that consistently flag unique findings (high value, keep), reviewer pairs with >50% overlap (consolidate), finding categories no reviewer catches (missing persona). Start with 3 core reviewers (Requirement Auditor, First Principles, Edge Case Prober), add personas incrementally based on gap analysis. Treat 6 reviewers as hypothesis to validate, not baseline. Three iterations complete—run coverage analysis before iteration 4."}
{"type":"finding","id":"v3-first-principles-013","title":"Critical-First Mode Creates Orphaned Finding Debt Across Iterations","severity":"Important","phase":{"primary":"design","contributing":null},"section":"UX Flow Step 4 (Critical-first mode), Cross-iteration tracking","issue":"Critical-first processing mode addresses only Critical findings, user revises design, re-reviews. Iteration 2 produces new findings. User processes Critical-first again. Important/Minor findings from iteration 1 are never processed—orphaned. After 3 Critical-first iterations, 40+ unprocessed findings accumulate across iterations. No mechanism tracks which findings are carried forward unprocessed vs obsoleted by design changes. Iteration 2 Edge Case Prober Finding 25 flagged this. Design doc sync (this iteration) added Critical-first mode specification but didn't add orphaned finding management.","why_it_matters":"Critical-first enables fast iteration but creates technical debt of unprocessed findings. Eventually user must process accumulated findings or accept valuable feedback was discarded. Design provides no orphaned finding management—when does user process deferred findings? How do you know if iteration 1 Important finding is still valid after iteration 3 design changes? Cross-iteration tracking (synced this iteration) tracks finding status but doesn't specify handling of perpetually-deferred findings.","suggestion":"Add orphaned finding management to Critical-first workflow specification. After Critical-first processing, mark remaining findings as \"deferred to next iteration.\" On re-review, synthesizer reconciles prior deferred findings with new findings (same issue? obsoleted by design change? still relevant?). After revise loop converges (no new Critical findings), prompt user to process accumulated deferred findings. Alternatively: include Important findings related to same subsystem as Critical findings (grouped processing). This should be in UX Flow Step 4 specification—iteration 3 design doc sync missed it."}
