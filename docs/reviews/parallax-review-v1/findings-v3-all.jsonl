{"type":"finding","id":"v3-assumption-hunter-001","title":"Auto-Fix Step Assumes It Can Correctly Identify \"Trivial\" Changes","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Step 4: Auto-Fix (synthesis section)","issue":"Design specifies auto-fix classifies findings as \"typos in markdown, missing file extensions, broken internal links\" and applies them automatically. This assumes the system can reliably distinguish trivial mechanical fixes from semantic changes. Example: \"broken internal link\" could mean (a) wrong file extension (trivial) or (b) wrong target document entirely (semantic). \"Typo\" could mean spelling error (trivial) or variable name that should be intentionally different (semantic). The design provides no validation mechanism beyond \"conservative criteria\" which is undefined.","why_it_matters":"Auto-fixes modify source files and commit changes automatically. A misclassified \"trivial\" fix that changes meaning breaks the design. Git blame shows auto-fix commit obscuring who made the semantic decision. Worse: if auto-fix runs before human review (as specified in Step 4), user cannot reject bad auto-fixes—they're already applied and committed.","suggestion":"Add validation requirements to auto-fix specification: (1) Define \"conservative\" with concrete examples and exclusion criteria, (2) Require that auto-fixes are presented as diffs for user approval before application, (3) Add rollback mechanism (separate git commit enables revert, but user must be told how), (4) Consider deferring all auto-fix to post-human-processing—user accepts findings first, then auto-fixable accepted findings are applied as batch. This prevents auto-fixing findings the user would reject."}
{"type":"finding","id":"v3-assumption-hunter-002","title":"Git-Based Iteration Tracking Assumes Design Doc Lives in Git","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Output Artifacts (\"Iteration history tracked by git\"), Cross-Iteration Finding Tracking","issue":"Design states \"Iteration history tracked by git (each re-review is a new commit, diffs show what changed)\" and \"git diff integration: highlight changed sections to reviewers.\" This assumes the design document being reviewed is git-tracked. If user reviews a design doc that lives outside the repository (e.g., Google Doc exported to markdown, Confluence page converted to local file, design doc from different repo), git diff fails. No fallback specified.","why_it_matters":"Requirements doc states this should be \"applicable to work contexts.\" Many teams use Confluence, Notion, or Google Docs for design documents, not git-tracked markdown. If parallax:review only works for git-tracked docs, it excludes significant use cases. Additionally, cross-iteration tracking depends on git diff to detect changed sections—if diff isn't available, reviewers lose focus prioritization.","suggestion":"Add input validation that checks whether design doc is git-tracked. If not: (1) Warn user that cross-iteration diff won't be available, (2) Fall back to file timestamp comparison or manual change notes from user, (3) Document git requirement as constraint, or (4) Implement text-based diff (compare iteration 1 file to iteration 2 file directly) as fallback."}
{"type":"finding","id":"v3-assumption-hunter-003","title":"Stable Finding IDs Assume Section Headings Don't Change","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Cross-Iteration Finding Tracking (\"Stable hash derived from section + issue content\")","issue":"Design specifies finding IDs as \"stable hash derived from section + issue content.\" This assumes design doc section headings remain stable across iterations. If designer refactors \"UX Flow\" section into \"User Workflow\" and \"State Management\" between iterations, all findings anchored to \"UX Flow\" become orphaned—system treats them as resolved when they're actually just relocated. Hash-based IDs break when input text changes, even if semantic content is unchanged.","why_it_matters":"Section refactoring is normal during design iteration. Improving document structure shouldn't invalidate finding tracking. Prior review (iteration 2) flagged this as Finding 7 (Critical) with suggestion for semantic matching, but design still specifies text hashing. If implemented as designed, cross-iteration tracking will produce false negatives (findings marked \"resolved\" when section renamed) and false positives (findings marked \"new\" when actually rephrased).","suggestion":"Replace text-based hashing with semantic anchoring or LLM-based matching. Options: (1) Store section heading + offset position, fuzzy-match on re-review if heading changed, (2) Use LLM to semantically match new findings to prior findings (\"Is this the same issue?\"), (3) Hybrid: hash as first pass, LLM disambiguation when hash misses, (4) Allow manual finding ID assignment by reviewers for critical findings that need guaranteed cross-iteration tracking."}
{"type":"finding","id":"v3-assumption-hunter-004","title":"Reviewer Prompt Context Assumes Single Iteration Per Session","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Reviewer Prompt Architecture (\"Variable suffix: prior review summary if re-review\")","issue":"Design specifies reviewers receive \"prior review summary (if re-review)\" as context. This assumes re-review happens after user processes findings from prior iteration and updates design doc. If user runs review, sees findings, immediately revises design, and re-runs review within same session (common workflow during rapid iteration), \"prior review summary\" is the review from 5 minutes ago. Reviewer context grows with each iteration in a single session. After 3 rapid iterations, reviewers receive summaries from iterations 1, 2, and 3, consuming thousands of tokens for diminishing value.","why_it_matters":"Rapid iteration is a common workflow (\"fix Critical findings immediately, re-review before moving on\"). Design doesn't specify how prior context is bounded when multiple iterations happen in quick succession. Token costs scale linearly with iteration count. After 5 iterations in one session, review becomes prohibitively expensive or exceeds context limits.","suggestion":"Add prior context pruning rules to prompt architecture: (1) Include only most recent N iterations (e.g., 2), (2) Summarize older iterations rather than include verbatim, (3) For rapid iteration, include only findings from most recent run that remain unresolved, (4) Track token budget and warn user if prior context exceeds threshold."}
{"type":"finding","id":"v3-assumption-hunter-005","title":"Async-First Architecture Assumes File System as Single Source of Truth","severity":"Important","phase":{"primary":"design","contributing":"calibrate"},"section":"UX Flow (async-first), Finding 2 disposition from iteration 2","issue":"Design specifies \"review always writes artifacts to disk\" as baseline, with interactive processing as \"convenience layer.\" This assumes all state lives in files under `docs/reviews/<topic>/`. If user runs review on machine A, processes findings on machine B (different clone of repo), or collaborates with teammate who processes findings from their checkout, state diverges. File-based state requires that all participants operate on same filesystem or rigorously sync via git. No synchronization mechanism specified.","why_it_matters":"Requirements emphasize \"applicable to work contexts\" and CLAUDE.md notes \"this repo may be worked on from multiple machines.\" File-based state doesn't handle distributed workflows without additional tooling (git push/pull between every operation). If two users process findings in parallel, last write wins—earlier dispositions are silently lost.","suggestion":"Either (1) Document single-user, single-machine constraint explicitly as MVP limitation, (2) Add conflict detection (check if summary.md has uncommitted changes before allowing processing), (3) Require git commit after each disposition batch, (4) Evaluate external state management (LangSmith annotation queues, as noted in iteration 2 Finding 44)."}
{"type":"finding","id":"v3-assumption-hunter-006","title":"Phase Classification Routing Assumes Single Phase Owns Each Finding","severity":"Important","phase":{"primary":"design","contributing":"calibrate"},"section":"Verdict Logic, Finding Phase Classification","issue":"Verdict logic routes findings based on primary phase: survey/calibrate gap → escalate, design flaw → revise. This assumes each finding has a clear owner phase. Real design failures are multi-causal: missing research (survey) creates unstated assumption (calibrate) enabling flawed design (design). Iteration 2 Finding 6 flagged this (Critical severity), disposition accepted primary+contributing classification, but design still shows single-phase routing. If a finding is \"design flaw (primary) caused by calibrate gap (contributing),\" routing to design revision doesn't fix the upstream calibrate issue.","why_it_matters":"Systemic failures require systemic fixes. If 30% of design findings trace to same calibrate gap (e.g., \"missing failure mode requirements\"), fixing each design finding individually is inefficient. Design acknowledges multi-causal reality (\"When >30% of findings share a contributing phase, synthesizer flags systemic issue\") but verdict logic doesn't use this for routing—it's informational only.","suggestion":"Update verdict logic to treat contributing phases as escalation triggers. If any finding has calibrate/survey as contributing phase AND that phase appears in >20% of findings, verdict cannot be \"proceed\"—must be \"escalate with systemic issue noted.\" User must acknowledge upstream gap before addressing downstream symptoms."}
{"type":"finding","id":"v3-assumption-hunter-007","title":"Reviewer Tool Access Assumes Read Operations Are Side-Effect-Free","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Reviewer Capabilities (new section in iteration 3), iteration 2 Finding 33","issue":"Design specifies reviewers have \"read-only\" file access and specific tools (git, gh, curl for Prior Art Scout). This assumes read operations have no side effects. But some \"read\" operations create state: `git status` is read-only, but running git commands creates `.git` lock files. `curl` fetching a URL may trigger server-side logging or rate limits. `gh api` calls count against API quotas. If 6 reviewers all run `gh search repos X`, that's 6 API calls per review—potentially hitting GitHub rate limits for unauthenticated requests (60/hour).","why_it_matters":"Tools with quotas or side effects need coordination. If multiple reviewers independently query same external resource, they waste quota. If review runs in CI/CD (future automation), read operations that modify filesystem state can cause permission errors or lock conflicts.","suggestion":"Add tool access constraints to design: (1) Specify which tools are quota-limited (gh, curl with external APIs), (2) Implement result caching—if Prior Art Scout searches GitHub for \"design review tools,\" cache result for other reviewers, (3) Document side-effect assumptions (git commands may create temp files, external API calls count against quotas), (4) For CI/CD: ensure reviewers run with appropriate permissions and avoid lock file conflicts."}
{"type":"finding","id":"v3-assumption-hunter-008","title":"Critical-First Mode Assumes Critical Findings Are Independent","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Step 5: Present Summary (Critical-first mode)","issue":"Design offers \"Critical-first\" processing mode: \"Address Critical findings only. Send source material back for rethink. Remaining findings processed in next pass.\" This assumes Critical findings can be processed independently without context from Important/Minor findings. But findings often have dependency chains: Critical finding \"No authentication specified\" depends on Important finding \"API endpoints exposed externally.\" Processing Critical finding without seeing Important finding may lead to wrong fix (add auth when correct fix is \"make internal-only\").","why_it_matters":"Out-of-context fixes waste time. User processes Critical finding, makes design decision, re-reviews, discovers Important finding that contradicts the decision. Must backtrack. Critical-first mode optimizes for iteration speed but may reduce fix quality by hiding necessary context.","suggestion":"Add dependency detection to Critical-first workflow: (1) If Important/Minor findings relate to same design section as Critical finding, present them together, (2) Add \"view related findings\" option during Critical processing, (3) Synthesizer groups findings by affected subsystem/section, processes clusters together regardless of severity, (4) Document tradeoff: Critical-first is fast but may miss context, all-findings is slower but more comprehensive."}
{"type":"finding","id":"v3-assumption-hunter-009","title":"Synthesizer Deduplication Assumes \"Same Issue\" Is Objectively Determinable","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Synthesis (deduplication responsibility)","issue":"Synthesizer must \"deduplicate—group findings flagged by multiple reviewers into single entries.\" Design doesn't specify how \"same issue\" is determined. This is a subjective judgment task. Example: Assumption Hunter flags \"assumes users have admin rights\" (calibrate gap), Edge Case Prober flags \"no permission check in delete operation\" (design flaw). Are these duplicates (both about permissions) or distinct (one is missing requirement, other is missing implementation)?","why_it_matters":"Aggressive deduplication (\"both about permissions → merge\") loses important distinctions (requirement-level vs implementation-level). Conservative deduplication (\"different wording → separate\") floods user with near-duplicates. Iteration 2 Finding 40 flagged this but design still provides no heuristics—just responsibility assignment.","suggestion":"Define deduplication criteria explicitly: (1) Findings are duplicates only if same section AND same root cause AND same suggested fix, (2) Findings about same topic but different phases are NOT duplicates—preserve as related but distinct, (3) Synthesizer groups related findings under common heading with cross-references (\"Related: Finding 3 addresses requirement gap, Finding 7 addresses implementation\") but counts them separately, (4) Provide examples of duplicate vs related in synthesizer prompt."}
{"type":"finding","id":"v3-assumption-hunter-010","title":"Prompt Caching Assumes Stable Prefix Doesn't Change Between Iterations","severity":"Important","phase":{"primary":"design","contributing":"plan"},"section":"Reviewer Prompt Architecture (stable prefix for caching)","issue":"Design structures prompts as \"stable cacheable prefix (persona + methodology + output format + voice guidelines) + variable suffix (design artifact + requirements + prior summary).\" This assumes stable prefix doesn't change across review iterations. But if user rejects findings as false positives and adds calibration rules to reviewer prompts (\"Assumption Hunter: don't flag X as assumption, it's explicitly stated in requirements\"), that modifies the stable prefix. Cache invalidates. Next review pays full input token cost.","why_it_matters":"Reviewer calibration (learning loop from iteration 2 Finding 29 disposition, Compound Engineering pattern from iteration 2 Finding 46) requires modifying reviewer prompts based on false positive/negative feedback. Every calibration change invalidates cache. Cost optimization via caching conflicts with quality improvement via calibration. Design acknowledges \"prompt changes to stable prefix invalidate cache\" but doesn't address how to balance these competing needs.","suggestion":"Separate calibration rules from stable prefix. Structure prompts as: (1) Stable prefix (persona, methodology, format—never changes), (2) Calibration rules (versioned, changes based on feedback), (3) Variable suffix (artifact being reviewed). Cache stable prefix. Include calibration rules in non-cached middle section. Track calibration rule version separately from prompt version. Cost increases linearly with calibration rule size, not prompt size."}
{"type":"finding","id":"v3-assumption-hunter-011","title":"Verdict \"Proceed\" Assumes Accepted Findings Will Be Addressed","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Verdict Logic, Step 6: Wrap Up","issue":"Verdict logic states \"Only Important/Minor → proceed with noted improvements.\" This assumes user will actually address accepted findings before moving to next phase. But \"proceed\" means \"move forward\"—skill exits, user continues to implementation. No mechanism tracks whether accepted findings were addressed or just noted and forgotten. If user accepts 15 Important findings, gets \"proceed\" verdict, and moves to execution without fixing them, review provided no value.","why_it_matters":"\"Proceed with noted improvements\" creates false sense of completeness. User thinks \"review passed\" when actually \"review passed conditionally on fixing these 15 things.\" If improvements aren't tracked, they become technical debt. Design has no follow-up mechanism.","suggestion":"Clarify verdict semantics: (1) \"Proceed\" means \"design is acceptable as-is, improvements are optional,\" (2) Add \"proceed-with-conditions\" verdict for \"must address these findings before implementation,\" (3) Track accepted findings as open work items (link to issue tracker, or create tasks file), (4) On next phase review (plan stage), check whether design-stage accepted findings were addressed."}
{"type":"finding","id":"v3-assumption-hunter-012","title":"Topic Label Validation Assumes Alphanumeric Is Safe","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Skill Interface (topic label validation)","issue":"Design specifies topic labels \"validated against safe character set (alphanumeric, hyphens, underscores only).\" This assumes these characters are safe for all filesystems and git branch names. Hyphens at start of filename on some systems are interpreted as command flags. Underscore-only filenames may be hidden. Very long alphanumeric strings (user provides 500-character topic) create filesystem path length issues.","why_it_matters":"Topic label becomes folder name under `docs/reviews/<topic>/`. Edge cases: (1) topic starting with hyphen: `docs/reviews/-foo/` may cause issues with tools that interpret leading hyphen as flag, (2) very long topic exceeds filesystem path limit (typically 255 chars for filename, but full path limit is OS-dependent), (3) topic that matches git special refs (e.g., \"HEAD\") could cause confusion.","suggestion":"Strengthen validation rules: (1) Require topic starts with alphanumeric (no leading hyphen/underscore), (2) Limit length (64 chars recommended, enforced maximum at 128), (3) Disallow reserved names (HEAD, master, main, refs), (4) Sanitize rather than reject—convert spaces to hyphens, lowercase, trim to length."}
{"type":"finding","id":"v3-edge-case-prober-001","title":"Cross-Iteration Tracking Breaks When Design Structure Changes","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Cross-Iteration Finding Tracking (lines 243-252)","issue":"Design specifies finding IDs via \"stable hash derived from section + issue content\" (line 248) but provides no specification when design doc structure changes. If iteration 2 adds new sections or renames existing sections, section-based hashing produces different IDs for conceptually identical findings. Example: Finding flagged in iteration 1 under \"Synthesis\" section gets re-flagged in iteration 2 under new \"Synthesizer Agent\" section—hash collision impossible, treated as new finding. Timestamped folders (line 23) make git-based section tracking unreliable across iterations.","why_it_matters":"Design restructuring (adding sections, splitting large sections, renaming for clarity) is normal during revision. Hash-based IDs tied to section names make tracking unreliable across structure changes. Defeats cross-iteration tracking purpose—users can't distinguish \"new finding\" from \"same finding under renamed section.\" V2 review flagged 55 findings, v3 design doc has 8 new sections—high probability of hash mismatches.","suggestion":"Add section rename detection to design. Options: (1) Hash issue content only (ignore section), rely on LLM-based semantic matching for deduplication per line 262 suggestion, (2) Maintain section mapping file across iterations (old section → new section), (3) Finding IDs include both semantic hash and section anchor, synthesizer reconciles when sections change, (4) Conservative: when structure changes >20%, flag all findings as \"unable to determine if new or repeated\" and prompt user. For MVP, recommend option 1 (content-only hash) + LLM semantic matching."}
{"type":"finding","id":"v3-edge-case-prober-002","title":"Auto-Fix Re-Review May Loop Indefinitely","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Auto-Fix (lines 276-277, \"design is re-reviewed with remaining findings only\")","issue":"Auto-fix applies changes, then \"design is re-reviewed with remaining findings only\" (line 277). If auto-fix introduces new errors (broken markdown formatting, incorrect path corrections), re-review flags new findings, triggers another auto-fix pass, which creates more errors, loops. No loop termination condition specified. Example: Auto-fix corrects typo but breaks link, re-review flags broken link, auto-fix attempts correction but introduces different typo, re-review flags typo, loops. Design specifies auto-fixes committed separately (line 298) but no loop guard.","why_it_matters":"Unbounded auto-fix loops consume API budget and block user workflow. If each auto-fix pass costs $0.50 and loop runs 10 times before user intervenes, $5 burned. Worse: auto-fix changes accumulate as separate git commits (line 298), producing commit spam in git history. Conservative criteria (line 277: \"typos in markdown, missing file extensions, broken internal links\") still allow errors—typo correction can break markdown syntax, path correction can point to wrong file.","suggestion":"Add auto-fix loop guard to design. Specify: (1) Maximum 1 auto-fix pass per review run (no recursive auto-fix), user must approve additional auto-fix iterations, (2) Auto-fix validation: schema check or diff review before applying (reject auto-fixes that break markdown parsing), (3) Auto-fix findings presented to user for approval before application (contradicts \"automatic\" but prevents loops), (4) Track auto-fix iteration count in summary, flag if attempted >1 time."}
{"type":"finding","id":"v3-edge-case-prober-003","title":"Partial Reviewer Completion Breaks Systemic Issue Detection","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Parallel Agent Failure Handling (line 220: \"minimum threshold met (4/6 agents succeed)\"), Finding Phase Classification (line 161: \"when >30% of findings share a contributing phase, synthesizer flags systemic issue\")","issue":"Design allows proceeding with 4/6 reviewers (line 220). Synthesizer detects systemic issues \"when >30% of findings share a contributing phase\" (line 161). If 2 missing reviewers would have flagged the systemic pattern, threshold calculation is wrong. Example: 4 reviewers produce 20 findings, 6 have \"calibrate gap (contributing)\" = 30% (triggers systemic flag). But missing 2 reviewers would have added 10 more findings with 8 calibrate gaps = 14/30 = 47%. Systemic issue severity underestimated. Verdict computed on partial data.","why_it_matters":"Partial results corrupt systemic issue detection. User sees \"proceed with noted improvements\" when full reviewer set would have flagged \"escalate—systemic calibrate gap.\" Missing reviewers aren't random—timeouts correlate with design complexity (harder designs take longer to review, more likely to hit timeout). Worst-case: hardest designs get weakest reviews. Design specifies conservative verdict logic (line 168: \"highest severity in range\") but doesn't account for missing reviewer impact on threshold calculations.","suggestion":"Add systemic detection adjustment for partial results to design. Specify: (1) Disable systemic issue detection when <100% reviewers complete (mark as \"unable to determine systemic patterns with partial results\"), (2) Extrapolate: assume missing reviewers would have flagged patterns at same rate as completed reviewers (statistical estimate with confidence interval reported), (3) Require full reviewer set for final verdict, allow partial results for draft review only, (4) Summary must flag when verdict is computed on partial data."}
{"type":"finding","id":"v3-edge-case-prober-004","title":"Severity Range Resolution Creates False Escalations","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Verdict Logic (lines 145-146: \"Resolve severity for verdict logic... use highest severity in range for verdict computation (conservative)\")","issue":"When reviewers rate same issue differently (line 143: \"severity ranges\"), design uses \"highest severity in range for verdict computation\" (line 146). If Assumption Hunter rates finding Critical, Feasibility Skeptic rates same finding Important, verdict logic treats as Critical. This is conservative but creates false escalations. Example: 5 reviewers rate issue Important, 1 reviewer rates Critical (overly pessimistic), verdict triggers \"revise\" when issue is genuinely Important-tier. Design acknowledges this (line 168: \"If false escalations become a problem, investigate per-agent prompt tuning first\") but accepts false escalations as baseline.","why_it_matters":"Single pessimistic reviewer can block entire design. If Edge Case Prober consistently over-rates severity (sees every edge case as Critical), every review escalates regardless of actual risk. Design defers mitigation to prompt tuning (line 168) but prompt tuning is iterative—false escalations occur throughout prototyping phase. User abandons review process as too heavyweight before prompts stabilize.","suggestion":"Revise severity range resolution to use consensus instead of max. Specify: (1) If ≥50% of reviewers agree on severity, use consensus severity for verdict, (2) If no consensus, use highest severity but flag as \"disputed severity\" in summary, (3) User can downgrade disputed findings during processing, verdict recomputed based on accepted severities, (4) Track per-reviewer severity calibration over time (if reviewer consistently over-rates, flag for prompt tuning). Alternative: use median severity instead of max (still conservative but less sensitive to outliers)."}
{"type":"finding","id":"v3-edge-case-prober-005","title":"Prior Summary Context Injection Creates Confirmation Bias","severity":"Important","phase":{"primary":"design","contributing":"calibrate"},"section":"Cross-Iteration Finding Tracking (line 250: \"prior summary.md included in reviewer context\")","issue":"Reviewers receive prior iteration summary showing which findings were flagged before (line 250: \"reviewers explicitly note 'previously flagged, now resolved' vs 'new finding' vs 'still an issue'\"). This anchors reviewers to prior findings, creating confirmation bias. Reviewers focus on \"is Finding 3 still an issue?\" instead of \"what's wrong with this design?\" Fresh eyes see different problems, but prior context steers them toward validating old findings rather than discovering new ones. Design intends this (line 251: \"changed section focus\" via git diff) but doesn't account for bias.","why_it_matters":"Iteration 2+ reviews become \"check if iteration 1 findings are fixed\" instead of \"find all design flaws.\" New edge cases introduced by fixes are missed because reviewers are pattern-matching against prior findings. Adversarial review requires independent judgment—prior context contaminates that. V2 summary shows 55 findings with 15 Critical—if reviewers anchor to those 15, they miss new Critical findings in freshly-added sections.","suggestion":"Evaluate whether cross-iteration context helps or hurts via eval framework. Test: (1) Run reviewers blind (no prior context), synthesizer reconciles findings after review completes, (2) Provide only changed sections to reviewers (git diff per line 251), not prior findings, (3) Split reviewers: half get prior context (validation set), half run blind (discovery set), compare finding coverage, (4) Prior context only for synthesizer, not individual reviewers. For MVP, recommend option 2 (changed sections only) to balance efficiency with independence."}
{"type":"finding","id":"v3-edge-case-prober-006","title":"Verdict Escalation on Any Calibrate Gap Blocks Design Work","severity":"Important","phase":{"primary":"design","contributing":"calibrate"},"section":"Verdict Logic (lines 165-166: \"Survey or calibrate gap at any severity → escalate\")","issue":"Any finding classified as calibrate gap triggers \"escalate\" verdict \"regardless of severity\" (line 166). If Requirement Auditor flags \"anti-goal X not explicitly stated\" (Minor severity, calibrate gap), entire review escalates, user must revise requirements before continuing design. For complex designs, Minor calibrate gaps are common (missing edge case requirements, unstated assumptions). Escalating on every Minor calibrate gap makes iteration impossible. Design acknowledges multi-causal classification (line 161: \"primary + contributing\") but verdict logic (line 166) treats all calibrate gaps equally.","why_it_matters":"Design review becomes blocked by Minor requirement clarifications. User workflow: write design, run review, get \"escalate—revise requirements,\" update requirements doc with one sentence, re-review, get another Minor calibrate gap, escalate again. After 3 escalations for Minor issues, user abandons review process as too heavyweight. V2 summary shows 7 calibrate gaps—if all trigger escalate, design iteration impossible.","suggestion":"Revise verdict logic to weight severity for calibrate gaps. Specify: (1) Critical calibrate gap → escalate (fundamental requirement conflict), (2) Important calibrate gap → revise with note (can proceed but risk of rework), (3) Minor calibrate gap → proceed with note (document assumption, revisit if implementation reveals problem), (4) Synthesizer flags when multiple Minor calibrate gaps cluster (>30% threshold per line 161) and escalates that pattern, not individual findings. Update line 166 to: \"Survey or Critical calibrate gap at any severity → escalate.\""}
{"type":"finding","id":"v3-edge-case-prober-007","title":"JSONL Format Decided But Schema Still Unspecified","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Prototype Scope (line 320: \"JSONL output enables jq-based filtering\"), Output Artifacts (lines 33-47, markdown only)","issue":"Design references JSONL output (line 320: \"JSONL output enables jq-based filtering by severity/persona/phase without LLM tokens\") but specifies only markdown format throughout (lines 33-47: Output Artifacts, lines 103-122: Per-Reviewer Output Format, lines 173-212: Summary Format). No JSONL schema, field definitions, or migration path specified. V2 summary Finding 9 (Critical, 4 reviewers) flagged this, disposition accepted, but design still incomplete. Line 320 treats JSONL as implemented when it's unspecified.","why_it_matters":"JSONL vs markdown is structural decision affecting storage, parsing, finding IDs (line 248: stable hash), disposition tracking (line 249: status field), and tool integration (jq filtering per line 320). Multiple features depend on JSONL (cost logging per line 323, finding IDs per line 248, phase filtering per line 320) but can't be implemented without schema. Design describes throwaway scaffolding (markdown) not actual system (JSONL). If JSONL deferred to post-MVP, line 320 reference is premature.","suggestion":"Add JSONL schema section to design specifying structure even if implementation deferred. Minimal spec: `{\"id\": \"string (section + issue hash)\", \"iteration\": N, \"reviewer\": \"persona\", \"severity\": \"Critical|Important|Minor\", \"phase\": {\"primary\": \"survey|calibrate|design|plan\", \"contributing\": \"string|null\"}, \"section\": \"string\", \"issue\": \"string\", \"why_it_matters\": \"string\", \"suggestion\": \"string\", \"disposition\": {\"status\": \"pending|accepted|rejected\", \"note\": \"string|null\"}}`. Specify: (1) One JSON object per finding per line, (2) Markdown rendering for human readability, JSONL as canonical storage, (3) Migration path: reviewers output markdown, synthesizer converts to JSONL, both formats maintained during transition."}
{"type":"finding","id":"v3-edge-case-prober-008","title":"Multi-Causal Phase Classification Routing Still Unresolved","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Finding Phase Classification (lines 153-161, primary + contributing phases), Verdict Logic (lines 165-170, uses primary phase only)","issue":"Design added \"primary phase\" and \"contributing phase\" classification (line 161) but verdict logic uses only primary phase (lines 165-170). Contributing phase is metadata only. If finding is \"design flaw (primary) caused by calibrate gap (contributing),\" verdict routes to \"revise design\" when it should escalate to \"fix requirements first\" (line 161: \"enables two actions: immediate fix and systemic upstream correction\"). User fixes design, re-review finds same flaw because underlying calibrate gap persists. V2 summary explicitly notes this (line 161 added during disposition sync) but verdict logic not updated.","why_it_matters":"Multi-causal findings require multi-phase remediation. Routing on primary phase only treats symptom, not root cause. User iterates on design 3 times fixing surface issues while calibrate gap creates new design flaws each time. Line 161 specifies \"when >30% of findings share a contributing phase, synthesizer flags systemic issue\" but no connection to verdict logic. Systemic issue detection is advisory only.","suggestion":"Update verdict logic to check contributing phases. Specify: (1) If ≥30% of findings have \"calibrate gap (contributing),\" escalate regardless of primary phase classification (per line 161 systemic detection), (2) If any Critical finding has calibrate/survey contributing phase, flag as \"fix upstream first\" advisory in verdict reasoning, (3) Synthesizer produces two remediation paths: immediate (primary phase fixes) and systemic (contributing phase fixes), (4) User can choose to fix primary and defer systemic, but must acknowledge risk in disposition notes."}
{"type":"finding","id":"v3-edge-case-prober-009","title":"Selective Re-Run of Failed Reviewers Changes Finding Context","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Parallel Agent Failure Handling (line 222: \"Allow re-running individual failed reviewers without redoing successful ones\")","issue":"If Feasibility Skeptic times out, user re-runs only that reviewer (line 222). But reviewers may reference findings from other reviewers (deduplication awareness, cross-reviewer reasoning). Re-running in isolation produces different findings than running in parallel with others. Worse: synthesis now runs twice (once with 5/6 reviewers producing summary, once with 6/6 re-synthesizing), producing two different summaries. Which is authoritative? Line 222 enables selective re-run but doesn't specify synthesis behavior.","why_it_matters":"Selective re-run breaks reviewer independence assumption. Re-synthesizing findings after selective re-run either duplicates work (reprocess all 41 findings from initial run + 8 from re-run) or creates partial synthesis (only process new reviewer's findings, miss deduplication opportunities with initial findings). User expects \"add missing reviewer's findings to summary\" but design doesn't specify whether synthesis is additive or regenerative.","suggestion":"Clarify selective re-run semantics in design. Specify: (1) Re-run produces separate finding file, user manually merges into summary (no automatic synthesis), (2) Re-run triggers full re-synthesis (all reviewer outputs re-processed, summary regenerated, prior summary archived), (3) Selective re-run disabled—failed reviewers produce empty findings file, review proceeds with partial results only (line 220 threshold), user can re-run entire review if needed, (4) Allow re-run but mark summary as \"re-synthesized with subset\" to flag potential inconsistency. Recommend option 3 for MVP (simplest, avoids synthesis ambiguity)."}
{"type":"finding","id":"v3-edge-case-prober-010","title":"Empty Design Doc or Requirements Doc Produces Nonsense Review","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Skill Interface (lines 17-19: design and requirements as inputs)","issue":"Skill accepts file paths for design and requirements (lines 17-18) but specifies no validation that files exist, are non-empty, or are parseable markdown. If user accidentally points to wrong file (typo, empty file, binary file, corrupted markdown), reviewers receive empty context and either produce hallucinated findings or fail with cryptic errors. No input validation specified in design. Line 25 specifies topic label validation (\"safe character set\") but not artifact validation.","why_it_matters":"Garbage in, garbage out. User typo (`docs/plans/design.md` vs `docs/plans/desing.md`) points to non-existent file, reviewers see empty design, produce findings about \"missing all required sections.\" User processes findings, wastes 30 minutes before realizing input was wrong. At minimum, costs API tokens for nonsense review. Could also produce false positives (reviewer flags \"no architecture section\" when architecture exists but file path was wrong).","suggestion":"Add input validation to Skill Interface section. Specify: (1) Check files exist and are readable before dispatching reviewers, (2) Validate markdown parsing succeeds (no corruption), (3) Check minimum content length (design doc <100 chars likely wrong, requirements <50 chars likely wrong), (4) Show user preview or confirmation before dispatching (first 200 chars of each input + file size), (5) If validation fails, prompt user to correct and retry. For MVP, minimal validation (file exists, non-empty, valid markdown)."}
{"type":"finding","id":"v3-edge-case-prober-011","title":"Cost Logging Specified But No Format or Schema","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Prototype Scope (line 323: \"Cost logging per review run in JSONL output\")","issue":"Design states \"Cost logging per review run in JSONL output\" (line 323) but JSONL output format itself is unspecified (Finding 7). What cost data is logged? Per-reviewer token counts? Aggregated cost? Cache hit rates? Model used? Timestamp? If format undefined, cost logging can't be implemented consistently. If JSONL output deferred to post-MVP (per Finding 7), cost logging has no destination. Line 323 references feature that depends on unspecified infrastructure.","why_it_matters":"Cost tracking is budgetary requirement (CLAUDE.md: $2000/month budget, $150-400 projected API spend). Without schema, cost data inconsistent across review runs, can't be aggregated or analyzed. Requirements specify model tiering decisions based on empirical cost data (Haiku vs Sonnet)—can't make those decisions without structured cost logs. Line 323 acknowledges \"pre-dispatch estimates deferred until empirical cost data available\" but doesn't specify how empirical data is collected.","suggestion":"Define cost logging schema regardless of JSONL timeline. Specify: (1) Per-reviewer cost breakdown (input tokens, output tokens, cached tokens, cache hit rate, cost at model pricing, model used), (2) Aggregated review cost (total tokens, total cost), (3) Cost estimate vs actual (when estimates added later), (4) Timestamp, topic, iteration number, reviewer completion status, (5) Schema: `{\"topic\": \"string\", \"iteration\": N, \"timestamp\": \"ISO8601\", \"reviewers\": [{\"persona\": \"string\", \"model\": \"string\", \"input_tokens\": N, \"output_tokens\": N, \"cached_tokens\": N, \"cost_usd\": N}], \"total_cost_usd\": N}`. Interim solution: log to separate `cost.json` file if JSONL output deferred."}
{"type":"finding","id":"v3-edge-case-prober-012","title":"Review Stage Auto-Detection Deferred But Impacts All Stages","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Skill Interface (lines 20-21: \"Auto-detection deferred (YAGNI for MVP—we know the stage)\")","issue":"Design defers auto-detection of review stage (requirements/design/plan) as YAGNI (line 21) but plans to support all 3 stages eventually (lines 64-99: Reviewer Personas, lines 326-330: Build later). User must manually specify stage for every review. If user specifies wrong stage (reviews design with requirements-stage personas), findings are off-target. No validation that stage matches artifact content. For MVP with design-stage only (line 25), low-risk. For post-MVP multi-stage support (line 326), wrong stage selection produces nonsense reviews.","why_it_matters":"Manual stage specification is error-prone. User runs `parallax:review --stage requirements` on a design doc by mistake, gets Product Strategist findings (requirements persona per line 72) that don't apply to design-stage artifact. Findings reference \"success metrics\" and \"user value\" when design doc discusses architecture. User wastes time processing irrelevant findings. For MVP, user knows stage (single-stage support). For post-MVP, multi-stage support requires validation or auto-detection.","suggestion":"Add stage validation to Skill Interface for when multi-stage support is implemented (post-MVP per line 326). Specify: (1) Heuristic check: design docs contain \"Architecture\" sections, requirements docs contain \"MoSCoW\" or \"Success Criteria\" sections, plans contain \"Task\" or \"Implementation\" sections, (2) Prompt user if stage doesn't match heuristic (\"You specified 'requirements' but doc contains 'Architecture' section—continue? [y/n]\"), (3) Auto-detection as suggestion with user confirmation, not automatic override. For MVP, document single-stage limitation (design only), defer validation to post-MVP."}
{"type":"finding","id":"v3-edge-case-prober-013","title":"Timestamped Folder Collision Creates Review Folder Sprawl","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Topic Labels (line 23: \"Collision handling: timestamped folders for iteration separation\")","issue":"Each iteration creates timestamped folder per line 23 (`docs/reviews/parallax-review-2026-02-15-143022/`). After 5 iterations, `docs/reviews/` contains 5 folders with 35+ files total (7 files per iteration per lines 36-44). No cleanup mechanism, no \"latest iteration\" symlink, no archival strategy. User must manually track which folder is current. Git history shows all folders, diffs are useless (comparing different paths). Requirements specify \"diffs show what changed\" (iteration comparison is core workflow per problem statement line 174) but timestamped folders make git diff impossible.","why_it_matters":"10 active topics × 3 iterations each = 30 folders, 210 files in `docs/reviews/`. Navigation becomes expensive. Git diff comparing `parallax-review-2026-02-15-143022/summary.md` to `parallax-review-2026-02-15-150341/summary.md` shows file addition, not content diff. Requirements (line 174: \"Git commits per iteration. Full history, diffable artifacts\") conflict with timestamped folders. This is calibrate gap—collision handling contradicts diffability requirement.","suggestion":"Revise folder structure to resolve requirement conflict. Options: (1) Single folder per topic, overwrite on re-review, rely on git history for iteration tracking (`git diff HEAD~1 docs/reviews/parallax-review/summary.md` shows content changes), (2) Nested iteration structure: `docs/reviews/parallax-review/iteration-1/`, `docs/reviews/parallax-review/iteration-2/` with symlink `latest/` pointing to most recent, (3) Timestamped folders but synthesizer produces cross-iteration diff report in root (`docs/reviews/parallax-review-diff.md`), (4) Archive old iterations to `docs/reviews/.archive/` after disposition completion. Recommend option 1 for MVP (simplest, satisfies diffing requirement, collision handled by overwrite + git history)."}
{"type":"finding","id":"v3-edge-case-prober-014","title":"Reviewer Timeout Without Progress Updates Appears Frozen","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Parallel Agent Failure Handling (line 218: \"Timeout: 60-120s per agent\"), UX Flow Step 2 (line 271: \"streams progress\")","issue":"Reviewers have 60-120s timeout (line 218). If reviewer takes 90s (near timeout), user sees progress update at start (\"Assumption Hunter dispatched [1/6]\" per line 271) then silence for 90s before next update (\"Assumption Hunter: done [1/6]\"). No indication reviewer is working vs frozen. User may interrupt thinking review is stuck. Step 2 specifies \"streams progress as reviewers complete\" and \"shows elapsed time\" (line 271) but not per-reviewer heartbeat during execution.","why_it_matters":"Long-running reviewers (complex designs, deep analysis) create UX anxiety. User doesn't know if 90s delay means \"reviewer is thinking hard\" or \"API call hung.\" For 6 parallel reviewers, max elapsed time is ~120s if one reviewer is slow—too long without feedback. For async mode (line 261), this is non-issue—user doesn't watch execution. For interactive mode, silence creates uncertainty.","suggestion":"Add per-reviewer progress updates to UX Flow Step 2. Specify: (1) Stream intermediate updates every 15-30s (\"Assumption Hunter: analyzing... [45s elapsed]\"), (2) Show all reviewers with status indicators (in-progress spinner, completed checkmark, failed X, timeout warning), (3) Add timeout warning at 80% of timeout threshold (\"Edge Case Prober approaching timeout [96s/120s], may fail\"), (4) For async mode, log final results only (no intermediate updates). For MVP, minimal: elapsed time counter for entire review run, final status per reviewer. Full per-reviewer streaming deferred to UX polish phase."}
{"type":"finding","id":"v3-edge-case-prober-015","title":"No Handling for Concurrent Reviews on Same Topic","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Output Artifacts (line 36: `docs/reviews/<topic>/`), Topic Labels (line 23)","issue":"Two users (or same user in two terminals) invoke `parallax:review` on same topic simultaneously. Both create `docs/reviews/<topic>/`, both dispatch reviewers, both write summary.md. Last writer wins, first review's findings are overwritten. No file locking, no collision detection beyond timestamped folders (line 23, which only helps if users use different topic labels). For team contexts (CLAUDE.md: \"applicable to work contexts\"), concurrent reviews are plausible.","why_it_matters":"In team contexts, concurrent reviews cause data loss. Two engineers reviewing same design doc in parallel, or user re-runs review while first run still in progress (forgot it's running in background per line 261 async mode). Overwriting findings loses work. Timestamped folders (line 23) solve collision if different timestamps (rare—collisions unlikely within same second) but don't solve concurrent writes to same timestamped folder if both reviews start simultaneously.","suggestion":"Add concurrent review handling to design. Specify: (1) Check for in-progress review (lockfile `docs/reviews/<topic>/.lock` created at start, deleted at end) before starting, (2) Timestamped folders by default for all reviews (always collision-safe but breaks diffing per Finding 13), (3) Prompt user if target folder exists and is <5 minutes old (\"Review may be in progress—abort or continue?\"), (4) Document single-user limitation if multi-user is out of scope for MVP. Recommend option 1 for MVP (lockfile, minimal implementation, prevents most collisions)."}
{"type":"finding","id":"v3-feasibility-skeptic-001","title":"Design Doc Sync Incomplete — JSONL Still Missing","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Output Artifacts, Per-Reviewer Output Format, Summary Format","issue":"Iteration 2 dominant theme was documentation debt — 23 accepted dispositions not reflected in design doc. Sync completed for most areas (voice guidelines, prompt caching, phase classification), but JSONL format specification is still completely missing. MEMORY.md states \"JSONL as canonical output format—decided but not yet implemented.\" Multiple iteration 2 disposition notes reference JSONL (\"jq filters by severity/persona/phase\"). Four reviewers flagged this independently in iteration 2. Design doc still specifies markdown-only output.","why_it_matters":"JSONL vs markdown affects finding IDs (stable JSON id field vs text hashing), disposition tracking (structured fields vs prose), CLI tooling (jq vs grep), and machine processability. This is the largest structural decision not documented. If JSONL is canonical, current markdown spec describes throwaway scaffolding. Migration later requires rewriting file I/O and parsing logic.","suggestion":"Add JSONL schema specification to design doc or explicitly defer to v2 with migration path. Minimal schema: one JSON object per finding with `{id, reviewer, severity, phase_primary, phase_contributing, section, issue, why_it_matters, suggestion, iteration, disposition, disposition_note}`. Specify whether reviewers output JSON directly or synthesizer converts markdown to JSONL. Clarify \"markdown works for MVP\" means \"sufficient for prototype\" (acceptable) or \"permanent format\" (conflicts with JSONL decision)."}
{"type":"finding","id":"v3-feasibility-skeptic-002","title":"Cross-Iteration Finding Tracking Complexity Underestimated","severity":"Critical","phase":{"primary":"design","contributing":"plan"},"section":"Cross-Iteration Finding Tracking (newly added section)","issue":"Design now specifies \"Finding IDs: Stable hash derived from section + issue content\" with note \"see Assumption Hunter Finding 3 for hash brittleness concerns\" but provides no resolution. Text-based hashing breaks when findings evolve (\"no retry logic\" iteration 1 becomes \"retry lacks backoff\" iteration 2 = different hash). Alternative is \"LLM-based matching: synthesizer compares new findings to prior findings and flags semantic overlap\" but token cost and complexity are not estimated.","why_it_matters":"LLM semantic matching requires N_new × N_prior comparisons every re-review. For 30 findings across 3 iterations: iteration 2 does 30×30=900 comparisons, iteration 3 does 30×60=1800 comparisons. Each comparison is ~200 tokens (finding A text + finding B text + \"are these the same issue?\" prompt) × $3 per million input tokens = $0.54 for iteration 2, $1.08 for iteration 3. Costs scale quadratically with iterations. Additionally, synthesizer must make 1800 semantic similarity judgments—error-prone and latency-heavy.","suggestion":"Simplify cross-iteration tracking for MVP. Option 1: Section-based anchoring (findings tracked by section heading + reviewer, simple but lossy). Option 2: Hybrid approach—hash for exact matches, flag potential semantic matches for user confirmation (synthesizer shows \"Finding 12 may relate to prior Finding 3, confirm?\"). Option 3: Defer semantic matching to v2, use iteration 1-2 data to validate it's worth the cost. Recommend option 1 or 2 for MVP."}
{"type":"finding","id":"v3-feasibility-skeptic-003","title":"Auto-Fix Git Workflow Has Unsolvable Temporal Ordering","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"UX Flow Step 4 (Auto-Fix, newly added)","issue":"Design now specifies auto-fix step: \"Synthesizer classifies findings as auto-fixable vs human-decision-required. Auto-fixable findings applied to design artifact automatically. Auto-fixes committed as separate git commit from human-reviewed changes.\" This requires: (1) auto-fixes run before user sees findings, (2) auto-fixes modify design doc and commit, (3) user then processes remaining findings and commits. If user made other edits to design doc between starting review and processing findings, auto-fix commit includes unrelated changes. If user rejects an auto-fix during processing, it's already committed. Temporal ordering is unsolvable.","why_it_matters":"Auto-fix as specified is invasive—modifies source files and commits automatically without user approval for each fix. Conflicts with Git Safety Protocol (\"NEVER commit changes unless user explicitly asks\"). Additionally, the \"separate commit\" requirement is impossible if user has intervening changes. If auto-fixes run before user review, user can't reject bad fixes. If after, separate commit is impossible (changes intermixed).","suggestion":"Radically simplify auto-fix or cut from MVP. Option 1: Defer to post-MVP—evaluate finding type distribution from first 5-10 reviews to see if auto-fix ROI justifies complexity. Option 2: Auto-fixes presented as suggested patches (diff format), user approves before application (not automatic). Option 3: Conservative criteria (formatting/whitespace only, zero semantic changes) with explicit approval. Recommend option 1—cut from MVP, validate need with data."}
{"type":"finding","id":"v3-feasibility-skeptic-004","title":"Systemic Issue Detection Requires Undefined Root Cause Taxonomy","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Finding Phase Classification (primary+contributing), Synthesis responsibilities","issue":"Design now specifies \"When >30% of findings share a contributing phase, the synthesizer flags a systemic issue.\" Implementation requires: (1) detecting which findings \"share\" a contributing phase (exact match of phase label? semantic similarity of root causes?), (2) computing percentage (how are multi-label findings counted?), (3) determining what \"systemic issue\" means (advisory flag? automatic escalation?). Root cause attribution is judgment-heavy. Synthesizer must cluster findings semantically to detect patterns.","why_it_matters":"The 30% threshold is arbitrary (why not 25%? 40%?). \"Share a common root cause\" is subjective—if 12 findings trace to \"missing error handling\" but phrased differently, does synthesizer detect this? If threshold is too low, every review triggers false systemic escalations. If too high, real systemic issues are missed. Root cause taxonomy is undefined in design (what are valid contributing phases? how granular?).","suggestion":"Make systemic detection advisory, not automatic, for MVP. Synthesizer notes when multiple findings have same contributing phase label (\"7 findings have calibrate gap as contributing phase—may indicate systemic requirements issue\") but doesn't auto-escalate. User decides whether pattern is meaningful. Remove 30% threshold (too arbitrary without empirical data). Defer automated clustering to post-MVP when eval data shows whether it adds value."}
{"type":"finding","id":"v3-feasibility-skeptic-005","title":"Verdict Still Computed Before User Processes Findings","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Verdict Logic, UX Flow Step 5 (Present Summary)","issue":"Iteration 2 flagged this (Finding 43 from Feasibility Skeptic). Design sync did not address it. UX flow still shows verdict presented in Step 5 before findings processed in Step 6. Verdict depends on severity ratings that may be false positives. If user rejects 3 Critical findings as invalid during processing, the \"revise\" verdict should become \"proceed.\" Design doesn't specify verdict recomputation.","why_it_matters":"Presenting incorrect verdict wastes cycles and creates anchoring bias. User sees \"revise\" based on 5 Critical findings, processes findings, rejects 4 as false positives. Actual verdict should be \"proceed\" but user already committed to revision mindset. Alternatively, user sees \"proceed\" (no Critical), processes findings, realizes Important finding should be Critical, but workflow said \"proceed.\"","suggestion":"Recompute verdict after finding processing based on accepted findings only. Reorder UX flow: Step 5 shows finding counts (no verdict), Step 6 processes findings, Step 7 computes final verdict based on accepted findings. Alternatively: show provisional verdict in Step 5, final verdict in Step 7, explain difference. Simplest: eliminate provisional verdict, let user decide after processing."}
{"type":"finding","id":"v3-feasibility-skeptic-006","title":"Synthesis Consolidation Heuristics Still Undefined","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Synthesis responsibilities (deduplication, contradiction surfacing)","issue":"Iteration 2 flagged this (Finding 40 from Feasibility Skeptic). Design sync did not add consolidation rules. Synthesizer must \"deduplicate findings flagged by multiple reviewers into single entries\" but design provides no heuristics. When are two findings \"the same issue\"? Same section + similar keywords? Same root cause? Same suggested fix? Iteration 2 had 55 findings across 6 reviewers—how many were duplicates? No data.","why_it_matters":"Aggressive deduplication loses nuance and reduces finding count artificially. Conservative deduplication floods user with near-duplicates. Inconsistent consolidation across review runs makes iteration comparison unreliable (\"finding count dropped from 55 to 30—did design improve or did synthesizer consolidate more?\"). Deduplication judgment directly affects verdict (Critical finding count determines revise vs proceed).","suggestion":"Define explicit consolidation rules in Synthesis section. Conservative baseline: deduplicate only if findings reference exact same section AND same root cause. Group related findings under common heading but preserve as separate entries with cross-references (\"Flagged by 3 reviewers: Assumption Hunter, Edge Case Prober, Feasibility Skeptic\"). Track deduplication decisions in summary (\"15 findings consolidated from 22 reviewer flags\"). Test on first 2-3 review runs, tune heuristics based on false merges."}
{"type":"finding","id":"v3-feasibility-skeptic-007","title":"No Exit Criteria for Re-Review Loop","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Verdict Logic (revise path), UX Flow","issue":"Iteration 2 flagged this (Finding 24 from Edge Case Prober). Design sync did not add stopping criteria. Design specifies `revise` verdict triggers re-review after user updates design. If iteration 2 produces new Critical findings (different from iteration 1), user revises again. Iteration 3 produces more findings. When does loop end? When findings = 0 (unrealistic)? When no new Critical? When user decides \"good enough\"? Design treats all Critical findings as blockers but provides no framework for acceptable risk.","why_it_matters":"Without stopping criteria, reviews iterate indefinitely. Real designs have tradeoffs—some edge cases deferred intentionally, some Important findings accepted as constraints. Current verdict logic has no \"proceed despite Critical findings with justification\" option. User needs escape hatch.","suggestion":"Add review convergence criteria to Verdict Logic section. Options: (1) Explicit threshold (\"2 consecutive iterations with no new Critical findings\"), (2) User override (\"proceed despite Critical findings\" with required justification recorded in summary), (3) Iteration count flag (if >3 iterations, prompt user to reconsider scope), (4) Add \"defer\" disposition for findings user accepts as out-of-scope. Recommend option 2 + option 4 (user control + explicit deferral tracking)."}
{"type":"finding","id":"v3-feasibility-skeptic-008","title":"Reviewer Prompt Versioning Strategy Missing","severity":"Important","phase":{"primary":"plan","contributing":"design"},"section":"Reviewer Prompt Architecture (newly added section)","issue":"Design now specifies prompt caching structure (stable prefix + variable suffix) but doesn't address prompt versioning. Iteration 2 notes \"Prompt changes to stable prefix invalidate cache and should be tracked as versioned changes\" but no versioning strategy specified. If Assumption Hunter's prompt changes between iteration 1 and iteration 2 of reviewing same design, finding differences could be due to prompt changes (not design improvements).","why_it_matters":"Eval framework depends on comparing review quality across iterations. If you can't distinguish \"design improved\" from \"prompts got better\" or \"prompts missed things,\" eval data is noisy. Cross-iteration finding tracking requires stable finding IDs, but if prompts change, same logical issue phrased differently breaks ID matching. Prompt changes invalidate cache (90% cost savings lost) so versioning has cost implications.","suggestion":"Add prompt versioning to Reviewer Prompt Architecture section. Specify: (1) Each prompt file includes version header, (2) Summary.md records prompt versions used per review run, (3) When comparing iterations, note prompt version changes, (4) Major prompt refactor increments version, findings non-comparable to prior versions. Alternatively: adopt immutable prompt strategy post-MVP (prompts locked, improvements go into new personas)."}
{"type":"finding","id":"v3-feasibility-skeptic-009","title":"Cost Per Review Run Still Unknown","severity":"Important","phase":{"primary":"plan","contributing":null},"section":"Prototype Scope (cost logging mentioned)","issue":"Iteration 2 flagged this (Finding 55 from Feasibility Skeptic). Design now mentions \"Cost logging per review run in JSONL output\" but no empirical data from prior review runs. After iteration 1 (smoke test, 44 findings) and iteration 2 (validation run, 55 findings), actual cost data should exist. How much did each review cost? Token counts? Cache hit rate? Budget validation?","why_it_matters":"Budget is $2000/month with $150-400 projected API spend. Unknown costs make iteration risky. If reviews cost $5 each, you can afford 30-60/month. If $0.20 each, hundreds. Without data, flying blind on burn rate. Critical for eval phase with many review iterations. Additionally, model tiering decisions (Haiku for mechanical reviewers) depend on cost/quality tradeoffs—need baseline.","suggestion":"Instrument next review run (iteration 3) with comprehensive cost logging. Capture: (1) input tokens per reviewer (design + requirements + system prompt), (2) output tokens per reviewer, (3) synthesizer tokens, (4) cache hit rate if enabled, (5) total cost at Sonnet pricing. Log to JSONL as specified. Use data to validate budget assumptions and inform model tiering (test Haiku for 1-2 reviewers, compare quality)."}
{"type":"finding","id":"v3-feasibility-skeptic-010","title":"Minimum Viable Reviewer Set Still Unvalidated","severity":"Important","phase":{"primary":"design","contributing":"calibrate"},"section":"Reviewer Personas (6 design-stage, 9 total across stages)","issue":"Iteration 2 flagged this (Finding 39 from Feasibility Skeptic). Design sync did not add coverage analysis methodology. Design specifies 6 reviewers for design stage, 9 total personas across stages. No evidence all are necessary or sufficient. Prior Art Scout and First Principles Challenger have overlap (both question problem framing). Edge Case Prober and Feasibility Skeptic both examine \"what could go wrong.\"","why_it_matters":"Each reviewer adds cost ($0.20-0.50 per reviewer at Sonnet pricing for 5-10k token reviews) and coordination overhead (deduplication, consolidation). If 3 reviewers catch 80% of findings, running 6 is low ROI. Conversely, if gaps exist (security review missing from design stage), 6 insufficient. Problem statement says \"optimal N is empirical\" but doesn't specify validation method.","suggestion":"Add coverage analysis to Open Questions or Evaluation section. Specify methodology: (1) Tag each finding with which reviewer(s) flagged it, (2) Calculate unique findings per reviewer (high value = keep), (3) Calculate overlap between reviewer pairs (>50% = consolidate), (4) Identify finding categories no reviewer catches (missing persona). Start with 3 core reviewers (Requirement Auditor, Feasibility Skeptic, Edge Case Prober), add one at a time, measure incremental coverage. Run on first 3-5 reviews before committing to 6."}
{"type":"finding","id":"v3-feasibility-skeptic-011","title":"Second Brain Test Case Still Not Run","severity":"Important","phase":{"primary":"design","contributing":"plan"},"section":"Prototype Scope (\"Validate with: Second Brain Design test case\")","issue":"Iteration 2 flagged this (Finding 54 from Feasibility Skeptic). Design still claims validation with \"Second Brain Design test case (3 reviews, 40+ findings in original session).\" Iteration 1 ran smoke test (parallax reviewing itself). Iteration 2 ran validation (same design doc, second review). No external test case executed. The Second Brain case is real project with known design flaws—gold standard for validation.","why_it_matters":"Self-review validates tool runs but not that it produces value. Second Brain case answers: did review system catch the same 40+ issues manual process found? Did it catch different issues? Did it miss critical issues? Did it flood with false positives? This is eval that shows whether tool works. Without it, you've built tool, tested execution, but not validated outcome quality.","suggestion":"Run Second Brain test case before finalizing design as approved. Use findings to validate: (1) personas catch real design flaws (not generic concerns), (2) finding counts comparable to manual review (not 10x higher/lower), (3) severity calibration sensible (Critical actually critical), (4) phase classification routes correctly (design flaws vs calibrate gaps). If test reveals failures, update design before marking approved."}
{"type":"finding","id":"v3-feasibility-skeptic-012","title":"Inspect AI and LangGraph Evaluation Still Deferred","severity":"Important","phase":{"primary":"design","contributing":"calibrate"},"section":"Open Questions, problem statement build-vs-leverage","issue":"Iteration 2 Prior Art Scout flagged Inspect AI (Finding 13) and LangGraph (Finding 14) as Critical—mature frameworks solving 60-80% of what's being custom-built. Disposition was \"evaluate during implementation.\" Design sync added these to \"Evaluate in early eval phase\" section but didn't specify evaluation criteria or decision framework. Custom orchestration is 40-60% of implementation surface area.","why_it_matters":"CLAUDE.md says \"BUILD adversarial review (novel), LEVERAGE mature frameworks\" but design custom-builds infrastructure. Novel contribution (finding classification, persona prompts) is 20% of surface area. Building custom orchestration (reviewer dispatch, state management, retry logic, progress tracking) is 80%. If Inspect AI and LangGraph already solve this, custom build is wasted effort and maintenance burden.","suggestion":"Add evaluation criteria to design. Specify decision framework: (1) Prototype single reviewer as Inspect solver, measure integration effort, (2) Prototype finding processing with LangGraph state management, validate human-in-loop patterns work, (3) Compare custom vs framework on: implementation time, maintenance burden, feature completeness, (4) Decision rule: if framework covers >70% of needs with <2x integration complexity, adopt framework. Document decision in design whether framework or custom."}
{"type":"finding","id":"v3-first-principles-001","title":"Documentation Sync Treated as Design Revision Conflates Two Different Actions","severity":"Critical","phase":{"primary":"calibrate","contributing":"design"},"section":"Iteration 2 verdict reasoning, documentation debt pattern","issue":"Iteration 2 verdict is \"revise\" based on 15 Critical findings. 10 of those are \"accepted iteration 1 findings not yet reflected in design doc\" (67% of Requirement Auditor findings flagged as documentation debt). These aren't design flaws requiring rethink—they're accepted decisions requiring documentation updates. User accepted the design decisions in iteration 1, implemented them in code/prompts (Task 8), didn't update design doc. Iteration 2 re-flags same issues as Critical. Treating documentation sync as design revision conflates \"design needs improvement\" (substantive rework) with \"design doc needs updates to match implemented design\" (writing).","why_it_matters":"Documentation debt is process failure (design doc not kept in sync with implementation), not design failure (wrong architecture, missed edge case). Requiring design revision when actual action is \"update docs to match code\" wastes iteration cycles. Iteration 2 summary correctly diagnoses this as systemic issue (\"design doc treated as static artifact rather than living spec\") but still routes to \"revise\" verdict. This creates false signal—design is good (evidenced by Task 8 implementing accepted decisions), documentation is stale.","suggestion":"Add documentation_sync verdict type distinct from revise. If majority of Critical findings are \"accepted decision not yet documented,\" verdict should be \"sync documentation\" with estimated time (1-2 hours writing), not \"revise design\" (implying rearchitecture). Alternatively: build documentation sync into the review workflow—when user accepts findings and implementation updates occur, prompt to sync design doc in same commit. Prevents documentation drift from becoming systemic debt. Iteration 2 flagged this pattern—iteration 3 should test whether design doc sync workflow prevents recurrence."}
{"type":"finding","id":"v3-first-principles-002","title":"Testing Orchestration Infrastructure by Building Design Review First Inverts Risk","severity":"Critical","phase":{"primary":"calibrate","contributing":"design"},"section":"Prototype Scope, problem statement framing","issue":"Problem statement identifies \"No structured requirement refinement\" as root cause of design failures, explicitly states \"requirement refinement has been the single biggest design quality lever in practice,\" then prototypes design-stage review to \"validate orchestration mechanics.\" Requirements review is deferred to eval phase. This validates infrastructure (parallel agent dispatch, finding consolidation) using a phase acknowledged to have lower leverage than the deferred phase. Iteration 2 prior review flagged this (Finding 10, Finding 12, Finding 27)—all noted as \"valid but exploratory\" in dispositions. Design doc sync (this iteration) still doesn't address the inversion.","why_it_matters":"If requirements review is actually the highest-leverage intervention (as stated), weeks spent tuning design-stage personas prove orchestration works but not that the right thing is being orchestrated. When you eventually build requirements review, you may discover it needs different persona types (Product Strategist added to requirements stage but not tested), different output format (success criteria vs design tradeoffs), different finding classification (missing requirement vs wrong architecture). This invalidates design-stage infrastructure decisions. The prototype tests \"can we dispatch agents in parallel?\" (yes, superpowers already does this) instead of \"does multi-perspective requirements review prevent downstream design failures?\" (unknown, highest value).","suggestion":"Flip the prototype order. Build requirements-stage review first with 4 requirement-focused personas (Product Strategist, Assumption Hunter, Requirement Auditor, First Principles). Run it against a real historical project where bad requirements led to design failures (Second Brain candidate—problem statement says \"review findings revealed design flaws that should have been caught during research phase\"). Compare review findings to actual downstream failures. If requirements review catches 60%+ of issues that became design problems, you've validated the value hypothesis and can confidently build design/plan review infrastructure. If it catches <40%, the hypothesis is wrong and orchestration doesn't matter. Alternatively: accept this is an infrastructure validation prototype (not value validation) and reframe scope explicitly."}
{"type":"finding","id":"v3-first-principles-003","title":"\"Adversarial Review\" is Coverage-Based Inspection, Not Adversarial Debate","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"Reviewer Personas, core hypothesis, overview","issue":"Design claims \"adversarial multi-agent design review\" as differentiator. The 6 design personas (Assumption Hunter, Edge Case Prober, Requirement Auditor, Feasibility Skeptic, First Principles, Prior Art Scout) are scoped by domain coverage—what part of the design to inspect. They can all be simultaneously correct because they're examining different surfaces. True adversarial review requires incompatible incentives forced to reconcile: \"ship fast, prove in production\" vs \"ship safe, prove upfront.\" Mitsubishi's adversarial debate AI (Jan 2026 validation) uses opposing models arguing for contradictory conclusions. Design is comprehensive checklisted inspection, not adversarial debate. Iteration 2 prior review flagged this (Finding 11: \"Adversarial review is misnamed\"), disposition said \"valid critique of problem framing, evaluate empirically in prototype.\"","why_it_matters":"If core hypothesis is \"adversarial tension surfaces design blind spots better than single-perspective review,\" your persona architecture doesn't test it. You're testing \"more inspectors find more issues\" (obviously true, not novel). The finding consolidation synthesizer deduplicates parallel findings—additive coverage. Adversarial debate synthesizer would reconcile opposing positions—forcing design to satisfy contradictory constraints reveals tradeoffs. Design doc sync (this iteration) added 23 accepted dispositions but didn't change persona architecture to address this.","suggestion":"Either (a) rename to \"comprehensive multi-perspective design review\" and acknowledge adversarial tension is deferred, OR (b) redesign 2-3 personas as stance-based adversaries with opposing success criteria. Example pairs: Optimizer (minimize complexity, defer edge cases, ship fast) vs Hardener (demand robustness, block on unknowns). User-Centric (prioritize UX even if implementation cost high) vs Operator-Centric (prioritize maintainability even if UX suffers). Require reconciliation with tradeoff justification. Force the designer to choose between contradictory goods, not accumulate independent findings. Alternatively: test both architectures in eval (coverage-based vs stance-based) and compare which surfaces more valuable findings. This was flagged in iteration 2 Finding 11 as exploratory—iteration 3 design doc still doesn't acknowledge naming mismatch."}
{"type":"finding","id":"v3-first-principles-004","title":"Solving \"Lack of Review Automation\" When Problem is \"Skipping Requirements Phase\"","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"Problem statement, desired workflow","issue":"Observed pain points trace to \"no structured requirement refinement—jumped from idea to design without prioritizing.\" Desired workflow shows calibrate phase with MoSCoW, anti-goals, success criteria, human checkpoint. All pain points are upstream of design (dimension mismatch should have been caught in requirements, open questions should have been resolved before design, dropped features indicate wrong prioritization). Design builds review automation to catch design flaws adversarially. This treats symptom (designs have flaws) not cause (teams skip requirements because ROI isn't obvious). Iteration 2 prior review Finding 12 flagged this—disposition: \"valid but exploratory.\"","why_it_matters":"Automating design review doesn't solve \"teams skip requirements.\" It moves problem detection downstream. You'll catch design flaws adversarially but requirement-level errors still compound into implementation failures. If real problem is \"requirements phase is skipped because value isn't obvious until later when errors surface,\" you need to make requirements refinement's value obvious immediately. Building calibrate skill that takes 5 minutes and produces MoSCoW + anti-goals + testable success criteria, then demonstrating it reduces downstream design errors, proves its own value and solves the skipping problem.","suggestion":"Reframe problem as \"requirement refinement is skipped because ROI isn't obvious\" and build parallax:calibrate first. Make it fast (5-minute interaction), high-value (prevents known failure modes), and self-demonstrating (run calibrate on this project, compare requirements doc produced to historical designs without it). If calibrate prevents 60%+ of observed design failures, you've proven value. Then build design review as validation layer (catching what calibrate missed), not primary intervention. This inverts current architecture but aligns with stated problem framing."}
{"type":"finding","id":"v3-first-principles-005","title":"Phase Classification Routing Logic Assumes Linear Causality","severity":"Critical","phase":{"primary":"design","contributing":"calibrate"},"section":"Finding Phase Classification, Verdict Logic, Synthesis","issue":"Reviewers classify findings by primary phase (survey/calibrate/design/plan) with optional contributing phase. Verdict logic routes on primary phase only: calibrate gap → escalate to requirements, design flaw → revise design. Real design failures are multi-causal: missing research (survey) leads to unstated assumption (calibrate gap) enables flawed design (design flaw) that's hard to implement (plan concern). Forcing single-phase classification loses multi-causal reality. Design doc now includes primary+contributing classification (accepted from iteration 1 Finding 7 disposition, synced in this iteration) but verdict logic still routes on primary phase only. Contributing phase is metadata, not operationalized in routing.","why_it_matters":"If finding is \"design flaw (primary) caused by calibrate gap (contributing),\" verdict routes to design revision when it should escalate to requirements. Fixing design without fixing upstream calibrate gap produces local patch, not systemic fix. Iteration 2 summary shows 7 calibrate gaps flagged (some contributing, some primary) but verdict is \"revise\" (design-level action). Design doc sync added primary+contributing to Finding Phase Classification section but didn't update Verdict Logic to use contributing phases for escalation triggers. If contributing phases aren't escalation signals, classification is decorative.","suggestion":"Revise verdict logic to treat contributing phases as escalation signals. Rule: If any finding has \"calibrate gap (contributing)\" or \"survey gap (contributing),\" verdict includes \"address upstream issue\" action—not just \"revise design.\" Synthesizer should aggregate: if >30% of findings share same contributing phase, that phase failed systemically and requires rework regardless of primary classifications. User must acknowledge systemic upstream issue before proceeding. Design doc now documents primary+contributing but doesn't operationalize it—iteration 3 should validate whether synced design doc reflects intended verdict behavior."}
{"type":"finding","id":"v3-first-principles-006","title":"JSONL Format Decided But Design Specifies Markdown Throughout","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Output Artifacts, Per-Reviewer Output Format, Summary Format","issue":"Memory states \"JSONL as canonical output format—decided but not yet implemented.\" Multiple iteration 1 disposition notes reference JSONL enabling features (Finding 14: \"JSONL format enables this naturally—jq filters by severity/persona/phase\"). Design document specifies markdown output format throughout with zero JSONL specification. Iteration 2 flagged this (Finding 9: 4 reviewers consensus). Design doc sync (this iteration) accepted 23 dispositions but didn't add JSONL schema. This isn't \"deferred to post-MVP\" (acceptable)—it's architectural decision (canonical format) not reflected in design.","why_it_matters":"If JSONL is decided canonical format, markdown specification in design is throwaway scaffolding. Building markdown-first then migrating to JSONL requires rewriting file I/O, parsing logic, finding ID generation, disposition tracking. JSONL vs markdown affects finding ID stability (hash vs structured ID field), disposition tracking (structured fields vs prose), tool consumption (jq vs grep), cross-iteration diffing (structured comparison vs text parsing). Deferring implementation is YAGNI (good). Deferring design of decided format means markdown format may bake in assumptions that conflict with JSONL schema (bad). Four reviewers flagged this in iteration 2—iteration 3 design doc sync still doesn't address it.","suggestion":"Either (a) add JSONL schema specification to design (finding format, summary format, disposition schema) with migration path from markdown, OR (b) reverse the decision—acknowledge markdown is MVP and JSONL is speculative v2 enhancement pending eval data. Clarify \"canonical format\" means long-term target (design now, implement later) vs \"current format\" means markdown sufficient (JSONL only if eval proves value). If JSONL is decided, design must specify schema even if implementation deferred—schema design affects current markdown format choices. Iteration 2 flagged this as highest-consensus finding (4 reviewers)—iteration 3 should resolve the ambiguity."}
{"type":"finding","id":"v3-first-principles-007","title":"Design-After-Implementation Creates Rationalization Risk","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Entire design document, prototype scope","issue":"Memory context shows 11 commits on feature/parallax-review branch before this design review (implementation exists). Design document describes system already built. Standard practice: design review before implementation. Post-implementation design docs rationalize what was built (glossing over implementation complexity, ideal-case framing) rather than specify buildable reality. This review examines idealized description, not actual system. Iteration 2 Feasibility Skeptic Finding 1 flagged this. Design doc sync (this iteration) synced 23 accepted dispositions but doesn't address post-hoc nature. If implementation diverged during development, findings are academic.","why_it_matters":"Reviewing a design written after implementation validates documentation quality, not design quality. The test that matters is \"does implementation match spec?\" which is inverted—spec was written to match implementation. Iteration 2 documentation debt finding (67% of Requirement Auditor issues) confirms: design doc is lagging indicator of implementation decisions, not leading blueprint. Three review iterations have now occurred post-implementation—validating design doc completeness, not design feasibility. This is acceptable for dogfooding (testing review skill on itself) but doesn't validate core hypothesis (multi-perspective review prevents design failures).","suggestion":"Acknowledge this is design extraction (documenting what was built for future reference) not design specification (blueprint for implementation). Review for completeness and accuracy, not feasibility. Include implementation artifacts (actual agent prompts, code snippets, error handling logic, test results) in next review iteration so findings reference reality. Better: establish design-before-implementation discipline for future phases (requirements review, plan review) where you can validate \"is this buildable?\" before building. Use iterations 1-3 as calibration for what post-hoc design review can/can't validate. Mark design doc status as \"post-implementation documentation\" not \"pre-implementation specification.\""}
{"type":"finding","id":"v3-first-principles-008","title":"Requirements Versioning Untracked Creates False Resolution Signals","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Skill Interface, Cross-Iteration Finding Tracking","issue":"All reviewer prompts reference requirements document as immutable input. Cross-iteration tracking (added to design doc in this iteration, accepted from iteration 1 Finding 5 disposition) detects when findings are resolved between iterations. In real design workflows, adversarial review findings trigger requirement clarification (\"we need to specify what happens at scale\" → requirement gets added to requirements doc). Next review cycle operates against updated requirements. Reviewers can't distinguish \"design improved to satisfy requirement\" from \"requirement was lowered/changed to match design.\" This produces false \"finding resolved\" signals when requirement moved, not design fixed. Iteration 2 Assumption Hunter Finding 1 flagged this.","why_it_matters":"Finding classification routes errors to pipeline phase that failed. If design finding from iteration 1 is \"resolved\" in iteration 2 because requirement was relaxed (not design improved), that's calibrate gap disguised as design improvement. Reviewer sees design now satisfies requirement X, doesn't see requirement X was rewritten. Should escalate to calibrate (requirement changed between iterations), not mark as design-resolved. Cross-iteration tracking (synced in this iteration) without requirement versioning can't detect this. Design doc now includes cross-iteration tracking mechanism but doesn't specify requirement versioning.","suggestion":"Add requirement versioning to Cross-Iteration Finding Tracking section. Timestamp or hash requirements doc, reviewers note which version they reviewed against. Synthesizer flags requirement changes across iterations. Pass git diff of requirements to reviewers on re-review (\"requirements changed: added failure mode section, relaxed latency constraint\"). Alternatively: require explicit user confirmation when re-reviewing with modified requirements—warn that finding resolution may be due to requirement changes, not design improvements. This was flagged in iteration 2—design doc sync should have included it if cross-iteration tracking was added."}
{"type":"finding","id":"v3-first-principles-009","title":"Building Custom Infrastructure When Mature Frameworks Exist","severity":"Critical","phase":{"primary":"calibrate","contributing":"design"},"section":"Entire design—Reviewer dispatch, Synthesis, UX Flow, Pipeline Integration","issue":"Design custom-builds parallel reviewer dispatch, finding consolidation, retry logic, timeout handling, state management for finding processing, cross-iteration tracking, and verdict computation. Iteration 2 Prior Art Scout identified: Inspect AI provides multi-agent patterns with handoff primitives and retry/timeout (Finding 13), LangGraph solves stateful workflows and human-in-the-loop gates (Finding 14), LangSmith provides annotation UI for finding processing (Finding 44), Braintrust provides LLM-as-judge for severity normalization (Finding 45). All are in tooling budget, MIT licensed, production-grade. Design builds 80% custom infrastructure for 20% novel contribution (persona prompts, phase classification). Design doc sync (this iteration) accepted these findings as \"evaluate during implementation\" but didn't update design to reflect evaluation results.","why_it_matters":"CLAUDE.md explicitly states \"BUILD adversarial review (novel), LEVERAGE LangGraph + Inspect AI + Claude Code Swarms (mature).\" Design inverts this—custom-builds infrastructure (orchestration, state, UI) and treats persona prompts as configuration. Maintaining custom agent dispatch, result collection, retries, timeout handling, progress tracking, cost estimation, failure recovery is 40-60% of implementation surface area. Inspect AI is purpose-built for multi-agent LLM evaluation (UK AI Safety Institute project, actively maintained). Using Inspect positions this as domain-specific prompt engineering (the actual novel contribution) rather than infrastructure work. Iteration 2 flagged this with 4 Critical findings from Prior Art Scout—iteration 3 design doc sync doesn't address build-vs-leverage decision.","suggestion":"Evaluate Inspect AI as implementation substrate before continuing custom orchestration development. Prototype reviewer personas as Inspect solvers with custom system prompts. Use Inspect's multi_agent pattern for parallel dispatch, scorer API for synthesis, trace collection for cost tracking. Reserve custom orchestration only if Inspect proves insufficient after prototyping. If Inspect works, 60% of design surface area becomes \"use Inspect's patterns\" and focus shifts to persona prompt quality (the novel work). This is build-vs-leverage decision that should have been explicit in calibrate phase—iteration 2 identified prior art exists, iteration 3 should evaluate before committing to custom build."}
{"type":"finding","id":"v3-first-principles-010","title":"Verdict Computed Before User Processes Findings Wastes Cycles","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Verdict Logic, UX Flow Step 4-6","issue":"UX flow presents verdict to user in Step 4 (after synthesis, before finding processing). Verdict is computed from all findings including those with incorrect severity (false positives). User processes findings in Step 5, rejects 3 Critical findings as invalid. Verdict presented in Step 4 was \"revise\" (based on 5 Critical findings). After rejecting 4 Critical findings as false positives, should have been \"proceed.\" Design doesn't specify whether verdict is recomputed after processing or remains unchanged. Iteration 2 Feasibility Skeptic Finding 43 flagged this.","why_it_matters":"Presenting verdict before user validates findings wastes cycles. User sees \"revise\" verdict, spends 20 minutes processing 41 findings, rejects 4 Critical findings as invalid (wrong assumptions, out of scope, misread design). Correct verdict was \"proceed with noted improvements.\" If verdict is recomputed post-processing, Step 4 verdict is provisional (confusing). If verdict is not recomputed, it's based on unvalidated findings (incorrect). Design doc doesn't specify verdict recomputation logic.","suggestion":"Eliminate provisional verdict or recompute post-processing. Option 1: Show finding counts and severity distribution in Step 4, compute verdict in Step 6 after user processing (based on accepted findings only). Option 2: Show provisional verdict in Step 4 with \"pending validation\" status, recompute final verdict in Step 6. Option 3: Keep current flow but label verdict as \"based on unprocessed findings—may change after review.\" This affects UX flow specification—iteration 3 should clarify verdict timing."}
{"type":"finding","id":"v3-first-principles-011","title":"Async-First Architecture is File-Based Output, Not Background Execution","severity":"Important","phase":{"primary":"design","contributing":null},"section":"UX Flow, iteration 1 Finding 2 disposition, Output Contract","issue":"Iteration 1 Finding 2 disposition states \"Async is the default—review always writes artifacts to disk. Interactive mode is convenience layer.\" Design doc now includes this in Output Contract (synced this iteration). This describes file-based output (disk artifacts vs in-memory), not async execution (background processing). True async requires: review runs without blocking terminal, user can close session and resume later, finding processing state persists across sessions. Current design: sync execution (terminal occupied during review), file-based output (artifacts on disk), optional interactive processing (same session). Background automation (CLAUDE.md track 6) requires true async.","why_it_matters":"If review takes 5 minutes, that's 5 minutes terminal occupancy. User can't work on other tasks during review. Calling this \"async-first\" is misleading—it's \"disk-first\" (good for persistence) but not \"background-capable\" (required for automation). File-based output enables async processing (process findings hours later) but execution is still blocking. True async would support --background flag, session-independent finding processing, in-progress review status checks. Design doc sync added \"async-first\" language but doesn't distinguish execution mode from output mode.","suggestion":"Distinguish execution mode (sync/async) from output mode (interactive/file-based). Current design: sync execution, file-based output with optional interactive processing. For true async: support --background flag (review runs detached), session-independent finding processing (resume on any terminal/machine), in-progress review status. Defer true async to post-MVP but document distinction—current architecture enables async finding processing (good first step), not async review execution (required for automation track). Update Output Contract to clarify \"async-first\" means file-based artifacts enabling deferred processing, not background execution."}
{"type":"finding","id":"v3-first-principles-012","title":"Minimum Viable Reviewer Set is Empirical Question Built Into Design","severity":"Important","phase":{"primary":"calibrate","contributing":"design"},"section":"Reviewer Personas, Open Questions","issue":"Design specifies 6 reviewers for design stage, acknowledges \"optimal number is empirical question for eval framework,\" but builds 6-reviewer architecture into skill. No coverage analysis, overlap measurement, or diminishing returns calculation. Prior Art Scout and First Principles Challenger have overlap (both question whether design should exist). Edge Case Prober and Feasibility Skeptic both examine \"what could go wrong.\" Iteration 2 Feasibility Skeptic Finding 39 flagged this. Design doc sync (this iteration) didn't change reviewer count or add empirical validation plan.","why_it_matters":"Each reviewer adds cost and coordination overhead. Design treats 6 reviewers as baseline when it's an untested hypothesis. Should be: start with 2-3 core reviewers, add one at a time, measure incremental value. Problem statement says \"optimal N is empirical\"—design should enable empirical testing, not hardcode 6. If first 3 reviewers catch 80% of findings, running all 6 is low ROI. Three review iterations have now occurred—coverage analysis data should exist but isn't referenced in design.","suggestion":"Build reviewer set as configurable parameter, not hardcoded architecture. Enable coverage analysis during prototyping: for each finding, tag which reviewer(s) flagged it. After iterations 1-3 (data now exists), analyze: reviewers that consistently flag unique findings (high value, keep), reviewer pairs with >50% overlap (consolidate), finding categories no reviewer catches (missing persona). Start with 3 core reviewers (Requirement Auditor, First Principles, Edge Case Prober), add personas incrementally based on gap analysis. Treat 6 reviewers as hypothesis to validate, not baseline. Three iterations complete—run coverage analysis before iteration 4."}
{"type":"finding","id":"v3-first-principles-013","title":"Critical-First Mode Creates Orphaned Finding Debt Across Iterations","severity":"Important","phase":{"primary":"design","contributing":null},"section":"UX Flow Step 4 (Critical-first mode), Cross-iteration tracking","issue":"Critical-first processing mode addresses only Critical findings, user revises design, re-reviews. Iteration 2 produces new findings. User processes Critical-first again. Important/Minor findings from iteration 1 are never processed—orphaned. After 3 Critical-first iterations, 40+ unprocessed findings accumulate across iterations. No mechanism tracks which findings are carried forward unprocessed vs obsoleted by design changes. Iteration 2 Edge Case Prober Finding 25 flagged this. Design doc sync (this iteration) added Critical-first mode specification but didn't add orphaned finding management.","why_it_matters":"Critical-first enables fast iteration but creates technical debt of unprocessed findings. Eventually user must process accumulated findings or accept valuable feedback was discarded. Design provides no orphaned finding management—when does user process deferred findings? How do you know if iteration 1 Important finding is still valid after iteration 3 design changes? Cross-iteration tracking (synced this iteration) tracks finding status but doesn't specify handling of perpetually-deferred findings.","suggestion":"Add orphaned finding management to Critical-first workflow specification. After Critical-first processing, mark remaining findings as \"deferred to next iteration.\" On re-review, synthesizer reconciles prior deferred findings with new findings (same issue? obsoleted by design change? still relevant?). After revise loop converges (no new Critical findings), prompt user to process accumulated deferred findings. Alternatively: include Important findings related to same subsystem as Critical findings (grouped processing). This should be in UX Flow Step 4 specification—iteration 3 design doc sync missed it."}
{"type":"finding","id":"v3-prior-art-scout-001","title":"Design Doc Sync Addresses V2 Documentation Debt But Misses Architectural Gaps","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Entire design document","issue":"V2 identified 10 accepted findings from iteration 1 not reflected in design doc. User backported 23 dispositions (per MEMORY.md), addressing documentation debt. However, architectural specification gaps flagged as Critical in V2 remain unaddressed: (1) JSONL output format (V2 Finding 9, flagged by 4 reviewers), (2) Auto-fix mechanism (V2 Finding 2), (3) Cross-iteration finding tracking (V2 Finding 3, flagged by 3 reviewers), (4) Prompt caching architecture (V2 Finding 4). These are structural decisions affecting storage, parsing, and implementation substrate—not cosmetic documentation updates.","why_it_matters":"V2 verdict was \"revise\" specifically because \"these aren't new discoveries requiring redesign—they're accepted decisions not yet documented.\" Design doc sync task addressed disposition notes (decisions) but not architectural specifications (how those decisions manifest in the system). Without JSONL schema, auto-fix workflow, finding ID mechanism, and prompt structure specifications, the design remains incomplete for implementation. This is the difference between \"we decided to do X\" (documented) and \"here's how X works\" (still missing).","suggestion":"Add four architectural specification sections to design: (1) \"Output Format Architecture\" specifying JSONL schema, field definitions, markdown rendering relationship, and migration path. (2) \"Auto-Fix Workflow\" specifying classification criteria, agent responsibilities, validation, and commit strategy. (3) \"Finding Persistence Mechanism\" specifying ID generation algorithm (semantic matching via LLM per V2 Finding 7 suggestion, not brittle hashing), status tracking, and cross-iteration diff format. (4) \"Reviewer Prompt Architecture\" specifying stable prefix structure (persona + methodology + format + voice), variable suffix structure (artifacts + iteration context), cache boundary, and versioning strategy. These specifications enable implementation; disposition notes alone do not."}
{"type":"finding","id":"v3-prior-art-scout-002","title":"Build-vs-Leverage Evaluation Still Deferred to \"When Limits Hit\"","severity":"Critical","phase":{"primary":"calibrate","contributing":"design"},"section":"Implicit throughout (no leverage evaluation section in design)","issue":"CLAUDE.md states \"BUILD adversarial review (novel), LEVERAGE LangGraph + Inspect AI + Claude Code Swarms (mature).\" V2 Prior Art Scout flagged Inspect AI (V2 Finding 13, Critical) and LangGraph (V2 Finding 14, Critical) as solving 80% of custom-built infrastructure. Design doc states \"LangGraph: Start native, evaluate when limits hit\" and has no Inspect AI evaluation. This defers leverage decisions to implementation phase after custom orchestration is already built. Standard practice: evaluate build-vs-leverage during design, not after building.","why_it_matters":"Custom-building parallel reviewer dispatch, finding consolidation, retry logic, timeout handling, result aggregation, state management, and human-in-the-loop gates is 40-60% of implementation surface area. If Inspect AI or LangGraph provides these as battle-tested primitives, parallax becomes domain-specific prompt engineering (20% of work, 100% of novel contribution) rather than infrastructure + prompts (80% plumbing + 20% novel). \"Evaluate when limits hit\" means building custom infrastructure first, discovering it's harder than expected, then evaluating alternatives when already committed. This is backwards—prototyping on existing frameworks validates feasibility before custom work.","suggestion":"Add \"Implementation Substrate Evaluation\" section to design specifying: (1) Prototype reviewer dispatch using Inspect AI's `multi_agent` pattern (1-2 hour spike), validate whether it supports parallel execution, retry/timeout, result collection. If insufficient, document specific gaps. (2) Prototype finding processing using LangGraph state graphs (1-2 hour spike), validate whether `interrupt` for human gates and `state` for disposition tracking meet requirements. If insufficient, document gaps. (3) Decision matrix: if both frameworks meet 80%+ of requirements, use them (reduce maintenance burden). If critical gaps exist, custom-build only the missing 20% as extensions. (4) Document evaluation results in design before committing to implementation approach. This follows CLAUDE.md's \"prototype-first\" philosophy—build to understand, don't design to death. Two 2-hour spikes de-risk the entire architecture."}
{"type":"finding","id":"v3-prior-art-scout-003","title":"Inspect AI Multi-Agent Patterns Are Production-Grade (Not Experimental)","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"V2 Finding 13 disposition, overall architecture","issue":"V2 Prior Art Scout flagged Inspect AI as solving orchestration (V2 Finding 13, Critical). Design doc doesn't reference Inspect AI. Web search reveals Inspect AI (2026) is production infrastructure: UK AI Safety Institute project, supports agent evaluations with flexible built-in agents, multi-agent primitives with handoff, arbitrary external agent support (Claude Code, Codex CLI), comprehensive trace analysis. 89% of organizations have implemented observability for agents, 62% have detailed tracing. Inspect provides exactly what parallax:review needs: multi-agent dispatch, tool-use evaluation, trajectory scoring, state tracking. This is not experimental—it's industry standard for agent evaluation in 2026.","why_it_matters":"Treating Inspect AI as \"future consideration\" ignores that evaluation frameworks have matured beyond simple metrics to comprehensive systems capturing reasoning quality, tool selection, and multi-agent coordination. Parallax:review IS an evaluation task—it evaluates design quality using multi-agent patterns. Inspect AI was designed for this exact use case. Not evaluating it before custom-building orchestration is building blindfolded when a standard solution exists.","suggestion":"Run Inspect AI prototype immediately (before finalizing design). Minimal test: implement 2 reviewer personas (Assumption Hunter, Edge Case Prober) as Inspect solvers, dispatch them in parallel against parallax's own design doc, collect findings via Inspect's trace system. Validate: (1) Does `multi_agent` pattern support parallel dispatch with progress tracking? (2) Does retry/timeout work as expected? (3) Can finding consolidation use Inspect's scorer API? (4) Does cost tracking integrate with Inspect's built-in telemetry? If yes to 3+, adopt Inspect as substrate. If no, document specific blockers and architect around them. Budget: 3-4 hours for spike. Payoff: de-risks 40-60% of implementation."}
{"type":"finding","id":"v3-prior-art-scout-004","title":"LangGraph Human-in-the-Loop Is First-Class (Not Afterthought)","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"UX Flow (Step 5: Process Findings), V2 Finding 14 disposition","issue":"V2 Prior Art Scout flagged LangGraph for stateful workflow and human gates (V2 Finding 14, Critical). Design doc states \"evaluate LangGraph when limits hit\" but doesn't specify what \"limits\" would trigger evaluation. Web search reveals LangGraph (2026) treats human-in-the-loop as first-class: workflows pause for manual oversight while retaining full context, persistence saves state after every step, `interrupt` feature pauses graphs waiting to be resumed, checkpointers save StateSnapshots at every super-step. This is exactly what parallax:review's interactive finding processing requires: pause for human review, persist disposition state, resume from exact position after interruption.","why_it_matters":"V2 identified state management gaps: Finding 21 (Important, crash/interruption loses progress), Finding 18 (Important, no resumption specification for async mode), Finding 19 (Important, timestamped folders break git diff workflow), Finding 28 (Important, single-user assumption, no multi-user support). All four findings are state management problems. LangGraph solves state management as its core value proposition—central memory that every step reads/updates, persistence across interruptions, explicit checkpointing. Custom-building this means reimplementing what LangGraph provides as production-grade primitives.","suggestion":"Same as Finding 2—prototype LangGraph for finding processing workflow. Minimal test: model review process as state graph (dispatch → synthesize → process findings loop → wrap up). Validate: (1) Does `state` properly track finding dispositions across processing loop? (2) Does `interrupt` pause at human gates without losing context? (3) Can user close terminal and resume processing later? (4) Does persistence enable multi-user workflows (Alice processes findings 1-5, Bob continues from 6)? If yes to 3+, use LangGraph for control flow. Budget: 2-3 hours. Payoff: solves 4 Important findings from V2."}
{"type":"finding","id":"v3-prior-art-scout-005","title":"JSONL Format Decision Needs Schema Specification (Not Just Intent)","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Output Artifacts, V2 Finding 9","issue":"V2 Finding 9 (Critical, 4 reviewers) flagged JSONL format missing from design. MEMORY.md states \"JSONL as canonical output format—decided but not yet implemented.\" Design doc sync (23 dispositions) didn't add JSONL specification. JSONL is not a future enhancement—it's foundational architecture affecting: (1) how finding IDs are generated and stored (JSON `id` field vs text parsing), (2) how dispositions are tracked (structured `status`/`disposition` fields vs markdown prose), (3) how tools consume output (jq filtering vs grep/awk), (4) how cross-iteration tracking works (JSON diffing vs LLM parsing). Deciding \"we'll use JSONL\" without specifying schema defers the hard design questions.","why_it_matters":"Current design commits to markdown output format throughout. If JSONL is the intended canonical format, every section describing output format is specifying transitional scaffolding, not the actual system. Prototyping in markdown then migrating to JSONL later requires either (1) rewriting all file I/O and parsing, or (2) maintaining two output formats indefinitely. Schema decisions (field names, nesting structure, relationship encoding) affect how findings are deduplicated, how severity ranges are represented, how phase classification is stored (single-label vs multi-label), and how cross-iteration diffs are computed.","suggestion":"Add \"Output Format Architecture\" section specifying JSONL schema even if implementation is deferred. Minimal schema: one JSON object per finding with fields `{id, iteration, reviewer, severity, phase_primary, phase_contributing, section, issue, why_it_matters, suggestion, disposition, disposition_note, timestamp}`. Specify: (1) Finding ID generation (semantic hash or LLM-based matching—see Finding 7 for hash brittleness), (2) Severity range encoding (array `[\"Critical\", \"Important\"]` when reviewers disagree), (3) Phase classification encoding (separate fields for primary/contributing or nested object), (4) Cross-iteration status (`open`/`addressed`/`rejected`/`partial`), (5) Markdown rendering relationship (JSONL is storage, markdown is human-readable view generated from JSONL, not separate format). Clarify whether reviewers output JSONL (affects prompt complexity) or synthesizer converts markdown to JSONL (affects parsing logic)."}
{"type":"finding","id":"v3-prior-art-scout-006","title":"Auto-Fix Criteria Conservative But Implementation Workflow Unsafe","severity":"Important","phase":{"primary":"design","contributing":null},"section":"V2 Finding 2 (auto-fix requirement), Synthesis section (should include auto-fix step)","issue":"V2 Finding 2 (Critical) flagged auto-fix requirement missing. Design doc sync didn't add auto-fix specification. Auto-fix was accepted in V2 with disposition: \"Auto-fix step between synthesis and human processing. Git history must show auto-fixes as separate commit.\" V2 Finding 20 (Important) flagged git safety concern: auto-fix modifies design doc before user sees findings, conflicts with Git Safety Protocol (\"NEVER commit changes unless user explicitly asks\"). These findings are in tension—auto-fix before human review enables automatic application but violates consent; auto-fix after human review requires user to pre-approve fixes without seeing them applied.","why_it_matters":"Auto-fix is high-value for obvious corrections (typos, broken internal links, path corrections) but dangerous if applied without user visibility. Standard pattern for auto-fix: show user proposed changes as diff/patch, get approval, then apply. This requires specification in design: (1) how fixes are presented (inline diff, separate file, patch format), (2) when user approves (before or after seeing other findings), (3) how approval is captured (interactive prompt, config flag, explicit command), (4) how fixes are validated before committing (subset of reviewers re-check auto-fixed sections).","suggestion":"Add \"Auto-Fix Workflow\" section to design specifying conservative MVP approach: (1) Synthesizer classifies findings as auto-fixable using strict criteria (typos in prose only—no code, paths, or structural changes), (2) Auto-fixable findings presented to user as \"can be auto-fixed\" flag in summary, (3) User explicitly approves auto-fix set before application (\"Apply 7 auto-fixes? [y/n]\"), (4) Fixes applied to design doc copy (not original), diff shown for review, (5) User confirms, then fixes committed as separate git commit with message listing fixed finding IDs, (6) Re-review triggered automatically on auto-fixed sections only (subset of reviewers, focused scope). This balances automation value with git safety and user consent. Expand auto-fix criteria based on eval data showing false-positive rate is acceptable."}
{"type":"finding","id":"v3-prior-art-scout-007","title":"Cross-Iteration Finding Tracking Needs LLM-Based Matching (Hashing Insufficient)","severity":"Important","phase":{"primary":"design","contributing":null},"section":"V2 Finding 3 (cross-iteration tracking), V2 Finding 7 (text-hash brittleness)","issue":"V2 Finding 3 (Critical, 3 reviewers) flagged cross-iteration tracking mechanism incomplete. V2 Finding 7 (Critical) flagged that hash-based finding IDs break when finding text is refined (\"Parallel agent dispatch has no retry logic\" in iteration 1 becomes \"Parallel agent retry logic lacks exponential backoff\" in iteration 2—same concern evolving, but different hashes). Design doc sync didn't add finding persistence specification. Text hashing is brittle for iterative review because reviewers naturally rephrase findings as design improves. Exact text matching fails precisely when iteration tracking is most valuable.","why_it_matters":"Finding persistence enables two high-value features: (1) showing user which prior findings are resolved vs still open vs new (iteration diff), (2) focusing reviewer scrutiny on sections that changed (via git diff). Without stable IDs, \"12 new findings\" is ambiguous—are these truly new concerns or rephrased versions of prior findings? User wastes time cross-referencing manually. Hash-based IDs optimize for implementation simplicity at cost of UX quality.","suggestion":"Add \"Finding Persistence Mechanism\" section specifying LLM-based semantic matching (not hashing). Process: (1) On re-review, synthesizer receives prior summary with all findings from iteration N, (2) For each new finding from iteration N+1, synthesizer asks \"does this semantically match any prior finding?\" using LLM comparison, (3) If match found, assign same finding ID and mark status (`addressed` if issue resolved, `persists` if still present, `refined` if partially addressed), (4) If no match, assign new finding ID, (5) Cross-iteration diff in summary shows: \"Iteration 2 resolved Findings 1, 3, 5 from iteration 1. Findings 2, 4 persist with updated details. 8 new findings introduced.\" This is more token-expensive than hashing but provides correct semantics. Alternatively, use hybrid: hash for exact matches (fast path), LLM matching for non-exact (quality path). Validate approach during Second Brain test case (known ground truth of which findings should match across iterations)."}
{"type":"finding","id":"v3-prior-art-scout-008","title":"Prompt Caching Architecture Underspecified (Affects Cost Model)","severity":"Important","phase":{"primary":"design","contributing":null},"section":"V2 Finding 4 (prompt caching missing), Reviewer Prompt Architecture (should exist)","issue":"V2 Finding 4 (Critical) flagged prompt caching architecture not addressed. Requirements doc states \"This is an architectural convention, not a feature—90% input cost reduction on cache hits.\" Design doc sync didn't add prompt architecture section. Design doc has one paragraph in \"Reviewer Prompt Architecture\" stating prompts are structured for future caching with stable prefix (persona + methodology + format + voice) and variable suffix (artifacts + iteration context), but defers optimization \"until prompts stabilize post-prototype.\" This treats caching as performance optimization when requirements frame it as cost strategy.","why_it_matters":"Prompt caching is Anthropic-specific feature requiring specific prompt structure. If prompts aren't architected for caching from start, refactoring later invalidates eval data (changing prompts changes review quality). CLAUDE.md cost strategy explicitly relies on prompt caching for 90% cost reduction—this is not optional for budget viability. Additionally, prompt versioning affects cache invalidation: minor tweaks to stable prefix invalidate cache and spike costs. Without explicit cache boundary specification in design, implementers won't know what belongs in prefix (cacheable, changes rarely) vs suffix (per-review, never cached).","suggestion":"Expand \"Reviewer Prompt Architecture\" section to specify: (1) Stable prefix exact structure (in order: persona identity and mandate, review methodology steps, output format rules with examples, voice guidelines, blind spot check instructions), (2) Variable suffix exact structure (design artifact, requirements artifact, prior review summary if re-review, iteration number, git diff if available), (3) Cache boundary delimiter (explicit marker showing where prefix ends and suffix begins—required by Anthropic caching API), (4) Prompt versioning (semantic versioning for prefix: major version = methodology change, minor version = wording refinement, patch = typo fix; major/minor changes invalidate cache), (5) Version tracking in output (summary.md records prompt version per reviewer to correlate finding quality with prompt changes). Note: this specification enables eval framework to measure cache hit rate and cost-per-review accurately."}
{"type":"finding","id":"v3-prior-art-scout-009","title":"LangSmith Annotation UI Is Standard Solution for Finding Processing","severity":"Important","phase":{"primary":"design","contributing":"calibrate"},"section":"Step 5: Process Findings, V2 Finding 44 (LangSmith for annotation)","issue":"V2 Prior Art Scout Finding 44 (Important) flagged LangSmith annotation UI as solving finding processing UX. Design doc sync didn't add LangSmith evaluation. Design custom-builds CLI-based interactive finding processing (accept/reject one-at-a-time) with state stored in markdown. LangSmith (already in tooling budget, 5k traces/month free tier) provides production annotation queues with web UI for human feedback, finding tagging by severity/phase, accept/reject/comment workflows, team collaboration, historical disposition tracking. This is standard solution for human-in-the-loop LLM workflows.","why_it_matters":"V2 identified UX limitations in custom CLI processing: no async mode (Finding 42), no resumption spec (Finding 18), state loss on crash (Finding 21), single-user assumption (Finding 28), orphaned findings in Critical-first mode (Finding 25). All five findings are UX problems stemming from CLI-based stateful interaction. LangSmith solves these as core features: web UI enables async (user processes findings hours later), persistent state enables resumption, database storage prevents loss, team features enable multi-user, filtering enables partial processing without orphaning. Adopting LangSmith shifts finding processing from custom implementation to configuration.","suggestion":"Prototype LangSmith for finding processing. Test: (1) Each reviewer run becomes LangSmith trace with metadata (reviewer name, iteration, timestamp), (2) Each finding becomes annotation attached to trace with tags (severity, phase, section), (3) User processes findings in LangSmith web UI: accept/reject with notes, filter by severity/reviewer, bulk operations, (4) Skill reads back dispositions from LangSmith API to update summary.md. Validate: does workflow feel natural? Does web UI add friction vs CLI? Does team collaboration work (multiple people processing same review)? If LangSmith UX fits, adopt it and eliminate custom processing UI. If not, document specific gaps and build custom. Budget: 2-3 hours to prototype. Payoff: solves 5 Important findings from V2."}
{"type":"finding","id":"v3-prior-art-scout-010","title":"Braintrust Scorers Are Standard Solution for Synthesis Tasks","severity":"Important","phase":{"primary":"design","contributing":"calibrate"},"section":"Synthesis responsibilities, V2 Finding 45 (Braintrust for severity normalization)","issue":"V2 Prior Art Scout Finding 45 (Important) flagged Braintrust LLM-as-judge for severity normalization and deduplication. Design doc sync didn't add Braintrust evaluation. Design treats synthesizer as custom prompt engineering problem: deduplicate findings (semantic similarity task), reconcile severity ranges (consensus scoring task), classify by phase (multi-class classification task). Braintrust (already in tooling budget, 1M spans/10k scores free tier) is specifically designed for LLM-as-judge evaluation with autoevals library, chain-of-thought reasoning for assessments, and configurable scorers.","why_it_matters":"Synthesizer responsibilities from design: (1) judge if two findings are \"the same issue\" (requires semantic comparison + similarity threshold), (2) reconcile severity when reviewers disagree (requires consensus logic), (3) assign phase classification (requires multi-label classification with reasoning), (4) detect systemic patterns (requires clustering/aggregation). All four are LLM-as-judge tasks. Custom implementation means writing scorer prompts, managing model calls, handling edge cases, tracking confidence. Braintrust provides battle-tested scorers as reusable components.","suggestion":"Prototype Braintrust for synthesis. Test: (1) Each reviewer output becomes Braintrust trace, (2) Deduplication scorer compares pairs of findings using semantic similarity (cosine similarity on embeddings or LLM-based comparison), threshold at 0.8+ for \"same issue\", (3) Severity scorer reconciles ranges using majority vote or conservative max, (4) Phase classification scorer assigns primary + contributing phases using chain-of-thought, (5) Synthesizer aggregates scorer results into summary.md. Validate: does scorer API provide needed flexibility? Are built-in scorers sufficient or do custom scorers need development? Does reasoning transparency (chain-of-thought) help debug synthesis errors? Budget: 3-4 hours. Payoff: synthesis becomes configuration of scorers rather than custom prompt engineering."}
{"type":"finding","id":"v3-prior-art-scout-011","title":"TRiSM Frameworks Provide Adversarial Resilience Standards","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Reviewer personas (security/adversarial focus)","issue":"2026 Trust, Risk, and Security Management (TRiSM) frameworks (Wiley review, January 2026) provide five foundational pillars for AI lifecycle: explainability with drift monitoring, ModelOps governance, application security, data protection/privacy, and adversarial resilience. Design doc specifies 6-9 reviewer personas but doesn't map to industry-standard adversarial testing frameworks. Parallax:review reviews design artifacts, not LLM outputs, but the adversarial lens is transferable—both seek to find flaws before production.","why_it_matters":"TRiSM frameworks represent industry consensus on what constitutes comprehensive adversarial review in 2026. Parallax's persona selection (Assumption Hunter, Edge Case Prober, etc.) was derived ad-hoc. If TRiSM pillars map cleanly to design review concerns, adopting standard taxonomy improves persona coverage and enables cross-industry comparison. Example mapping: explainability → Assumption Hunter (unstated assumptions), ModelOps governance → Feasibility Skeptic (implementation complexity), application security → (missing persona for security review at design stage), data protection → (missing persona for privacy/compliance), adversarial resilience → Edge Case Prober (failure modes).","suggestion":"Add TRiSM mapping to \"Reviewer Personas\" rationale section. For each pillar, identify which personas cover it and whether gaps exist. If security and privacy personas are missing from design-stage review, add them or document explicit choice to defer to plan-stage review. This grounds persona selection in industry standards rather than intuition."}
{"type":"finding","id":"v3-prior-art-scout-012","title":"Cloud Security Alliance Agentic AI Controls Apply to Parallax Architecture","severity":"Important","phase":{"primary":"design","contributing":"plan"},"section":"Reviewer capabilities, security boundaries","issue":"Cloud Security Alliance Singapore published (2026) draft addendum to Securing AI Systems guide targeting agentic AI with practical controls: risk assessment hardening, asset and secret hygiene, authentication/authorization, limiting agency, segmentation, secure inter-agent communication, monitoring/logging, human-in-loop, and vulnerability disclosure. Parallax:review is an agentic AI system (6+ agents with tool access orchestrated to review designs). Design doc has one sentence on reviewer capabilities (\"read-only, no write access\") but doesn't address CSA controls.","why_it_matters":"Security controls for agentic AI are emerging industry standards. Parallax agents have tool access (file reading, potentially `gh`, `curl`, `git`), inter-agent communication (findings passed to synthesizer), and human-in-loop gates. Without explicit security design: (1) reviewers could access files outside repo (no segmentation), (2) malicious input in design doc could prompt-inject reviewers (no input validation), (3) agents could exfiltrate data via tool calls (no monitoring), (4) credentials could leak in findings (no secret hygiene). These are standard agentic AI risks.","suggestion":"Add \"Security Architecture\" section to design addressing CSA controls relevant to parallax:review: (1) Limiting agency: reviewers have read-only file access scoped to repo, no network access except Prior Art Scout (least privilege), (2) Segmentation: reviewers cannot access parent directories, credentials files, or system paths outside workspace, (3) Secret hygiene: findings must not include content from .env, secrets.json, or credential files (filtering layer in synthesis), (4) Monitoring: all tool calls logged with parameters for audit trail, (5) Human-in-loop: findings are reviewed before any action taken (already in design), (6) Input validation: design/requirements artifacts scanned for prompt injection patterns before feeding to reviewers. Many of these controls may already exist in Claude Code's MCP server boundaries—document explicit reliance on platform security vs parallax-specific controls."}
{"type":"finding","id":"v3-prior-art-scout-013","title":"Multi-Agent Observability Is Industry Standard (62% Have Detailed Tracing)","severity":"Important","phase":{"primary":"design","contributing":"plan"},"section":"Cost tracking, parallel agent failure handling","issue":"Web search reveals 89% of organizations have implemented observability for agents, 62% have detailed tracing inspecting individual agent steps and tool calls (2026 industry data). Parallax:review dispatches 6 agents in parallel, consolidates findings, and processes them iteratively. Design doc specifies timeout/retry/partial results handling but has no observability specification beyond \"cost logging per review run in JSONL output.\"","why_it_matters":"Without detailed tracing, debugging parallax:review failures is opaque. If Assumption Hunter produces low-quality findings, is it prompt issue, model issue, or input artifact issue? If synthesizer misclassifies findings by phase, which step in synthesis logic failed? If review takes 8 minutes instead of expected 3 minutes, which reviewer was slow? Observability enables: (1) debugging (trace why finding was flagged), (2) optimization (identify bottleneck agents), (3) cost attribution (which reviewer consumed most tokens), (4) quality analysis (correlate finding quality with reviewer reasoning traces). Inspect AI and Braintrust both provide trace collection as built-in features. If parallax custom-builds orchestration without observability, it lacks production-grade debugging that frameworks provide for free. LangSmith provides trace visualization with nested spans showing agent interactions, tool calls, and decision points.","suggestion":"Add \"Observability and Tracing\" section to design specifying: (1) Each reviewer execution emits structured trace (start time, input tokens, output tokens, elapsed time, tool calls made, findings generated, errors encountered), (2) Synthesizer execution emits trace (deduplication decisions with reasoning, severity reconciliation with input/output, phase classification with justification), (3) Review run produces trace collection in JSONL (enables post-hoc analysis), (4) If using Inspect AI or Braintrust, rely on their tracing; if custom, implement minimal structured logging. (5) Eval framework uses traces to measure: reviewer coverage overlap, synthesis accuracy, cost per finding, time per reviewer. This is standard practice per 2026 industry data—not optional."}
{"type":"finding","id":"v3-prior-art-scout-014","title":"Mitsubishi Adversarial Debate Validates Stance-Based Review (Not Just Coverage)","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"V2 Finding 11 (adversarial pairs), Reviewer personas","issue":"Mitsubishi Electric announced (January 2026) multi-agent adversarial debate AI using argumentation framework where expert AI agents compete to derive better conclusions through debate. This validates parallax's adversarial review hypothesis but highlights gap: Mitsubishi's agents have incompatible worldviews forced to reconcile (true adversarial debate), whereas parallax's personas are coverage-based (inspecting different domains, not opposing each other). V2 Finding 11 (Critical) flagged this: \"adversarial review is misnamed—this is comprehensive coverage, not opposition.\" Design doc sync didn't address this.","why_it_matters":"If adversarial tension (forcing incompatible positions to reconcile) produces better decisions than comprehensive coverage (more inspectors examining different aspects), parallax's persona design is suboptimal. Mitsubishi's approach: agents argue FOR incompatible conclusions, synthesis must reconcile. Parallax's approach: agents examine different aspects, all can be right simultaneously. The former surfaces deep tensions, the latter improves breadth. Both are valuable but different mechanisms.","suggestion":"Run empirical test during Second Brain validation: (1) Coverage-based review (current design): 6 personas examine different aspects, (2) Stance-based review (Mitsubishi pattern): 3 pairs of opposing personas argue incompatible positions (Optimizer vs Hardener, User-Centric vs Operator-Centric, Ship-Fast vs Ship-Safe), synthesizer reconciles debates. Compare: which approach catches more design flaws? Which produces more actionable findings? Which is more expensive (token cost)? If stance-based review produces significantly better results, redesign personas. If comparable, document that coverage-based is simpler and equally effective."}
{"type":"finding","id":"v3-prior-art-scout-015","title":"Compound Engineering Learning Loop Is Exact Prior Art (Should Study Before Building)","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"V2 Finding 47 (Compound Engineering), calibration feedback loop","issue":"V2 Prior Art Scout Finding 47 (Important) flagged Compound Engineering (EveryInc, 8.9k stars) as exact prior art for learning loops where review findings feed back to improve prompts. Design doc sync didn't address this. Requirements doc describes \"correction compounding\" where false negatives/positives become permanent calibration rules in reviewer prompts. Compound Engineering implements exactly this: Plan → Work → Review → Compound → Repeat, where Review captures learnings and Compound feeds them back.","why_it_matters":"Compound Engineering demonstrates learning loops are viable and valuable in production. They've solved implementation problems parallax will encounter: how to structure calibration data, how to version prompts when learnings accumulate, how to measure whether learnings improve review quality, how to prevent prompt bloat as rules accumulate. Not studying their implementation before building parallax's calibration means reinventing solutions to known problems.","suggestion":"Study Compound Engineering's learning loop implementation before prototyping parallax calibration. Key research questions: (1) How do they capture learnings from review cycles (structured format, manual curation, LLM extraction)? (2) Where do learnings go in prompt (stable prefix, separate knowledge base, examples)? (3) How do they version prompts as learnings accumulate (prevent prompt bloat)? (4) How do they measure whether learnings improved quality (regression testing, A/B comparison)? (5) What's their prompt update workflow (manual review, automated injection, git-based versioning)? Document answers in design. If Compound Engineering already solved this well, consider: propose collaboration where parallax contributes design-stage reviewers to their ecosystem rather than building competing infrastructure."}
{"type":"finding","id":"v3-prior-art-scout-016","title":"adversarial-spec Multi-LLM Debate Pattern Requires Iteration Design","severity":"Important","phase":{"primary":"design","contributing":null},"section":"V2 Finding 48 (adversarial-spec study), Iteration loops","issue":"V2 Prior Art Scout Finding 48 (Important) flagged adversarial-spec (zscole, 487 stars) as demonstrating multi-LLM debate pattern where models critique each other iteratively until consensus. Design doc sync didn't address this. Current parallax design: reviewers examine artifact once in parallel, findings are consolidated, user processes findings. No iteration between reviewers, no debate, no critique-of-critique. This is single-pass review, not debate.","why_it_matters":"adversarial-spec's key insight: debate surfaces gaps better than independent inspection. Their process: Claude drafts spec → opponents critique → synthesize → revise → repeat. Multiple debate rounds force deeper thinking. Parallax's single-pass review means reviewers don't see each other's findings—Assumption Hunter might flag something Edge Case Prober would realize is actually fine if they debated. True adversarial review requires reviewers to respond to each other's findings, not just to the design.","suggestion":"Add \"Multi-Round Review (Post-MVP)\" section to design documenting debate pattern as future enhancement. Minimal design: (1) Round 1: reviewers examine design independently (current design), (2) Round 2: reviewers receive each other's findings and write responses (\"Assumption Hunter flagged X, but I think Y because...\"), (3) Synthesizer reconciles debates and flags unresolved tensions for human decision. Note implementation cost: 2x token usage (two rounds of review), 2x latency (sequential rounds). Defer to post-MVP pending eval data on whether single-pass review quality is sufficient. If Second Brain test case shows missed findings that multi-round would catch, prioritize this enhancement."}
{"type":"finding","id":"v3-requirement-auditor-001","title":"Documentation Debt Resolved — V1 Dispositions Now in Design","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Entire design doc (multiple sections updated)","issue":"V2 review flagged 10 accepted findings from iteration 1 not reflected in design doc (Critical severity, systemic issue). Between v2 and v3, 23 dispositions were synced. Spot-checking key sections: (1) Prompt caching architecture now specified in \"Reviewer Prompt Architecture,\" (2) Auto-fix step added to UX Flow Step 4, (3) Cross-iteration tracking section expanded with finding IDs + status + prior context, (4) Phase classification updated to primary+contributing, (5) Severity range handling in verdict logic, (6) Output voice guidelines added to Per-Reviewer Output Format, (7) Reviewer capabilities section added, (8) Synthesizer role updated (judgment acknowledged). All major v2 documentation debt resolved.","why_it_matters":"Design doc is now synchronized with accepted decisions from iteration 1. This resolves the systemic issue flagged in v2.","suggestion":"No action. Documentation debt cleared. Remaining gaps are unimplemented features, not missing documentation."}
{"type":"finding","id":"v3-requirement-auditor-002","title":"JSONL Format Still Not Designed","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Output Artifacts, Per-Reviewer Output Format, Summary Format","issue":"V2 review flagged JSONL format as Critical (Finding 9, 4 reviewers consensus). Disposition notes reference JSONL as canonical format. MEMORY.md states \"decided but not yet implemented.\" Design doc sync added cross-iteration tracking and finding IDs but still specifies only markdown output. No JSONL schema, no field definitions, no migration path. Lines 319-320 mention \"JSONL output enables jq-based filtering\" but no structure specified.","why_it_matters":"This is the largest structural decision not documented. Finding IDs (line 248), status tracking (line 249), filtering by severity/persona/phase (line 320), cost logging (line 323)—all depend on structured output. Markdown format doesn't support stable IDs or machine-processable fields. Building markdown-first then migrating later means rewriting file I/O, parsing, and potentially invalidating iteration 1-2 data.","suggestion":"Add JSONL schema section specifying structure even if implementation deferred. Minimal schema: `{finding_id, reviewer, severity, phase_primary, phase_contributing, section, issue, why_it_matters, suggestion, iteration_status, disposition, disposition_note}`. Specify: (1) reviewers output markdown (unchanged for MVP), (2) synthesizer converts to JSONL as canonical storage, (3) summary.md is markdown rendering of JSONL for human reading, (4) finding IDs generated during JSONL conversion, (5) tools consume JSONL via jq. Or: acknowledge markdown is permanent and JSONL is deferred indefinitely."}
{"type":"finding","id":"v3-requirement-auditor-003","title":"Requirement for \"Structured Requirement Refinement\" Not Addressed","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"Prototype Scope (deferred), Problem Statement","issue":"Requirements doc pain point #5: \"No structured requirement refinement—we jumped from 'interesting idea' to 'write the design' without formally prioritizing must-have vs nice-to-have. A major feature was dropped late, after significant design work.\" Requirements doc explicitly states \"Outcome-focused—define what success looks like before defining how to get there. This has been the single biggest design quality lever in practice.\" First Principles Finding 12 from v2 notes \"The real problem is 'skipping requirements refinement,' not 'lack of review automation.'\" Design prototypes design-stage review. Requirement refinement (parallax:calibrate) deferred to \"Build later.\"","why_it_matters":"You're building a solution to catch design flaws when the requirements doc identifies preventing those flaws (via requirements review) as the highest-leverage intervention. Building design review first means you'll discover during testing that it catches symptoms, not root causes. V2 review already surfaced this (Finding 10: circularity, Finding 12: wrong problem framed).","suggestion":"Either (1) prototype requirements-stage review first (4 personas: Assumption Hunter, Requirement Auditor, First Principles, Prior Art Scout, Product Strategist), validate it catches requirement-level errors that prevent design failures, then build design review as extension, OR (2) explicitly reframe problem statement as \"validate orchestration mechanics\" prototype (not value hypothesis test) and accept that requirements review hypothesis is untested. Current framing claims to solve a problem the design doesn't address."}
{"type":"finding","id":"v3-requirement-auditor-004","title":"Cross-Iteration Finding IDs Still Unspecified","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Cross-Iteration Finding Tracking (lines 244-252)","issue":"Section added in v2 sync. Line 248: \"Finding IDs: Stable hash derived from section + issue content. Enables cross-iteration diff.\" But hash generation algorithm unspecified. V2 Assumption Hunter Finding 7 noted text-hash brittleness: minor wording changes break tracking. Same design concern evolving across iterations produces different hashes, treated as unrelated findings. Section acknowledges the mechanism but doesn't specify it.","why_it_matters":"Finding persistence is the entire value of cross-iteration tracking. Without stable ID specification, implementation will produce brittle hashes (wording changes → false new findings) or inconsistent IDs (manual assignment → errors). When Finding 3 from iteration 1 reappears in iteration 2, system must detect it.","suggestion":"Specify ID generation in design. Options from v2 Finding 7 + Finding 15: (1) Semantic hash (section + first 100 chars of issue, normalized), (2) Section-based anchoring (section + reviewer + iteration, dedupe by overlap), (3) LLM-based semantic matching (synthesizer asks \"which prior findings does this match?\"), (4) Hybrid (auto-hash with manual override). Recommend LLM semantic matching for MVP—synthesizer already exercises judgment, matching is natural extension."}
{"type":"finding","id":"v3-requirement-auditor-005","title":"Multi-Causal Phase Classification Routing Not Operationalized","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Finding Phase Classification (lines 153-161), Verdict Logic (lines 163-169)","issue":"Lines 161 note \"Findings classified by primary phase (most actionable fix) and optional contributing phase (upstream cause).\" Lines 161: \"When >30% share contributing phase, synthesizer flags systemic issue.\" But verdict logic (lines 163-169) only uses primary phase for routing. V2 Assumption Hunter Finding 6 notes: if finding is \"design flaw (primary) caused by calibrate gap (contributing),\" system routes to design revision when it should escalate to calibrate. Contributing phase documented but not operationalized.","why_it_matters":"Multi-causal classification was accepted in v1 Finding 7 disposition specifically to catch systemic upstream failures. Current verdict logic defeats this—user revises design when requirements are broken. Wastes iteration cycles.","suggestion":"Update verdict logic to treat contributing phases as escalation signals. Add rule: \"If any finding has calibrate gap (contributing) or survey gap (contributing), verdict includes systemic escalation warning. User must acknowledge upstream issue before proceeding.\" Alternatively: if >30% of findings share contributing phase, force escalate regardless of primary classifications."}
{"type":"finding","id":"v3-requirement-auditor-006","title":"Model Tiering Strategy Missing","severity":"Important","phase":{"primary":"design","contributing":null},"section":"None (should be in Reviewer Personas or cost section)","issue":"Requirements doc: \"Model tiering: Haiku for simple evals, Sonnet for review agents, Opus sparingly for adversarial deep analysis.\" CLAUDE.md cost strategy relies on model tiering. Design specifies Sonnet for all reviewers (line 62 table note: \"all use model: sonnet\"). No architecture for per-persona model configuration. V2 Requirement Auditor Finding 38 flagged this.","why_it_matters":"Cost optimization. If Haiku works for mechanical reviewers (Edge Case Prober, Prior Art Scout for search), per-review cost drops 30-40%. Without design-level specification, model becomes hardcoded implementation detail rather than configurable parameter. Eval framework can't test model variants.","suggestion":"Add model configuration subsection. Specify: (1) default model per stage (Sonnet for design), (2) model parameter per persona (allows empirical testing), (3) configuration via persona definition (not hardcoded), (4) eval framework tests Haiku for mechanical personas vs Sonnet baseline."}
{"type":"finding","id":"v3-requirement-auditor-007","title":"Auto-Fix Git Safety Concerns Unresolved","severity":"Important","phase":{"primary":"design","contributing":null},"section":"UX Flow Step 4 (Auto-Fix, lines 276-278)","issue":"Lines 276-278: \"Auto-fixable findings (typos in markdown, missing file extensions, broken internal links) applied automatically. Auto-fixes committed as separate git commit from human-reviewed changes.\" V2 Edge Case Prober Finding 20 flagged this conflicts with Git Safety Protocol (\"NEVER commit changes unless user explicitly asks\"). Auto-fix modifies design doc before user sees findings. If user has unrelated edits, auto-fix commit includes them. If user wants to reject auto-fixes, they can't—already applied.","why_it_matters":"Auto-fix as described is invasive. Modifies source files without user approval. Step 4 (auto-fix) happens before Step 5 (user processes findings), so user can't reject bad auto-fixes. Separate commit impossible if auto-fixes intermixed with user changes between review start and completion.","suggestion":"Revise auto-fix workflow. Options: (1) auto-fixes presented as suggested changes (diff format), user approves before application, (2) auto-fixes applied to copy of design doc (user merges if approved), (3) auto-fixes deferred to post-human-processing (user accepts/rejects findings, then auto-fixable accepted findings applied as batch), (4) conservative criteria (whitespace/formatting only) with explicit approval. Recommend option 3: auto-fix only accepted findings, not all findings."}
{"type":"finding","id":"v3-requirement-auditor-008","title":"No Stopping Criteria for Re-Review Iteration Loop","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Verdict Logic, UX Flow (revise path)","issue":"Line 170: \"`revise` = fix the design and re-review.\" No exit condition. If iteration 2 produces new Critical findings (different from iteration 1), user revises. Iteration 3 produces more. When does it stop? When findings = 0 (unrealistic)? When no new Critical findings? When user decides \"good enough\"? V2 Edge Case Prober Finding 24 flagged this.","why_it_matters":"Without convergence criteria, reviews iterate indefinitely. Real designs have tradeoffs—edge cases deferred, Important findings accepted as constraints. Current design treats all Critical findings as blockers with no framework for \"acceptable risk, proceed anyway.\"","suggestion":"Add review convergence criteria. Options: (1) explicit threshold (\"2 consecutive iterations with no new Critical findings\"), (2) allow user override (\"proceed despite Critical findings\" with justification required), (3) track iteration count, flag if >3, (4) add \"defer\" disposition for out-of-scope findings accepted by user. Recommend option 2: user can override verdict with required note."}
{"type":"finding","id":"v3-requirement-auditor-009","title":"Requirements-Stage Review Deferred But Problem Statement Says It's Highest Leverage","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"Prototype Scope (lines 314-330), Requirements doc (lines 99-103)","issue":"Requirements doc lines 99-103: \"Outcome-focused—define what success looks like before defining how to get there. This has been the single biggest design quality lever in practice: missing a critical angle during requirements means the design optimizes for the wrong thing.\" Design builds design-stage review as prototype. Requirements-stage review (with Product Strategist persona) deferred to \"Build later\" (lines 325-327). V2 First Principles Finding 10 notes this is circular: building design review to validate orchestration when problem statement identifies requirement refinement as root cause.","why_it_matters":"If requirements review is actually highest leverage (per requirements doc), spending weeks tuning design-stage personas validates mechanics but misses value hypothesis. You'll build battle-tested infrastructure for the wrong problem. Test case validation (Second Brain) may show design review catches symptoms while requirement errors remain uncaught.","suggestion":"Reframe prototype scope. Either (1) build requirements-stage review first (4 personas including Product Strategist), test against historical requirement docs that led to design failures, validate hypothesis, then extend to design stage, OR (2) make explicit: \"This prototype validates orchestration mechanics (parallel agents, synthesis, finding processing, iteration tracking), not value hypothesis. Requirements review is acknowledged as higher leverage but deferred pending orchestration validation.\" Current framing claims to solve a problem it doesn't address."}
{"type":"finding","id":"v3-requirement-auditor-010","title":"Verdict Computed Before Findings Processed","severity":"Important","phase":{"primary":"design","contributing":null},"section":"UX Flow Step 4 (Present Summary), Verdict Logic","issue":"UX flow: Step 3 synthesize → Step 4 present verdict → Step 5 process findings. Verdict based on all findings, but Step 5 may reject findings as false positives. If user rejects 3 of 5 Critical findings in Step 5, the \"revise\" verdict from Step 4 should become \"proceed.\" Design doesn't specify whether verdict recomputed. V2 Feasibility Skeptic Finding 43 flagged this.","why_it_matters":"Presenting provisional verdict wastes cycles. User sees \"revise\" (5 Critical), processes findings for 20 minutes, rejects 4 Critical as invalid. Should have been \"proceed\" verdict. User took action (started revising design) based on wrong signal.","suggestion":"Recompute verdict after finding processing based on accepted findings only. Show provisional verdict in Step 4 (\"Based on all findings: revise. Processing findings may change this.\"), final verdict in Step 6 after processing completes. Alternatively: eliminate provisional verdict, show finding counts only, let user process first then compute verdict."}
{"type":"finding","id":"v3-requirement-auditor-011","title":"\"Critical-First\" Mode Orphans Non-Critical Findings","severity":"Important","phase":{"primary":"design","contributing":null},"section":"UX Flow Step 5 (Processing modes, lines 280-286)","issue":"Lines 282-284: \"Critical-first: Address Critical findings only. Send source material back for rethink. Remaining findings processed in next pass.\" User processes Critical, revises design, re-reviews. Iteration 2 produces new findings. User processes Critical-first again. Important/Minor from iteration 1 never processed—orphaned. After 3 iterations, 40+ unprocessed findings accumulated. V2 Edge Case Prober Finding 25.","why_it_matters":"Critical-first designed for fast iteration but creates technical debt. Eventually user must process orphaned findings or accept valuable feedback discarded. No mechanism tracks which findings carried forward unprocessed vs obsoleted by design changes.","suggestion":"Add orphaned finding management. Specify: (1) after Critical-first processing, synthesizer marks remaining findings \"deferred to next iteration,\" (2) on re-review, synthesizer reconciles prior deferred findings with new findings (mark as resolved/persists/new), (3) after revise loop converges, prompt user to process accumulated deferred findings before final proceed. Track deferred finding count in summary."}
{"type":"finding","id":"v3-requirement-auditor-012","title":"Test Cases Not Run — Second Brain Validation Missing","severity":"Important","phase":{"primary":"design","contributing":"plan"},"section":"Prototype Scope (Validate with, lines 332-334)","issue":"Lines 332-334: \"Validate with: Second Brain Design test case (3 reviews, 40+ findings in the original session).\" MEMORY.md shows only smoke test run (self-review of parallax:review design). Second Brain test case—real project with known design flaws—not executed. V2 Feasibility Skeptic Finding 54.","why_it_matters":"Self-review validates orchestration works but weak for quality validation. Second Brain case is the test that matters: did implemented system catch the same 40+ issues manual process found? Without this, design approval is premature—you don't know if reviewers catch real design flaws.","suggestion":"Run Second Brain test case before marking design approved. Use findings to validate: (1) personas catch real design flaws, (2) finding counts comparable to manual review, (3) severity calibration sensible, (4) phase classification routes correctly. If test reveals failures, update design. Alternatively: acknowledge this is post-approval validation and Second Brain test will inform iteration 4 refinements."}
{"type":"finding","id":"v3-requirement-auditor-013","title":"CLI Tool Access Per Persona Still Unspecified","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Reviewer Capabilities (lines 253-258)","issue":"Section added in v2 sync (lines 253-258). Line 257: \"Tool access boundaries specified in stable prompt prefix per persona. Specific tool assignments empirical question for eval framework—start with baseline access and expand based on observed reviewer needs.\" Requirements doc lists specific CLI tools (gh, jq, git, curl). Section acknowledges tool access but defers specifics to eval. Which reviewers get which tools?","why_it_matters":"Some personas need tools to perform their function. Prior Art Scout searches existing solutions (needs gh, curl, web search). Assumption Hunter validates assumptions (needs grep for codebase checks). Without per-persona specification, prompt authors must guess or over-provision (security risk).","suggestion":"Add per-persona tool access table to design. Minimal specification: (1) all reviewers: Read (design/requirements), (2) Prior Art Scout: gh, curl, WebSearch, (3) Assumption Hunter: grep, jq for config validation, (4) all reviewers: read-only, no write access. Note: specific assignments tuned via eval, this is baseline."}
{"type":"finding","id":"v3-requirement-auditor-014","title":"Multi-User Disposition Workflows Not Addressed","severity":"Minor","phase":{"primary":"design","contributing":"calibrate"},"section":"UX Flow (assumes single human), Finding 2 disposition (async-first)","issue":"Async-first architecture (lines 261, disposition from v1 Finding 2) correctly decouples review execution from human availability. But finding disposition workflow assumes single human. CLAUDE.md states \"useful beyond personal infra, applicable to work contexts.\" Work contexts have teams. If three engineers process same summary.md, last writer wins. No support for finding assignment or collaborative review. V2 Assumption Hunter Finding 4.","why_it_matters":"Teams need multi-user disposition tracking. Current design: Alice accepts findings 1-5, Bob reviews 6-10 simultaneously, both write summary.md, one overwrites the other. Or: team wants to discuss Finding 12 before disposition—no workflow for \"flagged for team discussion.\"","suggestion":"Document single-user limitation in design. Add to summary.md format: dispositions include reviewer name + timestamp (enables detecting conflicts). Or: defer to LangSmith which has built-in team annotation UI (v2 Prior Art Scout Finding 44). For MVP, acknowledge limitation: \"Multi-user workflows deferred. Summary.md dispositions are last-writer-wins. For team review, use external tooling (LangSmith, GitHub PR comments on summary.md).\""}
{"type":"finding","id":"v3-requirement-auditor-015","title":"Cost Per Review Run Still Not Tracked","severity":"Minor","phase":{"primary":"plan","contributing":null},"section":"Open Questions (line 346), Prototype Scope (line 323)","issue":"Line 346: \"Cost per review run and whether model tiering is worth quality tradeoff\" flagged as eval question. Line 323 notes \"cost logging per review run in JSONL output.\" But after smoke test + v2 validation run (55 findings), actual cost data should exist. How much did v2 review cost? Token counts? Was cost within budget?","why_it_matters":"Budget $2000/month, $150-400 projected API spend. Unknown costs make iteration risky. If review costs $5 (10x estimate), you can afford 30-60 reviews/month. If $0.20 (5x under), hundreds. Without data, flying blind on burn rate. Two review runs completed, no cost logging.","suggestion":"Instrument next review run (iteration 3, this review) with token/cost logging. Capture: (1) input tokens per reviewer, (2) output tokens per reviewer, (3) synthesizer tokens, (4) total cost at Sonnet pricing, (5) cache hit rate if enabled. Log to summary metadata or separate cost file. Use data to validate budget assumptions and inform model tiering decisions."}
{"type":"finding","id":"v3-requirement-auditor-016","title":"Rejection Note Processing Unspecified","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"UX Flow Step 6 (Process Findings, lines 288-290), Finding 6 disposition","issue":"Lines 289-290: \"Reject with note—finding is wrong or not applicable. Rejection note becomes calibration input to next review cycle.\" V1 Finding 6 disposition cut \"discuss\" mode, replaced with reject-with-note. Where are rejection notes stored? Who reads them? What format? V2 Edge Case Prober Finding 17 flagged this.","why_it_matters":"Reject-with-note is now primary mechanism for disputed findings and capturing design decisions. Without schema or processing specified, notes are write-only (captured but never used). If note says \"This assumes REST architecture, we're event-driven,\" how does that context reach reviewers who need it?","suggestion":"Add rejection note schema to summary.md format. Specify: (1) rejected findings section with disposition notes, (2) prior review summary includes \"Previously rejected findings and why\" (fed to reviewers on re-review), (3) synthesizer checks if rejected findings reappear and surfaces to user, (4) rejection notes feed reviewer calibration (when same false positive recurs, update prompt)."}
{"type":"finding","id":"v3-requirement-auditor-017","title":"Prompt Versioning Strategy Missing","severity":"Minor","phase":{"primary":"plan","contributing":"design"},"section":"Reviewer Prompt Architecture (lines 225-242)","issue":"Lines 225-242 specify prompt structure (stable prefix + variable suffix) for caching. Line 241: \"Prompt changes to stable prefix invalidate cache and should be tracked as versioned changes.\" But no version tracking mechanism specified. Task 8 updated prompts (smoke test → iteration), but how are prompt versions tracked? Can iteration 1 findings be compared to iteration 2 if prompts changed? V2 Feasibility Skeptic Finding 53.","why_it_matters":"Eval framework depends on comparing review quality across iterations. If Assumption Hunter prompt changes between iteration 1 and 2, finding differences could be due to prompt changes (not design improvements). Can't distinguish \"design improved\" from \"prompts got better\" or \"prompts regressed.\"","suggestion":"Version reviewer prompts explicitly. Each prompt file includes version header (or git commit SHA). Summary.md records prompt versions used. When comparing iterations, note prompt version changes. If major prompt refactor needed, increment version, treat findings as non-comparable to prior versions."}
{"type":"finding","id":"v3-requirement-auditor-018","title":"Timestamped Folders Break Git Diff Workflow","severity":"Minor","phase":{"primary":"design","contributing":"calibrate"},"section":"Skill Interface (topic label, lines 22-24), Requirements (git diffing)","issue":"Lines 23-24: \"Collision handling: timestamped folders for iteration separation.\" But requirements doc: \"Git commits per iteration. Full history, diffable artifacts\" and \"diffs show what changed.\" If each iteration creates timestamped folder (`docs/reviews/topic-2026-02-15-143022/`), git diff between iterations compares different file paths—useless. Lose iteration diffing, core design goal. V2 Edge Case Prober Finding 19.","why_it_matters":"Timestamped folders solve collision but break diffability. Contradiction between disposition and requirement.","suggestion":"Revise collision handling. Options: (1) single folder per topic, overwrite on re-review, rely on git history for diffs (collision = overwrite), (2) timestamped folders + symlink `docs/reviews/topic-latest/` pointing to most recent, synthesizer produces diff report, (3) nested structure `docs/reviews/topic/iteration-1/`. Recommend option 1: overwrite + git history."}
{"type":"finding","id":"v3-requirement-auditor-019","title":"Async Mode Is Not Actually Background Execution","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"UX Flow, Finding 2 disposition","issue":"V1 Finding 2 disposition: \"Async is default—review writes artifacts to disk.\" This describes batch execution (run, write files, exit), not async workflow (background execution, resume later). True async: (1) review runs without blocking terminal, (2) user closes session and resumes later, (3) state persists across sessions. Current design has none. V2 Feasibility Skeptic Finding 42.","why_it_matters":"Background automation (CLAUDE.md track #6) requires true async. If review takes 5 minutes, that's 5 minutes terminal occupancy. \"Async\" framing in disposition is misleading—it's file-based output (good) but not background execution (required for automation).","suggestion":"Distinguish execution mode (sync/async) from output mode (interactive/file-based). Current design: sync execution, file-based output with optional interactive processing. For true async: (1) skill supports `--background` flag, (2) finding processing session-independent, (3) re-running skill on same topic detects in-progress review and shows status. Defer true async to post-MVP but document distinction."}
