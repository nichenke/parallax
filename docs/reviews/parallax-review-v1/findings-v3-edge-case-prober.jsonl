{"type":"finding","id":"v3-edge-case-prober-001","title":"Cross-Iteration Tracking Breaks When Design Structure Changes","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Cross-Iteration Finding Tracking (lines 243-252)","issue":"Design specifies finding IDs via \"stable hash derived from section + issue content\" (line 248) but provides no specification when design doc structure changes. If iteration 2 adds new sections or renames existing sections, section-based hashing produces different IDs for conceptually identical findings. Example: Finding flagged in iteration 1 under \"Synthesis\" section gets re-flagged in iteration 2 under new \"Synthesizer Agent\" section—hash collision impossible, treated as new finding. Timestamped folders (line 23) make git-based section tracking unreliable across iterations.","why_it_matters":"Design restructuring (adding sections, splitting large sections, renaming for clarity) is normal during revision. Hash-based IDs tied to section names make tracking unreliable across structure changes. Defeats cross-iteration tracking purpose—users can't distinguish \"new finding\" from \"same finding under renamed section.\" V2 review flagged 55 findings, v3 design doc has 8 new sections—high probability of hash mismatches.","suggestion":"Add section rename detection to design. Options: (1) Hash issue content only (ignore section), rely on LLM-based semantic matching for deduplication per line 262 suggestion, (2) Maintain section mapping file across iterations (old section → new section), (3) Finding IDs include both semantic hash and section anchor, synthesizer reconciles when sections change, (4) Conservative: when structure changes >20%, flag all findings as \"unable to determine if new or repeated\" and prompt user. For MVP, recommend option 1 (content-only hash) + LLM semantic matching."}
{"type":"finding","id":"v3-edge-case-prober-002","title":"Auto-Fix Re-Review May Loop Indefinitely","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Auto-Fix (lines 276-277, \"design is re-reviewed with remaining findings only\")","issue":"Auto-fix applies changes, then \"design is re-reviewed with remaining findings only\" (line 277). If auto-fix introduces new errors (broken markdown formatting, incorrect path corrections), re-review flags new findings, triggers another auto-fix pass, which creates more errors, loops. No loop termination condition specified. Example: Auto-fix corrects typo but breaks link, re-review flags broken link, auto-fix attempts correction but introduces different typo, re-review flags typo, loops. Design specifies auto-fixes committed separately (line 298) but no loop guard.","why_it_matters":"Unbounded auto-fix loops consume API budget and block user workflow. If each auto-fix pass costs $0.50 and loop runs 10 times before user intervenes, $5 burned. Worse: auto-fix changes accumulate as separate git commits (line 298), producing commit spam in git history. Conservative criteria (line 277: \"typos in markdown, missing file extensions, broken internal links\") still allow errors—typo correction can break markdown syntax, path correction can point to wrong file.","suggestion":"Add auto-fix loop guard to design. Specify: (1) Maximum 1 auto-fix pass per review run (no recursive auto-fix), user must approve additional auto-fix iterations, (2) Auto-fix validation: schema check or diff review before applying (reject auto-fixes that break markdown parsing), (3) Auto-fix findings presented to user for approval before application (contradicts \"automatic\" but prevents loops), (4) Track auto-fix iteration count in summary, flag if attempted >1 time."}
{"type":"finding","id":"v3-edge-case-prober-003","title":"Partial Reviewer Completion Breaks Systemic Issue Detection","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Parallel Agent Failure Handling (line 220: \"minimum threshold met (4/6 agents succeed)\"), Finding Phase Classification (line 161: \"when >30% of findings share a contributing phase, synthesizer flags systemic issue\")","issue":"Design allows proceeding with 4/6 reviewers (line 220). Synthesizer detects systemic issues \"when >30% of findings share a contributing phase\" (line 161). If 2 missing reviewers would have flagged the systemic pattern, threshold calculation is wrong. Example: 4 reviewers produce 20 findings, 6 have \"calibrate gap (contributing)\" = 30% (triggers systemic flag). But missing 2 reviewers would have added 10 more findings with 8 calibrate gaps = 14/30 = 47%. Systemic issue severity underestimated. Verdict computed on partial data.","why_it_matters":"Partial results corrupt systemic issue detection. User sees \"proceed with noted improvements\" when full reviewer set would have flagged \"escalate—systemic calibrate gap.\" Missing reviewers aren't random—timeouts correlate with design complexity (harder designs take longer to review, more likely to hit timeout). Worst-case: hardest designs get weakest reviews. Design specifies conservative verdict logic (line 168: \"highest severity in range\") but doesn't account for missing reviewer impact on threshold calculations.","suggestion":"Add systemic detection adjustment for partial results to design. Specify: (1) Disable systemic issue detection when <100% reviewers complete (mark as \"unable to determine systemic patterns with partial results\"), (2) Extrapolate: assume missing reviewers would have flagged patterns at same rate as completed reviewers (statistical estimate with confidence interval reported), (3) Require full reviewer set for final verdict, allow partial results for draft review only, (4) Summary must flag when verdict is computed on partial data."}
{"type":"finding","id":"v3-edge-case-prober-004","title":"Severity Range Resolution Creates False Escalations","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Verdict Logic (lines 145-146: \"Resolve severity for verdict logic... use highest severity in range for verdict computation (conservative)\")","issue":"When reviewers rate same issue differently (line 143: \"severity ranges\"), design uses \"highest severity in range for verdict computation\" (line 146). If Assumption Hunter rates finding Critical, Feasibility Skeptic rates same finding Important, verdict logic treats as Critical. This is conservative but creates false escalations. Example: 5 reviewers rate issue Important, 1 reviewer rates Critical (overly pessimistic), verdict triggers \"revise\" when issue is genuinely Important-tier. Design acknowledges this (line 168: \"If false escalations become a problem, investigate per-agent prompt tuning first\") but accepts false escalations as baseline.","why_it_matters":"Single pessimistic reviewer can block entire design. If Edge Case Prober consistently over-rates severity (sees every edge case as Critical), every review escalates regardless of actual risk. Design defers mitigation to prompt tuning (line 168) but prompt tuning is iterative—false escalations occur throughout prototyping phase. User abandons review process as too heavyweight before prompts stabilize.","suggestion":"Revise severity range resolution to use consensus instead of max. Specify: (1) If ≥50% of reviewers agree on severity, use consensus severity for verdict, (2) If no consensus, use highest severity but flag as \"disputed severity\" in summary, (3) User can downgrade disputed findings during processing, verdict recomputed based on accepted severities, (4) Track per-reviewer severity calibration over time (if reviewer consistently over-rates, flag for prompt tuning). Alternative: use median severity instead of max (still conservative but less sensitive to outliers)."}
{"type":"finding","id":"v3-edge-case-prober-005","title":"Prior Summary Context Injection Creates Confirmation Bias","severity":"Important","phase":{"primary":"design","contributing":"calibrate"},"section":"Cross-Iteration Finding Tracking (line 250: \"prior summary.md included in reviewer context\")","issue":"Reviewers receive prior iteration summary showing which findings were flagged before (line 250: \"reviewers explicitly note 'previously flagged, now resolved' vs 'new finding' vs 'still an issue'\"). This anchors reviewers to prior findings, creating confirmation bias. Reviewers focus on \"is Finding 3 still an issue?\" instead of \"what's wrong with this design?\" Fresh eyes see different problems, but prior context steers them toward validating old findings rather than discovering new ones. Design intends this (line 251: \"changed section focus\" via git diff) but doesn't account for bias.","why_it_matters":"Iteration 2+ reviews become \"check if iteration 1 findings are fixed\" instead of \"find all design flaws.\" New edge cases introduced by fixes are missed because reviewers are pattern-matching against prior findings. Adversarial review requires independent judgment—prior context contaminates that. V2 summary shows 55 findings with 15 Critical—if reviewers anchor to those 15, they miss new Critical findings in freshly-added sections.","suggestion":"Evaluate whether cross-iteration context helps or hurts via eval framework. Test: (1) Run reviewers blind (no prior context), synthesizer reconciles findings after review completes, (2) Provide only changed sections to reviewers (git diff per line 251), not prior findings, (3) Split reviewers: half get prior context (validation set), half run blind (discovery set), compare finding coverage, (4) Prior context only for synthesizer, not individual reviewers. For MVP, recommend option 2 (changed sections only) to balance efficiency with independence."}
{"type":"finding","id":"v3-edge-case-prober-006","title":"Verdict Escalation on Any Calibrate Gap Blocks Design Work","severity":"Important","phase":{"primary":"design","contributing":"calibrate"},"section":"Verdict Logic (lines 165-166: \"Survey or calibrate gap at any severity → escalate\")","issue":"Any finding classified as calibrate gap triggers \"escalate\" verdict \"regardless of severity\" (line 166). If Requirement Auditor flags \"anti-goal X not explicitly stated\" (Minor severity, calibrate gap), entire review escalates, user must revise requirements before continuing design. For complex designs, Minor calibrate gaps are common (missing edge case requirements, unstated assumptions). Escalating on every Minor calibrate gap makes iteration impossible. Design acknowledges multi-causal classification (line 161: \"primary + contributing\") but verdict logic (line 166) treats all calibrate gaps equally.","why_it_matters":"Design review becomes blocked by Minor requirement clarifications. User workflow: write design, run review, get \"escalate—revise requirements,\" update requirements doc with one sentence, re-review, get another Minor calibrate gap, escalate again. After 3 escalations for Minor issues, user abandons review process as too heavyweight. V2 summary shows 7 calibrate gaps—if all trigger escalate, design iteration impossible.","suggestion":"Revise verdict logic to weight severity for calibrate gaps. Specify: (1) Critical calibrate gap → escalate (fundamental requirement conflict), (2) Important calibrate gap → revise with note (can proceed but risk of rework), (3) Minor calibrate gap → proceed with note (document assumption, revisit if implementation reveals problem), (4) Synthesizer flags when multiple Minor calibrate gaps cluster (>30% threshold per line 161) and escalates that pattern, not individual findings. Update line 166 to: \"Survey or Critical calibrate gap at any severity → escalate.\""}
{"type":"finding","id":"v3-edge-case-prober-007","title":"JSONL Format Decided But Schema Still Unspecified","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Prototype Scope (line 320: \"JSONL output enables jq-based filtering\"), Output Artifacts (lines 33-47, markdown only)","issue":"Design references JSONL output (line 320: \"JSONL output enables jq-based filtering by severity/persona/phase without LLM tokens\") but specifies only markdown format throughout (lines 33-47: Output Artifacts, lines 103-122: Per-Reviewer Output Format, lines 173-212: Summary Format). No JSONL schema, field definitions, or migration path specified. V2 summary Finding 9 (Critical, 4 reviewers) flagged this, disposition accepted, but design still incomplete. Line 320 treats JSONL as implemented when it's unspecified.","why_it_matters":"JSONL vs markdown is structural decision affecting storage, parsing, finding IDs (line 248: stable hash), disposition tracking (line 249: status field), and tool integration (jq filtering per line 320). Multiple features depend on JSONL (cost logging per line 323, finding IDs per line 248, phase filtering per line 320) but can't be implemented without schema. Design describes throwaway scaffolding (markdown) not actual system (JSONL). If JSONL deferred to post-MVP, line 320 reference is premature.","suggestion":"Add JSONL schema section to design specifying structure even if implementation deferred. Minimal spec: `{\"id\": \"string (section + issue hash)\", \"iteration\": N, \"reviewer\": \"persona\", \"severity\": \"Critical|Important|Minor\", \"phase\": {\"primary\": \"survey|calibrate|design|plan\", \"contributing\": \"string|null\"}, \"section\": \"string\", \"issue\": \"string\", \"why_it_matters\": \"string\", \"suggestion\": \"string\", \"disposition\": {\"status\": \"pending|accepted|rejected\", \"note\": \"string|null\"}}`. Specify: (1) One JSON object per finding per line, (2) Markdown rendering for human readability, JSONL as canonical storage, (3) Migration path: reviewers output markdown, synthesizer converts to JSONL, both formats maintained during transition."}
{"type":"finding","id":"v3-edge-case-prober-008","title":"Multi-Causal Phase Classification Routing Still Unresolved","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Finding Phase Classification (lines 153-161, primary + contributing phases), Verdict Logic (lines 165-170, uses primary phase only)","issue":"Design added \"primary phase\" and \"contributing phase\" classification (line 161) but verdict logic uses only primary phase (lines 165-170). Contributing phase is metadata only. If finding is \"design flaw (primary) caused by calibrate gap (contributing),\" verdict routes to \"revise design\" when it should escalate to \"fix requirements first\" (line 161: \"enables two actions: immediate fix and systemic upstream correction\"). User fixes design, re-review finds same flaw because underlying calibrate gap persists. V2 summary explicitly notes this (line 161 added during disposition sync) but verdict logic not updated.","why_it_matters":"Multi-causal findings require multi-phase remediation. Routing on primary phase only treats symptom, not root cause. User iterates on design 3 times fixing surface issues while calibrate gap creates new design flaws each time. Line 161 specifies \"when >30% of findings share a contributing phase, synthesizer flags systemic issue\" but no connection to verdict logic. Systemic issue detection is advisory only.","suggestion":"Update verdict logic to check contributing phases. Specify: (1) If ≥30% of findings have \"calibrate gap (contributing),\" escalate regardless of primary phase classification (per line 161 systemic detection), (2) If any Critical finding has calibrate/survey contributing phase, flag as \"fix upstream first\" advisory in verdict reasoning, (3) Synthesizer produces two remediation paths: immediate (primary phase fixes) and systemic (contributing phase fixes), (4) User can choose to fix primary and defer systemic, but must acknowledge risk in disposition notes."}
{"type":"finding","id":"v3-edge-case-prober-009","title":"Selective Re-Run of Failed Reviewers Changes Finding Context","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Parallel Agent Failure Handling (line 222: \"Allow re-running individual failed reviewers without redoing successful ones\")","issue":"If Feasibility Skeptic times out, user re-runs only that reviewer (line 222). But reviewers may reference findings from other reviewers (deduplication awareness, cross-reviewer reasoning). Re-running in isolation produces different findings than running in parallel with others. Worse: synthesis now runs twice (once with 5/6 reviewers producing summary, once with 6/6 re-synthesizing), producing two different summaries. Which is authoritative? Line 222 enables selective re-run but doesn't specify synthesis behavior.","why_it_matters":"Selective re-run breaks reviewer independence assumption. Re-synthesizing findings after selective re-run either duplicates work (reprocess all 41 findings from initial run + 8 from re-run) or creates partial synthesis (only process new reviewer's findings, miss deduplication opportunities with initial findings). User expects \"add missing reviewer's findings to summary\" but design doesn't specify whether synthesis is additive or regenerative.","suggestion":"Clarify selective re-run semantics in design. Specify: (1) Re-run produces separate finding file, user manually merges into summary (no automatic synthesis), (2) Re-run triggers full re-synthesis (all reviewer outputs re-processed, summary regenerated, prior summary archived), (3) Selective re-run disabled—failed reviewers produce empty findings file, review proceeds with partial results only (line 220 threshold), user can re-run entire review if needed, (4) Allow re-run but mark summary as \"re-synthesized with subset\" to flag potential inconsistency. Recommend option 3 for MVP (simplest, avoids synthesis ambiguity)."}
{"type":"finding","id":"v3-edge-case-prober-010","title":"Empty Design Doc or Requirements Doc Produces Nonsense Review","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Skill Interface (lines 17-19: design and requirements as inputs)","issue":"Skill accepts file paths for design and requirements (lines 17-18) but specifies no validation that files exist, are non-empty, or are parseable markdown. If user accidentally points to wrong file (typo, empty file, binary file, corrupted markdown), reviewers receive empty context and either produce hallucinated findings or fail with cryptic errors. No input validation specified in design. Line 25 specifies topic label validation (\"safe character set\") but not artifact validation.","why_it_matters":"Garbage in, garbage out. User typo (`docs/plans/design.md` vs `docs/plans/desing.md`) points to non-existent file, reviewers see empty design, produce findings about \"missing all required sections.\" User processes findings, wastes 30 minutes before realizing input was wrong. At minimum, costs API tokens for nonsense review. Could also produce false positives (reviewer flags \"no architecture section\" when architecture exists but file path was wrong).","suggestion":"Add input validation to Skill Interface section. Specify: (1) Check files exist and are readable before dispatching reviewers, (2) Validate markdown parsing succeeds (no corruption), (3) Check minimum content length (design doc <100 chars likely wrong, requirements <50 chars likely wrong), (4) Show user preview or confirmation before dispatching (first 200 chars of each input + file size), (5) If validation fails, prompt user to correct and retry. For MVP, minimal validation (file exists, non-empty, valid markdown)."}
{"type":"finding","id":"v3-edge-case-prober-011","title":"Cost Logging Specified But No Format or Schema","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Prototype Scope (line 323: \"Cost logging per review run in JSONL output\")","issue":"Design states \"Cost logging per review run in JSONL output\" (line 323) but JSONL output format itself is unspecified (Finding 7). What cost data is logged? Per-reviewer token counts? Aggregated cost? Cache hit rates? Model used? Timestamp? If format undefined, cost logging can't be implemented consistently. If JSONL output deferred to post-MVP (per Finding 7), cost logging has no destination. Line 323 references feature that depends on unspecified infrastructure.","why_it_matters":"Cost tracking is budgetary requirement (CLAUDE.md: $2000/month budget, $150-400 projected API spend). Without schema, cost data inconsistent across review runs, can't be aggregated or analyzed. Requirements specify model tiering decisions based on empirical cost data (Haiku vs Sonnet)—can't make those decisions without structured cost logs. Line 323 acknowledges \"pre-dispatch estimates deferred until empirical cost data available\" but doesn't specify how empirical data is collected.","suggestion":"Define cost logging schema regardless of JSONL timeline. Specify: (1) Per-reviewer cost breakdown (input tokens, output tokens, cached tokens, cache hit rate, cost at model pricing, model used), (2) Aggregated review cost (total tokens, total cost), (3) Cost estimate vs actual (when estimates added later), (4) Timestamp, topic, iteration number, reviewer completion status, (5) Schema: `{\"topic\": \"string\", \"iteration\": N, \"timestamp\": \"ISO8601\", \"reviewers\": [{\"persona\": \"string\", \"model\": \"string\", \"input_tokens\": N, \"output_tokens\": N, \"cached_tokens\": N, \"cost_usd\": N}], \"total_cost_usd\": N}`. Interim solution: log to separate `cost.json` file if JSONL output deferred."}
{"type":"finding","id":"v3-edge-case-prober-012","title":"Review Stage Auto-Detection Deferred But Impacts All Stages","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Skill Interface (lines 20-21: \"Auto-detection deferred (YAGNI for MVP—we know the stage)\")","issue":"Design defers auto-detection of review stage (requirements/design/plan) as YAGNI (line 21) but plans to support all 3 stages eventually (lines 64-99: Reviewer Personas, lines 326-330: Build later). User must manually specify stage for every review. If user specifies wrong stage (reviews design with requirements-stage personas), findings are off-target. No validation that stage matches artifact content. For MVP with design-stage only (line 25), low-risk. For post-MVP multi-stage support (line 326), wrong stage selection produces nonsense reviews.","why_it_matters":"Manual stage specification is error-prone. User runs `parallax:review --stage requirements` on a design doc by mistake, gets Product Strategist findings (requirements persona per line 72) that don't apply to design-stage artifact. Findings reference \"success metrics\" and \"user value\" when design doc discusses architecture. User wastes time processing irrelevant findings. For MVP, user knows stage (single-stage support). For post-MVP, multi-stage support requires validation or auto-detection.","suggestion":"Add stage validation to Skill Interface for when multi-stage support is implemented (post-MVP per line 326). Specify: (1) Heuristic check: design docs contain \"Architecture\" sections, requirements docs contain \"MoSCoW\" or \"Success Criteria\" sections, plans contain \"Task\" or \"Implementation\" sections, (2) Prompt user if stage doesn't match heuristic (\"You specified 'requirements' but doc contains 'Architecture' section—continue? [y/n]\"), (3) Auto-detection as suggestion with user confirmation, not automatic override. For MVP, document single-stage limitation (design only), defer validation to post-MVP."}
{"type":"finding","id":"v3-edge-case-prober-013","title":"Timestamped Folder Collision Creates Review Folder Sprawl","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Topic Labels (line 23: \"Collision handling: timestamped folders for iteration separation\")","issue":"Each iteration creates timestamped folder per line 23 (`docs/reviews/parallax-review-2026-02-15-143022/`). After 5 iterations, `docs/reviews/` contains 5 folders with 35+ files total (7 files per iteration per lines 36-44). No cleanup mechanism, no \"latest iteration\" symlink, no archival strategy. User must manually track which folder is current. Git history shows all folders, diffs are useless (comparing different paths). Requirements specify \"diffs show what changed\" (iteration comparison is core workflow per problem statement line 174) but timestamped folders make git diff impossible.","why_it_matters":"10 active topics × 3 iterations each = 30 folders, 210 files in `docs/reviews/`. Navigation becomes expensive. Git diff comparing `parallax-review-2026-02-15-143022/summary.md` to `parallax-review-2026-02-15-150341/summary.md` shows file addition, not content diff. Requirements (line 174: \"Git commits per iteration. Full history, diffable artifacts\") conflict with timestamped folders. This is calibrate gap—collision handling contradicts diffability requirement.","suggestion":"Revise folder structure to resolve requirement conflict. Options: (1) Single folder per topic, overwrite on re-review, rely on git history for iteration tracking (`git diff HEAD~1 docs/reviews/parallax-review/summary.md` shows content changes), (2) Nested iteration structure: `docs/reviews/parallax-review/iteration-1/`, `docs/reviews/parallax-review/iteration-2/` with symlink `latest/` pointing to most recent, (3) Timestamped folders but synthesizer produces cross-iteration diff report in root (`docs/reviews/parallax-review-diff.md`), (4) Archive old iterations to `docs/reviews/.archive/` after disposition completion. Recommend option 1 for MVP (simplest, satisfies diffing requirement, collision handled by overwrite + git history)."}
{"type":"finding","id":"v3-edge-case-prober-014","title":"Reviewer Timeout Without Progress Updates Appears Frozen","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Parallel Agent Failure Handling (line 218: \"Timeout: 60-120s per agent\"), UX Flow Step 2 (line 271: \"streams progress\")","issue":"Reviewers have 60-120s timeout (line 218). If reviewer takes 90s (near timeout), user sees progress update at start (\"Assumption Hunter dispatched [1/6]\" per line 271) then silence for 90s before next update (\"Assumption Hunter: done [1/6]\"). No indication reviewer is working vs frozen. User may interrupt thinking review is stuck. Step 2 specifies \"streams progress as reviewers complete\" and \"shows elapsed time\" (line 271) but not per-reviewer heartbeat during execution.","why_it_matters":"Long-running reviewers (complex designs, deep analysis) create UX anxiety. User doesn't know if 90s delay means \"reviewer is thinking hard\" or \"API call hung.\" For 6 parallel reviewers, max elapsed time is ~120s if one reviewer is slow—too long without feedback. For async mode (line 261), this is non-issue—user doesn't watch execution. For interactive mode, silence creates uncertainty.","suggestion":"Add per-reviewer progress updates to UX Flow Step 2. Specify: (1) Stream intermediate updates every 15-30s (\"Assumption Hunter: analyzing... [45s elapsed]\"), (2) Show all reviewers with status indicators (in-progress spinner, completed checkmark, failed X, timeout warning), (3) Add timeout warning at 80% of timeout threshold (\"Edge Case Prober approaching timeout [96s/120s], may fail\"), (4) For async mode, log final results only (no intermediate updates). For MVP, minimal: elapsed time counter for entire review run, final status per reviewer. Full per-reviewer streaming deferred to UX polish phase."}
{"type":"finding","id":"v3-edge-case-prober-015","title":"No Handling for Concurrent Reviews on Same Topic","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Output Artifacts (line 36: `docs/reviews/<topic>/`), Topic Labels (line 23)","issue":"Two users (or same user in two terminals) invoke `parallax:review` on same topic simultaneously. Both create `docs/reviews/<topic>/`, both dispatch reviewers, both write summary.md. Last writer wins, first review's findings are overwritten. No file locking, no collision detection beyond timestamped folders (line 23, which only helps if users use different topic labels). For team contexts (CLAUDE.md: \"applicable to work contexts\"), concurrent reviews are plausible.","why_it_matters":"In team contexts, concurrent reviews cause data loss. Two engineers reviewing same design doc in parallel, or user re-runs review while first run still in progress (forgot it's running in background per line 261 async mode). Overwriting findings loses work. Timestamped folders (line 23) solve collision if different timestamps (rare—collisions unlikely within same second) but don't solve concurrent writes to same timestamped folder if both reviews start simultaneously.","suggestion":"Add concurrent review handling to design. Specify: (1) Check for in-progress review (lockfile `docs/reviews/<topic>/.lock` created at start, deleted at end) before starting, (2) Timestamped folders by default for all reviews (always collision-safe but breaks diffing per Finding 13), (3) Prompt user if target folder exists and is <5 minutes old (\"Review may be in progress—abort or continue?\"), (4) Document single-user limitation if multi-user is out of scope for MVP. Recommend option 1 for MVP (lockfile, minimal implementation, prevents most collisions)."}
