{"type": "finding", "id": "problem-framer-001", "title": "Problem statement unchanged despite root cause being wrong in v1", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "Problem Statement (unchanged from v1)", "issue": "The v2 problem statement is verbatim from v1: 'no systematic way to measure effectiveness.' But v1 failed not because measurement was absent — it failed because the measurement architecture assumed skills were single-turn reviewers when they are orchestration workflows. The real root cause is: the eval framework was designed against a wrong mental model of what a 'skill' is. Carrying forward an incorrect problem statement risks repeating the same class of assumption errors in v2.", "why_it_matters": "If the problem is framed as 'no measurement capability,' the fix is 'add measurement.' But if the problem is 'wrong mental model of the unit under test,' the fix requires explicitly establishing what the testable unit is before writing requirements. v2 adds FR-ARCH-1 through FR-ARCH-5 without updating the problem statement to reflect the lesson learned — future reviewers and implementers will not understand why v1 failed from the problem statement alone.", "suggestion": "Update the problem statement to include: 'v1 implementation revealed that the eval unit is the individual reviewer agent, not the full orchestration skill. The root cause of v1 failure was treating orchestration workflows as eval system prompts. v2 defines the testable unit explicitly.' This makes the lesson learned part of the requirements contract, not buried in the Background section."}
{"type": "finding", "id": "problem-framer-002", "title": "v2 addition reframes a solution as a problem requirement", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Problem Statement (unchanged from v1)", "issue": "The 'v2 addition' to the problem statement reads: 'The eval framework itself must be testable at the component level. The unit of testing is the individual reviewer agent.' This is a solution prescription embedded in a problem statement. It asserts HOW to solve the architecture problem (decompose to per-reviewer tasks) before establishing WHY the orchestration-level test is insufficient. The jump from symptom (0.000 accuracy) to solution (per-reviewer decomposition) skips the problem analysis step.", "why_it_matters": "Skipping problem analysis means the alternative framing — test the orchestration skill via mock tools — is never evaluated. FR-ARCH-1 rationale does mention the alternative implicitly but dismisses it without formal comparison. A reader cannot tell from the requirements whether per-reviewer decomposition was chosen over mock-tools testing or in addition to it.", "suggestion": "Separate the problem from the solution in the problem statement. State: 'v1 revealed that orchestration-level testing is insufficient because [specific reasons]. Component-level testing of individual reviewer agents is required to [achieve these outcomes].' Then let FR-ARCH-1 describe the solution. The problem statement should answer 'why isn't orchestration-level testing enough?' not assert the solution."}
{"type": "finding", "id": "problem-framer-003", "title": "ADR-006 referenced as authoritative but does not exist", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": "design"}, "section": "Background", "issue": "The Background section states 'See ADR-006 for the full decision rationale' and FR-ARCH-1 rationale also defers to ADR-006. ADR-006 is not present in the docs/requirements/ directory. All five new functional requirements (FR-ARCH-1 through FR-QUALITY-1) have their decision rationale partially described inline but reference ADR-006 for completeness. A requirements document that delegates its key rationale to a non-existent reference is incomplete.", "why_it_matters": "Requirements without traceable rationale are fragile. If ADR-006 does not exist when implementation begins, developers must reconstruct the decision context from Background prose and session notes (MEMORY.md). The per-reviewer decomposition decision — including tradeoffs considered and alternatives rejected — exists nowhere that travels with git.", "suggestion": "Either: (a) create ADR-006 before this requirements doc is finalized, or (b) inline the key decision rationale into the Background section and remove the ADR-006 reference. ADR-005 is a good model — it exists, is referenced, and contains the actual decision logic. Don't mark requirements as final while referencing documents that don't exist yet."}
{"type": "finding", "id": "problem-framer-004", "title": "FR-ARCH-2 dual-context requirement creates unresolvable design tension", "severity": "Important", "phase": {"primary": "calibrate", "contributing": "design"}, "section": "FR-ARCH-2: Eval-Compatible Agent Interface", "issue": "FR-ARCH-2 requires agents to function correctly in both (1) production context with full Claude Code tool access, multi-turn, and (2) eval context with no tools, single-turn — without modifying the agent file. The acceptance criteria then specifies: 'Agent prompts instruct: The design document content will be provided to you in this message.' But in production, the document is not provided in the message — it is fetched via tool calls. These are structurally different inputs. The requirement that the same file satisfies both contexts is stated as a given, not analyzed as a constraint.", "why_it_matters": "If this constraint cannot be satisfied for some agents (e.g., an agent that needs tool calls to resolve references in the design doc), either the eval degrades to a lower-fidelity test or the agent file is forked. Neither outcome is acknowledged. The requirement as written may force agent prompts to be worse in production (verbose 'content will be provided' language) to satisfy the eval context.", "suggestion": "Explicitly state whether this is a hard constraint or a preference: 'Agent files must not fork between production and eval context because [reason]. If an agent cannot satisfy both contexts from one file, [resolution].' Also define the fidelity implications: 'Eval context is a lower-fidelity approximation of production — the goal is to test reasoning quality, not tool-use behavior.' This frames the constraint honestly."}
{"type": "finding", "id": "problem-framer-005", "title": "FR-ARCH-3 treats output format as requirement but root cause is missing instructions", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR-ARCH-3: JSONL Output Format as Explicit Requirement", "issue": "FR-ARCH-3 states 'the output format is a requirement, not an implementation choice' and mandates JSONL. The rationale is that 'assumption-hunter was the only agent with markdown output format.' But the root cause is that individual agents were not given consistent, explicit output format instructions during initial development. Mandating JSONL as a requirement is correct, but it treats a consistency problem (some agents had format instructions, some didn't) as if it were a deliberate design ambiguity. The requirement does not state what should happen if an agent has good reasons to output a different format in future.", "why_it_matters": "A format mandate without a rationale for why JSONL specifically creates a brittle contract. If a future reviewer agent needs to output structured prose (e.g., a synthesizer), the requirement as written blocks it without exception process. More concretely: the acceptance criteria 'Zero findings parsed is treated as an output format failure' could mask legitimate cases where an agent found nothing to report.", "suggestion": "Add rationale: 'JSONL is required for machine-parseable output that enables automated scoring. Agents that output zero findings must still confirm JSONL format compliance (e.g., emit an explicit empty-result line) so that zero-findings-found is distinguishable from format failure.' Also explicitly state: 'This requirement applies to all reviewer agents in eval context. Non-reviewer agents (synthesizer, orchestrator) are out of scope.'"}
{"type": "finding", "id": "problem-framer-006", "title": "v2 has no success criteria — only a phase map", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "MVP Phase Map (updated from v1)", "issue": "v1 had a 'Success Criteria' section with eight explicit, numbered criteria defining when MVP was complete. v2 removes this and replaces it with a Phase Map table. The Phase Map shows Phase 1 status as 'In progress' and lists blocking issues, but does not define what 'Phase 1 complete' means in measurable terms. FR-ARCH-1 has acceptance criteria, but there is no document-level success gate that answers: 'How do we know v2 requirements are correctly implemented?'", "why_it_matters": "Without v2 success criteria, the requirements document cannot be used to declare implementation done. The v1 success criteria included '≥90% detection rate' and '<5 minute eval run' — measurable gates. v2 additions (per-reviewer decomposition, JSONL output, ground truth refresh) have acceptance criteria per-FR but no synthesized completion signal. This means 'done' is determined by whoever is implementing, not by the requirements.", "suggestion": "Add a 'v2 Success Criteria' section that defines Phase 1 completion in terms of: (1) all 5 per-reviewer eval tasks run without error, (2) at least one reviewer achieves non-zero accuracy on ground truth, (3) JSONL output parseable from all reviewer agents, (4) eval run cost within FR-ARCH-5 budget. Do not use time estimates. Use measurable outcomes."}
{"type": "finding", "id": "problem-framer-007", "title": "Alternative architecture (mock tools) excluded without documented rationale", "severity": "Important", "phase": {"primary": "calibrate", "contributing": "survey"}, "section": "FR-ARCH-1: Per-Reviewer Eval Task Decomposition", "issue": "FR-ARCH-1 supersedes v1's orchestration-level eval approach. The session notes (MEMORY.md) mention Tier 2 (mock tools pattern) as 'high priority' and 'generic enough to reuse across skill types.' The v2 requirements document does not reference mock-tools testing at all. The Phase Map includes Phase 2 as 'mock tools (Tier 2)' but this is described as a separate, future concern rather than as an alternative to per-reviewer decomposition. The tradeoff between the two approaches — what each catches that the other misses — is not documented.", "why_it_matters": "Per-reviewer eval tests whether individual agents reason correctly about a document. Mock-tools eval tests whether the orchestration skill correctly dispatches agents, collects results, and synthesizes. These are different failure modes. If v2 only implements per-reviewer tasks, the orchestration layer has no coverage. This gap may not matter for Phase 1, but it should be explicitly acknowledged as a coverage limitation, not silently absent.", "suggestion": "Add a 'Coverage Scope and Limitations' subsection to FR-ARCH-1 that states: 'Per-reviewer tasks cover reasoning quality of individual agents. They do not test orchestration correctness (dispatch, collection, synthesis). Mock-tools testing (Phase 2) is required to cover orchestration. A passing per-reviewer eval does not imply the full skill works end-to-end.' This makes the coverage gap explicit and intentional."}
{"type": "finding", "id": "problem-framer-008", "title": "Open Question 2 (reviewer consensus) has requirements implications if left unresolved", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Open Questions (v2)", "issue": "Open Question 2 asks: 'If 3/5 reviewers independently flag the same root cause, should duplicates be deduplicated to one ground truth finding or kept as separate findings for each reviewer?' This is not a theoretical question — it directly affects FR-ARCH-1's acceptance criteria ('Each task filters ground truth to findings where reviewer == agent_name'). If the same root cause appears in three reviewer outputs and is stored once in ground truth, two of the three per-reviewer tasks will have empty ground truth sets, and accuracy metrics will be misleading.", "why_it_matters": "If this question is unresolved at implementation time, the implementer must make an undocumented choice that affects ground truth validity, per-reviewer metrics, and the meaning of 'detection rate.' A ground truth dataset where some reviewer tasks have zero matching findings due to deduplication will produce artificially low per-reviewer recall.", "suggestion": "Resolve this in requirements: 'Duplicate findings across reviewers that share a root cause are stored once in ground truth with a reviewer_coverage field listing all reviewers that detected it. Per-reviewer tasks match against findings where the reviewer appears in reviewer_coverage. This preserves cross-reviewer redundancy as signal (consensus) rather than noise (duplication).'"}
