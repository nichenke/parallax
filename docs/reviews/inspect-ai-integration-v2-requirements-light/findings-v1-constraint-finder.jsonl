{"type": "finding", "id": "constraint-finder-001", "title": "Python version constraint unspecified for eval framework", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "FR-ARCH-1: Per-Reviewer Eval Task Decomposition", "issue": "The requirements specify Python-based eval definitions in `evals/reviewer_eval.py` but never state a minimum Python version. Inspect AI 0.3+ requires Python 3.10+ for match statement syntax and modern typing constructs. If the development environment uses Python 3.9 or earlier, the entire eval framework breaks at import time. The prior art research (prior-art-spike-findings.md) references CPython 3.14 in pycache paths, suggesting the dev environment may be ahead of what CI or teammates use.", "why_it_matters": "A Python version mismatch causes silent environment-specific failures: `make eval` works locally but fails in any other environment. Reproducing bugs across environments becomes difficult without a pinned constraint.", "suggestion": "Add to requirements: 'Python >=3.10 required (Inspect AI dependency). Pin in pyproject.toml `requires-python = \">=3.10\"` and document in README. Verify against CI environment before Phase 1 completes.'"}
{"type": "finding", "id": "constraint-finder-002", "title": "Inspect AI version not pinned — breaking changes not guarded against", "severity": "Critical", "phase": {"primary": "design", "contributing": null}, "section": "FR-ARCH-1: Per-Reviewer Eval Task Decomposition", "issue": "The requirements depend on Inspect AI conventions (Dataset/Sample/Task/Scorer, `system_message()`, `MemoryDataset`, `@task` decorator) but never specify which version of inspect_ai is required. The prior art research notes 'Latest release: Feb 12, 2026' — a framework under active development. API changes between minor versions (e.g., Sample input type changed from dict to str, discovered during Session 21) caused accuracy: 0.000 that took a full debug session to find. Without a pinned version, any `pip install --upgrade` can silently re-break the eval framework.", "why_it_matters": "Inspect AI is actively maintained (47M+ downloads, AISI-backed). API surface changes between minor versions. A broken eval framework defeats the purpose of the eval framework — you cannot trust regression detection if the tooling itself can drift.", "suggestion": "Require: 'Pin inspect_ai to a specific version in pyproject.toml (e.g., `inspect_ai==0.3.x`). Add changelog review step before any version bump. Document the exact API surface used (Sample, MemoryDataset, @task, system_message, @scorer) so version compatibility can be checked quickly.'"}
{"type": "finding", "id": "constraint-finder-003", "title": "ANTHROPIC_API_KEY availability in eval context not specified", "severity": "Critical", "phase": {"primary": "design", "contributing": null}, "section": "FR-ARCH-2: Eval-Compatible Agent Interface", "issue": "FR-ARCH-2 requires agents to function 'correctly in eval context' and FR-ARCH-5 specifies cost budgets per run, but neither addresses where the API key comes from during eval execution. The requirements mention both Bedrock (work) and direct API access as available in CLAUDE.md. These require different environment variable names (`ANTHROPIC_API_KEY` vs AWS credentials) and different Inspect AI provider configuration. Phase 1.5 adds Google (Gemini), which requires yet another credential. There is no stated requirement for how credentials are managed across environments (local dev, CI, different machines).", "why_it_matters": "If the API key source is wrong, every eval run fails at the provider initialization step — before any test logic runs. The cost budget in FR-ARCH-5 also becomes meaningless if evals run against the wrong account (Bedrock vs direct may have different billing).", "suggestion": "Add requirement: 'Specify credential source for each eval environment: local dev (ANTHROPIC_API_KEY from shell), CI (secret injection mechanism TBD), multi-model (separate env vars per provider). Document in `evals/README.md`. `make eval` must fail fast with a clear error if required credentials are absent, before making any API calls.'"}
{"type": "finding", "id": "constraint-finder-004", "title": "Content hash function for ground truth refresh not specified", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "FR-ARCH-4: Ground Truth Refresh Cadence", "issue": "FR-ARCH-4 requires `metadata.json` to include `design_doc_hash` and `make validate` to re-run if the hash differs. However, the requirements do not specify: which hash function (MD5, SHA-256?), whether whitespace normalization is applied before hashing (trailing newlines cause spurious hash mismatches on macOS vs Linux), or whether the hash covers the full file or only content-meaningful sections. A hash computed on raw bytes will differ between platforms if line endings differ (CRLF vs LF), triggering unnecessary ground truth re-validation on every clone.", "why_it_matters": "Spurious hash mismatches cause false 'ground truth stale' alerts, eroding trust in the automation and creating friction in the TDD loop. Inconsistent hash behavior across platforms (macOS dev, CI) is a subtle constraint that will surface only when someone other than the original developer runs the eval.", "suggestion": "Specify: 'Hash function: SHA-256 of UTF-8 file content with normalized line endings (LF). Implementation: `hashlib.sha256(content.encode(\"utf-8\")).hexdigest()`. Document normalization in `evals/utils/dataset_loader.py`. Add a test that verifies the same hash is produced from a file with CRLF endings.'"}
{"type": "finding", "id": "constraint-finder-005", "title": "Cost budget assumes direct API pricing — Bedrock pricing differs", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "FR-ARCH-5: Cost Budget Per Eval Suite Run", "issue": "FR-ARCH-5 specifies concrete cost targets ($0.10/task, $0.50/suite, $2.00 with LLM judge, $1.50 multi-model). CLAUDE.md states that Claude API is available via both Bedrock (work) and direct API, with different cost structures. Anthropic direct pricing and Bedrock pricing for the same Claude model differ (Bedrock adds per-token overhead and has different rate limits). The batch API (50% discount lever #1 in FR-ARCH-5) is available on direct API but Bedrock batch behavior differs. `make cost-report` reading eval logs will report token counts but cannot translate to dollars without knowing which provider was used.", "why_it_matters": "If evals are routinely run on Bedrock (work account) but budgets are calibrated against direct API pricing, the cost signal is systematically wrong. The $2,000/month budget in CLAUDE.md becomes unreliable as a guard.", "suggestion": "Add requirement: 'Cost report must include provider name alongside estimated cost. Budget thresholds in FR-ARCH-5 are denominated in direct-API pricing. Bedrock runs note that actual billing may differ. `make cost-report` derives provider from EvalLog metadata and annotates accordingly.'"}
{"type": "finding", "id": "constraint-finder-006", "title": "No rate limit constraint specified for multi-model Phase 1.5 runs", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "MVP Phase Map (updated from v1)", "issue": "Phase 1.5 runs 3 models × 5 agents simultaneously. Anthropic direct API enforces per-model rate limits (requests/minute, tokens/minute) that vary by tier. Running 15 concurrent eval tasks (3 models × 5 agents) against the same API account can hit rate limits, causing eval tasks to fail with 429 errors mid-run. The requirements specify cost budgets but not throughput constraints. Batch API (the primary cost reduction lever) has its own rate limits and a 1-24 hour completion window that is incompatible with interactive eval runs.", "why_it_matters": "Rate limit hits during a multi-model eval run produce partial results — some agents complete, others fail. Partial results are worse than no results because they appear as low detection accuracy rather than a clearly-errored run. Debugging requires distinguishing 'agent missed finding' from 'agent hit rate limit.'", "suggestion": "Add requirement: 'Multi-model eval runs (Phase 1.5) must handle 429 rate limit responses with exponential backoff. Inspect AI retry configuration must be set before Phase 1.5. Document rate limit tiers for each provider in `evals/README.md`. `make cost-report` flags runs with retry counts >0.'"}
{"type": "finding", "id": "constraint-finder-007", "title": "Ground truth dataset size constraint for statistical significance unstated", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Open Questions (v2)", "issue": "Open Question 3 asks whether 10 validated Critical findings is sufficient for Phase 1 detection baseline, but the requirements do not provide a principled answer. With 10 ground truth findings and a per-reviewer task, some reviewers may have only 2-3 relevant ground truth findings (findings are filtered to `reviewer == agent_name`). With N=2, an 80% precision target means 1.6 findings — precision/recall statistics are unreliable at this sample size. A single false positive changes precision from 100% to 50%. The evaluation framework cannot distinguish 'agent is bad' from 'sample too small' with fewer than ~10 findings per reviewer.", "why_it_matters": "If Phase 1 declares a detection baseline from 2-3 findings per reviewer, Phase 2 improvements will be tuned against noise. Regressions may be undetected because the confidence interval is wider than the regression signal.", "suggestion": "Add constraint: 'Minimum ground truth findings per reviewer agent before Phase 1 baseline is considered valid: 5 findings. If any reviewer has <5 ground truth findings after filtering, expand ground truth to include Important findings for that reviewer before finalizing Phase 1. Document sample size rationale in `datasets/README.md`.'"}
{"type": "finding", "id": "constraint-finder-008", "title": "Eval file discovery constraint: inspect CLI working directory sensitivity", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "FR-ARCH-1: Per-Reviewer Eval Task Decomposition", "issue": "The requirements specify `make reviewer-eval` runs all per-reviewer tasks, but do not state that the Makefile must be invoked from the repo root. Session 20 learning explicitly noted 'DATASET_PATH must be file-relative: `Path(__file__).parent...` not a bare string. CWD-dependent paths fail when inspect CLI invoked from different directories.' FR-ARCH-1's acceptance criteria ('`make reviewer-eval` runs all per-reviewer tasks') passes if invoked from repo root but silently breaks if invoked from `evals/` or any other subdirectory. This is an environmental constraint on how the tool is used.", "why_it_matters": "A developer running `cd evals && make reviewer-eval` (natural reflex when working in the evals directory) gets a confusing error about missing dataset files rather than a clear 'must run from repo root' message. This was already discovered once and not captured as a constraint.", "suggestion": "Add requirement: 'All `make` targets that invoke inspect CLI must be defined in the repo root Makefile. The Makefile must include a guard: `$(if $(wildcard pyproject.toml),,$(error Run make from repo root))`. Document in `evals/README.md` that eval commands must be run from repo root.'"}
{"type": "finding", "id": "constraint-finder-009", "title": "Agent system prompt length constraint not specified for eval context", "severity": "Minor", "phase": {"primary": "design", "contributing": null}, "section": "FR-ARCH-2: Eval-Compatible Agent Interface", "issue": "FR-ARCH-2 requires agent system prompts to be 'self-contained' and to instruct that document content will be provided in the message. However, reviewer agents receive both a system prompt (their persona) and the full document content in `Sample.input`. Large documents combined with verbose agent personas can approach or exceed context window limits for Haiku (200k tokens but smaller practical limits for cost). No maximum agent prompt length or document size limit is specified. FR-ARCH-5 sets cost budgets but cost depends directly on token count, and token count depends on prompt length.", "why_it_matters": "An agent system prompt that balloons (e.g., by adding extensive examples per FR-QUALITY-1) combined with a large reviewed document can exceed the budget thresholds in FR-ARCH-5 without any warning. The cost report would show overages only after the fact.", "suggestion": "Add constraint: 'Each reviewer agent system prompt must remain under 2,000 tokens (approximately 1,500 words). Measure with `tiktoken` during development. Document in agent template. If a requirements doc exceeds 4,000 tokens, consider chunking strategy before Phase 2.'"}
{"type": "finding", "id": "constraint-finder-010", "title": "Blind spot check: Constraint Finder perspective", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Meta", "issue": "This review focused on technical and environmental constraints (Python version, API keys, rate limits, hash behavior, working directory). Areas potentially underweighted: (1) Regulatory/compliance constraints — none raised because this is internal R&D with no PII or regulated data in the eval datasets. (2) Team size constraints — the project appears to be a single developer; multi-developer constraints (conflicting ground truth edits, parallel eval runs exhausting budget) may be premature. (3) Dependency on the Inspect AI community — if AISI deprioritizes the project or changes licensing, there is no fallback plan stated. (4) The 'Google account' blocker for Phase 1.5 multi-model (noted in the Phase Map) is a real organizational constraint that is dismissed as 'not started' without a mitigation path.", "why_it_matters": "Blind spot awareness helps catch gaps in the review process.", "suggestion": "Consider: (1) Add a brief note in the requirements about what happens if Inspect AI breaks a core API between releases (fallback: pin version and defer upgrade). (2) The Google account blocker for Phase 1.5 should be filed as a dependency that needs a resolution date, not just flagged as 'not started.'"}
