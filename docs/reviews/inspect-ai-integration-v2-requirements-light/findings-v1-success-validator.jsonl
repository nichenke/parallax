{"type": "finding", "id": "success-validator-001", "title": "Phase 1 completion has no measurable success criterion", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "MVP Phase Map", "issue": "The Phase map states Phase 1 goal is to produce 'non-zero accuracy' but never defines a numeric threshold for what constitutes Phase 1 complete. 'Non-zero' could be 1% detection — which is not a useful baseline. There is no criterion that gates progression to Phase 1.5.", "why_it_matters": "Without a concrete Phase 1 exit threshold, the team cannot know when to advance. Phase 1.5 (multi-model comparison) requires a stable baseline to compare against — if the baseline is undefined, the comparison is meaningless.", "suggestion": "Add a Phase 1 completion criterion to the phase map: e.g., 'Phase 1 complete when: (a) all 5 reviewer tasks parse non-zero findings, (b) detection rate ≥50% on ground truth Critical findings across all reviewers combined, (c) make eval exits 0.' Set 50% as a conservative floor to confirm architecture correctness before tuning."}
{"type": "finding", "id": "success-validator-002", "title": "v1 detection thresholds (90%/80%) not confirmed or superseded in v2", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR-QUALITY-1", "issue": "v1 established 90% detection rate (recall) and 80% precision as targets. v2 references these only in an example finding (v1-success-validator-001) but never restates, adjusts, or explicitly deprecates them. It is unclear whether these thresholds still apply to Phase 1, apply only after Phase 2 rubric is in place, or have been replaced by something else.", "why_it_matters": "The scorer and ablation tests cannot determine pass/fail without a stated target. If the 90%/80% thresholds are still the goal but only apply post-Phase 2, running evals in Phase 1 against those numbers will produce misleading 'failures' that are actually expected gaps.", "suggestion": "Add a 'Detection Targets' section: state explicitly that Phase 1 target is a minimum floor (e.g., ≥50% recall, any precision), Phase 2 target is the v1 thresholds (90% recall, 80% precision), and the rubric quality score (≥3.5/5.0) is a Phase 2 gate. Make each threshold phase-gated."}
{"type": "finding", "id": "success-validator-003", "title": "FR-ARCH-2 acceptance criteria define structure but not correctness", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR-ARCH-2: Eval-Compatible Agent Interface", "issue": "The acceptance criteria for FR-ARCH-2 verify that agent prompts have the right shape (self-contained, instruct document delivery, no tool calls required, output parseable). None of these criteria verify that the agent actually produces useful findings in eval context — a compliant agent could produce 5 trivially low-quality findings and pass all criteria.", "why_it_matters": "FR-ARCH-2 is intended to ensure agents 'function correctly in eval context' (per the requirement statement), but the criteria only check format compliance. An agent that satisfies all criteria but produces garbage findings will not be caught until Phase 2's quality rubric is applied — which is blocked on Phase 1 being stable.", "suggestion": "Add a behavioral criterion: 'When evaluated against the requirements-light ground truth document, each agent produces at least 1 finding that matches a Critical ground truth finding (by title or root cause) — confirming the agent is functional in eval context, not just structurally compliant.'"}
{"type": "finding", "id": "success-validator-004", "title": "make validate staleness check has no defined outcome requirement", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR-ARCH-4: Ground Truth Refresh Cadence", "issue": "The acceptance criterion states 'make validate re-runs if document hash differs from stored hash' but does not specify what the re-run must produce or what outcome is acceptable. Re-running validation could succeed (ground truth still valid), fail (findings no longer match), or be inconclusive. There is no criterion for each outcome.", "why_it_matters": "A staleness check that triggers a re-run without a defined pass/fail outcome provides no actionable signal. If the re-run produces different results, the engineer has no specified action to take — the requirement reads as 'detect stale, re-run' but not 'and then what?'", "suggestion": "Extend the acceptance criterion: 'If hash differs and re-run produces ground truth findings with <80% overlap with stored ground truth, make validate exits non-zero and prints: GROUND TRUTH STALE — refresh dataset before running evals.' Define overlap as title-based exact match or semantic match threshold."}
{"type": "finding", "id": "success-validator-005", "title": "Cost budget acceptance criteria have no blocking behavior defined", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR-ARCH-5: Cost Budget Per Eval Suite Run", "issue": "The acceptance criteria state 'if single run exceeds budget, report flags for optimization' — but 'flags' is not actionable. The criteria do not specify: does the eval fail? Does make eval exit non-zero? Is a warning printed only? Is the result still usable? A cost overrun that only produces a warning will be ignored in practice.", "why_it_matters": "Cost budgets are only enforced if exceeding them has a consequence. A silent warning in logs is not a consequence. Without a blocking exit or escalation path, the budget constraint has no teeth — Phase 1.5 multi-model runs could silently exceed budget by 10x before anyone notices.", "suggestion": "Specify: 'make cost-report exits 0 if under budget, exits 1 (with cost breakdown printed to stderr) if any task exceeds per-task budget. Full suite overrun exits 1 with suite total. CI run (once added) treats cost exit 1 as build warning, not failure.' This makes the threshold testable."}
{"type": "finding", "id": "success-validator-006", "title": "Quality rubric target score (3.5/5.0) has no pass/fail gate defined", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR-QUALITY-1: Quality Rubric Definition", "issue": "FR-QUALITY-1 states 'target aggregate quality score defined (e.g., ≥3.5/5.0 average across all dimensions)' but this is written as an example, not a requirement. The acceptance criterion for the rubric does not specify what happens if the average score is below the target — there is no defined gate for Phase 2 completion or reviewer agent promotion.", "why_it_matters": "A quality score with no consequence is a vanity metric. If reviewer agents score 2.8/5.0, is Phase 2 complete? Is the agent rejected? Is the eval flagged? Without a defined gate, the rubric validates nothing — it just produces a number.", "suggestion": "Firm up the criterion: replace 'e.g., ≥3.5/5.0' with an explicit required target. Add: 'make eval in Phase 2 exits non-zero if aggregate quality score across ground truth findings is below target. Target: ≥3.5/5.0 average across all four rubric dimensions.' Make this a required gate before any Phase 2 reviewer agent prompt is promoted to production."}
{"type": "finding", "id": "success-validator-007", "title": "No acceptance criterion covers eval suite reliability (flakiness)", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "MVP Phase Map", "issue": "The document has no requirement or acceptance criterion for eval suite reliability. LLM outputs are non-deterministic — the same agent prompt, same input, and same temperature setting can produce different numbers of findings across runs. Without a stability criterion, a 'passing' eval on one run may fail on the next, making the framework unreliable as a regression gate.", "why_it_matters": "A flaky eval framework is worse than no framework — it creates false confidence on passing runs and alarm on incidental failures. This is a known problem with LLM evals and must be addressed before the framework is used to gate prompt changes or agent promotions.", "suggestion": "Add FR-RELIABILITY-1: 'Eval suite must produce stable results across repeated runs on unchanged inputs. Acceptance criterion: running make eval three times on the same agent and ground truth produces detection rates within ±10 percentage points across runs. If variance exceeds 10pp, temperature must be set to 0 or N must be increased to reduce variance.'"}
{"type": "finding", "id": "success-validator-008", "title": "parse_review_output 'successfully parses' is not a measurable criterion", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR-ARCH-3: JSONL Output Format as Explicit Requirement", "issue": "The acceptance criterion 'parse_review_output() successfully parses output from any reviewer agent' uses 'successfully' without defining what success means. Does it mean: parses without exception? Parses at least one finding? Parses all findings with all required fields present? A parser that silently drops malformed lines and returns an empty list would 'successfully parse' under the current wording.", "why_it_matters": "Silent parse failures are harder to debug than loud ones. If the parser drops malformed JSONL lines without error, the scorer sees zero findings, which triggers the 'output format failure' guard — but the engineer has no information about what was malformed or how many lines were dropped.", "suggestion": "Replace 'successfully parses' with: 'parse_review_output() returns at least 1 Finding object per agent output, logs a warning for each line that fails JSON parsing (with line number and raw content), and raises ParseError if 0 valid findings are returned from an output with >0 non-empty lines.'"}
{"type": "finding", "id": "success-validator-009", "title": "No success criterion for the ground truth dataset itself", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Open Questions", "issue": "Open Question 3 asks whether 10 Critical findings are sufficient for Phase 1. The document does not specify any acceptance criterion for ground truth dataset adequacy — minimum size, coverage across reviewer types, or confidence level. The dataset is described as 'established' (Phase 0 complete) but there is no criterion for what makes it adequate for the eval baseline it is supposed to support.", "why_it_matters": "A ground truth dataset that is too small produces high variance in detection rate — a single missed finding swings recall by 10%. If Important findings are excluded but they represent 70% of real findings, the eval is measuring a narrow slice of reviewer performance and will not detect regressions in Important finding detection.", "suggestion": "Add a ground truth adequacy criterion to FR-ARCH-4 or Phase 0: 'Ground truth dataset is adequate for Phase 1 if: (a) ≥8 Critical findings validated by human review, (b) at least 1 finding per reviewer agent in ground truth, (c) findings span ≥3 distinct sections of the reviewed document. Phase 1 may proceed with Critical-only; add Important findings before Phase 2.'"}
