{"type": "finding", "id": "assumption-hunter-001", "title": "Assumes agent files are already eval-compatible without modification", "severity": "Critical", "phase": {"primary": "design", "contributing": null}, "section": "FR-ARCH-2: Eval-Compatible Agent Interface", "issue": "FR-ARCH-2 requires agent system prompts to be self-contained and not assume tool availability, yet the acceptance criteria state 'without modifications to the agent file.' This assumes all five existing reviewer agents already contain language like 'The design document content will be provided to you in this message.' If any agent currently instructs the model to use a Read tool or reference a file path, FR-ARCH-2's acceptance criterion is already violated before Phase 1 begins. There is no audit step in the requirements to confirm current agent file compliance.", "why_it_matters": "If even one agent file requires a modification to become eval-compatible, the acceptance criterion as written is unmet, but the fix is also untracked as work. Phase 1 cannot validate that the no-modification constraint holds until all agents are tested — by which point any required modification is already a surprise scope item.", "suggestion": "Add an acceptance criterion: 'All 5 reviewer agent files audited against eval-compatibility checklist before Phase 1 implementation begins. Findings logged. Agent files may be updated to be eval-compatible — this is permitted work; the constraint is that no eval-specific branching logic is added.' Remove 'without modifications to the agent file' as an absolute constraint — replace with 'without eval-specific conditionals in the agent file.'"}
{"type": "finding", "id": "assumption-hunter-002", "title": "Assumes document content in Sample.input is sufficient for all reviewer types", "severity": "Critical", "phase": {"primary": "design", "contributing": null}, "section": "FR-ARCH-2: Eval-Compatible Agent Interface", "issue": "FR-ARCH-2 specifies that document content is delivered via `Sample.input` and that agents need no tool calls to produce findings. This works for reviewers that analyze a single document, but some reviewer agents (e.g., prior-art-scout, feasibility-skeptic) may rely on external context — prior art references, comparable implementations, or multi-document cross-checks — that cannot be embedded in a single `Sample.input` string. The requirements do not address multi-document or multi-context reviewer patterns.", "why_it_matters": "A reviewer agent whose persona requires external lookup cannot be tested in single-turn eval context by design. If the prior-art-scout is one of the five agents, its eval task will structurally underperform not because the prompt is wrong, but because the context it needs is unavailable. This produces a misleading detection baseline — the agent looks weak in eval but performs correctly in production.", "suggestion": "Add an explicit statement of which reviewer agents are in scope for Phase 1 eval, and confirm each agent's information needs are satisfiable from a single document input. Agents that require external context should be flagged as Phase 3+ (agent bridge) eval candidates only, and excluded from Phase 1 per-reviewer tasks."}
{"type": "finding", "id": "assumption-hunter-003", "title": "Assumes JSONL output from reviewer agents is parseable without format negotiation", "severity": "Critical", "phase": {"primary": "design", "contributing": null}, "section": "FR-ARCH-3: JSONL Output Format as Explicit Requirement", "issue": "FR-ARCH-3 requires all reviewer agents to output valid JSONL and states 'parse_review_output() successfully parses output from any reviewer agent.' This assumes the agents produce clean, line-delimited JSONL with no preamble, postamble, or markdown code fencing (e.g., ```json ... ```). In practice, LLMs frequently wrap structured output in markdown fences, add introductory sentences, or produce trailing commas that break JSON parsers. The requirements do not specify whether the parser should be strict (fail on fencing) or lenient (strip fencing before parsing).", "why_it_matters": "If `parse_review_output()` is strict and agents produce fenced output, the parser returns zero findings for a valid-but-fenced response. This is indistinguishable from the agent finding nothing, and FR-ARCH-3's zero-findings-as-failure criterion would incorrectly flag a parser incompatibility as an agent quality failure. The root cause would be invisible in eval logs.", "suggestion": "Specify the parser's fence-tolerance behavior as a requirement: 'parse_review_output() strips markdown code fences (```json, ```) before parsing. Parsing attempts line-by-line after stripping. Parser logs raw output on parse failure for diagnosis.' Add an acceptance criterion: 'Parser tested against fenced, unfenced, and mixed outputs in unit tests.'"}
{"type": "finding", "id": "assumption-hunter-004", "title": "Assumes 10 Critical findings provide statistically meaningful detection baseline", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "MVP Phase Map (updated from v1)", "issue": "The phase map lists Phase 0 as complete with a 10-finding ground truth dataset, and Phase 1 targets a 'detection baseline' from this dataset. Statistically, a 10-sample ground truth distributed across 5 reviewer agents averages 2 findings per agent. A per-reviewer task that detects 1/2 of its assigned findings scores 50% recall; detecting 2/2 scores 100%. This binary sensitivity means a single missed finding swings recall by 50 percentage points. The 90% recall target from v1 requirements cannot be meaningfully distinguished from 100% recall with only 2 samples per agent.", "why_it_matters": "A detection baseline from a 2-sample per-agent ground truth is too noisy to drive prompt tuning decisions. An agent prompt change that moves from 1/2 to 2/2 looks like a 50% improvement but may reflect a single prompt word change whose effect is not generalizable. Phase 1 decisions about which agents need work will be based on unreliable signal.", "suggestion": "Either (a) explicitly acknowledge the statistical limitation and state Phase 1 establishes a directional baseline only (not a decision-quality signal), requiring Important findings to be added before prompt tuning decisions are made; or (b) add Important findings before Phase 1 completes to achieve at least 5 findings per reviewer agent as a minimum meaningful sample. The requirements should state which path is taken."}
{"type": "finding", "id": "assumption-hunter-005", "title": "Assumes Inspect AI batch API can be trivially enabled after Phase 1 validates", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "FR-ARCH-5: Cost Budget Per Eval Suite Run", "issue": "FR-ARCH-5 lists batch API as 'no code changes' required. This assumption is incorrect for Anthropic's batch API: the Message Batches API has a different response model (async, poll-based, 24-hour window) compared to the synchronous Messages API. Inspect AI's Anthropic provider may or may not abstract this difference. If Inspect AI requires explicit batch mode configuration or a different solver pattern, enabling batch API is not zero-code — it requires testing that eval tasks behave correctly under async batch semantics (e.g., timeouts, partial batch failures).", "why_it_matters": "If batch API activation is non-trivial, the cost budget established in FR-ARCH-5 may not be achievable without a Phase 1.5 batch integration spike. The 50% cost reduction assumption underpins the multi-model comparison budget in Phase 1.5. If batch is harder than expected, multi-model evals may be 2x the budgeted cost.", "suggestion": "Add a Phase 1.5 prerequisite: 'Validate that Inspect AI's Anthropic provider supports batch API via CLI flag with no code changes. If code changes required, file a separate task and adjust Phase 1.5 cost estimates accordingly.' Do not describe batch API as 'no code changes' until this is confirmed."}
{"type": "finding", "id": "assumption-hunter-006", "title": "Assumes reviewer agent output schema is stable across production and eval contexts", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "FR-ARCH-3: JSONL Output Format as Explicit Requirement", "issue": "FR-ARCH-3 specifies a required JSONL schema with fields including `phase.contributing: null`. This schema is defined in the requirements document itself, not in a shared schema file consumed by both production agents and the eval scorer. The schema exists in at least three places: the requirements doc, the agent system prompts, and `evals/utils/output_parser.py`. Any schema evolution (e.g., adding a `confidence` field or changing `severity` enum values) requires synchronized updates across all three locations. There is no single source of truth.", "why_it_matters": "Schema drift between agent prompts and the parser is the most likely source of silent eval failures. If an agent adds a field the parser doesn't expect, the parser still succeeds. If the parser adds a required field an agent doesn't emit, detection scores drop with no clear error. The v1 implementation already suffered from agent/parser mismatch — this requirement addresses the symptom (JSONL format) but not the root cause (no schema ownership).", "suggestion": "Define the canonical JSONL schema in a single location (e.g., `evals/schema.py` or `docs/schemas/finding-schema.json`) and require both agent prompts and the parser to reference it. Add an acceptance criterion: 'Schema changes require updating schema.py and regenerating any agent prompt that embeds the schema.'"}
{"type": "finding", "id": "assumption-hunter-007", "title": "Assumes ground truth findings map 1:1 to individual reviewer agent identities", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "FR-ARCH-1: Per-Reviewer Eval Task Decomposition", "issue": "FR-ARCH-1 requires each per-reviewer task to 'filter ground truth to findings where reviewer == agent_name.' This assumes that when ground truth was established, each finding was tagged with a single authoritative reviewer identity. However, the Background section states the v1 ground truth was validated from a prior review run where findings were produced by the full orchestration skill (not individual agents). If ground truth findings were not tagged with per-reviewer identity at validation time, the `reviewer` field may be missing, inferred, or ambiguous — making FR-ARCH-1's filtering criterion unimplementable without retroactively assigning reviewer identities.", "why_it_matters": "If ground truth findings lack reliable `reviewer` fields, per-reviewer ground truth filtering produces empty or incorrect sets for each agent. An assumption-hunter task with an empty ground truth set produces 0/0 recall — technically undefined, practically silent failure. The eval framework appears to run but produces no signal.", "suggestion": "Add a prerequisite acceptance criterion: 'All 10 ground truth findings in the requirements-light dataset have a `reviewer` field assigned and validated before FR-ARCH-1 implementation begins. Assignment method documented (human-assigned, or agent-name inferred from finding ID prefix).'"}
{"type": "finding", "id": "assumption-hunter-008", "title": "Assumes LLM-as-judge rubric can be defined before any Phase 2 output exists", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR-QUALITY-1: Quality Rubric Definition (Phase 2 Prerequisite)", "issue": "FR-QUALITY-1 requires the LLM-as-judge rubric to be defined with 1-example and 5-example anchors for each dimension before Phase 2 implementation begins. The rubric examples are drawn from existing ground truth findings (e.g., v1-success-validator-001 cited). This assumes the existing ground truth findings provide sufficient coverage of the quality spectrum — specifically, that low-quality (1/5) examples exist in ground truth. Ground truth by definition contains only validated, accepted findings. Low-quality examples may not exist in the ground truth dataset.", "why_it_matters": "A rubric anchored only on high-quality examples (ground truth) cannot reliably calibrate the LLM judge to distinguish 'good finding' from 'mediocre finding.' The judge learns what 5/5 looks like but has no contrast examples for 1/5 or 3/5. This produces an LLM judge that scores most findings at 4-5/5 regardless of actual quality — the classic LLM-as-judge leniency bias.", "suggestion": "Add a rubric construction step: 'Generate synthetic low-quality versions of 2 ground truth findings per dimension to serve as 1/5 anchors. These are intentionally degraded examples, not real findings.' Alternatively, draw 1/5 anchors from the known quality failure test cases logged in Session 18 (6 quality failure patterns documented). Either approach provides genuine low-quality contrast without requiring Phase 2 output to exist first."}
{"type": "finding", "id": "assumption-hunter-009", "title": "Assumes make cost-report can compute cost from Inspect AI EvalLog without API pricing data", "severity": "Minor", "phase": {"primary": "design", "contributing": null}, "section": "FR-ARCH-5: Cost Budget Per Eval Suite Run", "issue": "FR-ARCH-5 requires `make cost-report` to report 'estimated cost per run' from eval logs. Inspect AI EvalLog records token counts but not dollar costs — cost requires multiplying token counts by per-model pricing from a pricing table. Model pricing changes over time (Claude pricing has changed multiple times). The requirement does not specify where pricing data comes from, how it is updated, or what happens when the model ID in the log does not match any entry in the pricing table.", "why_it_matters": "A cost report that hard-codes pricing will silently produce wrong numbers when Anthropic adjusts prices. A report that fails on unknown model IDs will break silently when a new model variant is used in eval (e.g., claude-sonnet-4-5 vs claude-sonnet-4-5-20250929). Either failure mode undermines the cost visibility the requirement is trying to provide.", "suggestion": "Specify the pricing source: 'Pricing table maintained in `evals/pricing.py` as a dict of model_id -> (input_cost_per_mtok, output_cost_per_mtok). `make cost-report` warns if model_id not found in table. Table updated manually when Anthropic publishes pricing changes.' Accept that cost estimates are approximate and document the approximation in the report output."}
