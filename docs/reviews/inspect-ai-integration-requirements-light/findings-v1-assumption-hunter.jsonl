{"type": "finding", "id": "v1-assumption-hunter-001", "title": "Ground truth validity assumed without verification", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "Problem Statement / FR0", "issue": "The entire eval framework assumes v3 Critical findings are correct design flaws. They were produced by the skill under test with no human expert validation. If the v3 findings contain false positives or miss real flaws, the framework optimizes against the wrong targets. FR0 names this problem but treats it as an administrative prerequisite rather than a blocking dependency — Phase 1 implementation proceeds in parallel with Phase 0 validation, allowing framework construction to proceed against unvalidated ground truth.", "why_it_matters": "A 90% detection rate against false-positive-contaminated ground truth is meaningless or worse — it certifies a broken skill as passing. Every downstream decision (baseline, regression threshold, ablation pass/fail) inherits this corruption. Blast radius: entire eval framework produces misleading results.", "suggestion": "Make Phase 0 completion a hard gate before any Phase 1 code is merged. Add an acceptance criterion to FR0: 'eval framework CI must fail if datasets/v3_review_validated/critical_findings.jsonl contains zero validated findings.' Enforce the dependency in the Makefile: `make eval` must exit non-zero if the dataset is empty."}
{"type": "finding", "id": "v1-assumption-hunter-002", "title": "Single validator is assumed sufficient for ground truth quality", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR0", "issue": "FR0 calls for 'human expert review' but defines the validator as a single person ('reviewer ID'). The inter-rater agreement criterion (Cohen's kappa ≥0.6) is gated behind 'if multiple reviewers' — making the multiple-reviewer path optional. With a single validator, there is no mechanism to detect reviewer bias, reviewer fatigue, or the validator inadvertently confirming their own prior design decisions as real flaws.", "why_it_matters": "Single-reviewer ground truth on self-produced artifacts (the validator also wrote the requirements) is the same circular validation problem as using unvalidated LLM output. If the validator's bias inflates or deflates the confirmed flaw count, the 15-finding target may be met with low-quality data.", "suggestion": "Require a second reviewer for at least a random sample (30%) of validated findings. Inter-rater agreement measurement should be mandatory, not optional. If only one reviewer is available, add an explicit caveat that ground truth reflects single-reviewer judgment and thresholds should be conservative."}
{"type": "finding", "id": "v1-assumption-hunter-003", "title": "Model output format assumed to be parseable JSONL", "severity": "Critical", "phase": {"primary": "design", "contributing": "calibrate"}, "section": "FR2 / Success Criteria", "issue": "FR2.2 requires measuring recall and precision against ground truth, but the requirements never specify that the model's review output will be in JSONL format. The scoring logic depends on parse_review_output() successfully extracting structured findings from a raw LLM completion. The model may produce Markdown prose, mixed prose and JSON, incomplete JSON, or JSON with different field names depending on prompt phrasing. The requirements treat structured output as given.", "why_it_matters": "If parse_review_output() fails to extract findings, every eval run returns 0% recall and 0% precision — indistinguishable from a broken skill. The framework produces 'skill fails' for every run until the parser is fixed. Blast radius: all quantitative success criteria become unmeasurable.", "suggestion": "Add an explicit NFR or FR1 acceptance criterion: 'Skill prompt must instruct model to output findings in JSONL format. Eval task must validate that model output is parseable before scoring. Parser failures are logged as eval errors, not scored as zero recall.' Consider using Inspect AI's structured output support (JSON schema enforcement) rather than post-hoc parsing."}
{"type": "finding", "id": "v1-assumption-hunter-004", "title": "Fuzzy matching threshold of 80% assumed correct without empirical basis", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "FR2", "issue": "The scorer uses fuzzy title matching (≥80% text overlap) as a fallback when finding IDs don't match. This threshold is stated as a 'starting hypothesis' in the design doc but the requirements present it as an acceptance criterion. No justification is provided for why 80% is the right boundary. At 80% overlap, 'Python environment constraints missing' and 'Python environment constraints present' could match. At 80%, short titles are more vulnerable to false matches than long titles.", "why_it_matters": "A threshold that is too low produces false matches (inflated recall, precision appears to drop). A threshold that is too high produces false non-matches (deflated recall, skill appears worse than it is). Either direction corrupts the baseline and all subsequent regression comparisons.", "suggestion": "Treat fuzzy matching threshold as a configurable parameter with a documented default and required calibration step. After first eval run, manually inspect the top-5 fuzzy matches and top-5 fuzzy non-matches to validate the threshold. Add FR2.3: 'Fuzzy match threshold calibrated against at least one complete eval run before baseline is stored.'"}
{"type": "finding", "id": "v1-assumption-hunter-005", "title": "Skill prompt assumed to exist at a stable path at eval runtime", "severity": "Important", "phase": {"primary": "design", "contributing": "plan"}, "section": "FR1 / FR4", "issue": "The eval task loads skills/parallax:requirements/SKILL.md at runtime. This assumes the skill directory exists, the skill name is stable, and the directory structure is unchanged from when the eval was written. The requirements do not specify what happens if the skill is renamed, moved to a plugin, or the skills/ directory changes. The CLAUDE.md shows the planned structure has skills/ at the repo root, but this is architectural intent, not an enforced constraint.", "why_it_matters": "If the skill file moves (e.g., to a superpowers plugin, or the namespace changes), every eval silently loads stale content or crashes with FileNotFoundError. Ablation tests become invalid because they ablate old skill content while the actual skill has changed. Detection: silent corruption.", "suggestion": "Add an acceptance criterion to FR1.1: 'Eval setup step verifies skills/parallax:requirements/SKILL.md exists before running. Missing skill path fails fast with actionable error message.' Document the skill path contract in NFR5 (reproducibility) — reproducing an eval requires the skill at the specified path."}
{"type": "finding", "id": "v1-assumption-hunter-006", "title": "15-finding minimum assumed sufficient for statistical validity", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR0 / Success Criteria", "issue": "The target of ≥15 confirmed Critical findings is stated as sufficient test data, but no statistical justification is provided. With N=15, a single finding difference (14 vs 15 detected) changes recall from 93% to 100% — a 7% swing that could trigger or suppress a regression alert. The 10% regression threshold (FR6.2) is effectively meaningless at this sample size because the confidence interval for a binomial proportion at N=15 spans roughly ±25% at 95% confidence.", "why_it_matters": "False regression alerts (noise exceeds threshold) will erode trust in the eval framework. The team will override WARN/FAIL statuses because they observe them triggering on runs with no skill changes. Once overrides become habit, the regression detection mechanism is bypassed.", "suggestion": "Either (a) acknowledge in the requirements that N=15 is too small for reliable regression detection and set the regression threshold to >25% until N≥50, or (b) plan to reach N≥50 before setting a 10% regression threshold. Add a note: 'Regression thresholds are calibrated to sample size. At N=15, use 25% threshold. At N=30, use 15%. At N=50+, use 10%.'"}
{"type": "finding", "id": "v1-assumption-hunter-007", "title": "Ablation test assumes model baseline capability is near zero without skill", "severity": "Important", "phase": {"primary": "design", "contributing": "calibrate"}, "section": "FR4", "issue": "The ablation test validates that dropping skill content causes detection rate to drop >50%. This assumes the base model (Sonnet without skill content) has near-zero detection capability for the specific design flaws in the ground truth. If Sonnet can detect 40% of the flaws through general reasoning without any skill content, a 90% baseline with skill yields only a 50% absolute drop — which the test calls a pass. But if the base model gets 60% without the skill, a 90% baseline only needs to drop to 40% — barely passing the threshold, meaning the skill contributes only 30 percentage points of improvement.", "why_it_matters": "The ablation test conflates 'skill content contributes' with 'detection drops by X%'. A test that passes with a 50% drop when base model capability is 40% means the skill adds only 50 percentage points of lift — but 40% of that baseline came from the model itself, not the skill. The test validates skill presence but not skill quality.", "suggestion": "Add a zero-skill baseline measurement (empty system prompt) to FR4. Report both: 'base model detection rate' and 'skill-augmented detection rate.' Ablation passes only if detection with skill is significantly higher than base model AND dropping specific sections causes measurable drops beyond base model capability."}
{"type": "finding", "id": "v1-assumption-hunter-008", "title": "Prompt caching assumed to apply but not included in cost model", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "NFR2 / Success Criteria", "issue": "ADR-005 cites prompt caching as a 90% input cost reduction. The cost target ($2.00/run) is set without stating whether this assumes caching is active or inactive. If the cost target was calculated assuming cold cache (conservative), the true cost is 5-10x lower when caching is active. If the cost target was calculated assuming warm cache, running on a cold cache will unexpectedly exceed the target on first run.", "why_it_matters": "Cost target exceedance on first run will look like a requirements failure, triggering optimization effort for a non-problem. Conversely, if cost target is set with caching assumed, and caching doesn't apply (Inspect AI eval runner may not reuse cache across samples), the target is infeasible.", "suggestion": "State explicitly in NFR2 whether cost target assumes cold cache or warm cache. Add a note: 'First eval run uses cold cache and may cost 2-3x target. Subsequent runs with prompt caching active should meet target. Cost target is evaluated over steady-state usage, not first-run cost.'"}
{"type": "finding", "id": "v1-assumption-hunter-009", "title": "Inspect AI version pinning assumed unnecessary", "severity": "Minor", "phase": {"primary": "plan", "contributing": null}, "section": "FR1.1", "issue": "FR1.1 requires installing Inspect AI without specifying a version constraint beyond '>=0.3'. The Inspect AI project (inspect_ai on PyPI) releases frequently. The Dataset/Sample/Task/Scorer API has changed between minor versions. The design doc's code examples assume a specific API shape (e.g., Sample takes input as a dict, Dataset takes samples as a list). A new release could silently break the API.", "why_it_matters": "Without version pinning, make setup installs whatever the latest version is at setup time. Two developers running make setup a week apart could get different behavior. An API change in Inspect AI breaks all eval code without any change to the parallax codebase.", "suggestion": "Pin Inspect AI to a specific minor version (e.g., inspect-ai>=0.3,<0.4) in pyproject.toml. Record the pinned version and the API shapes it provides in the NFR5 reproducibility requirement. Update the pin intentionally when upgrading, not accidentally when running pip install."}
{"type": "finding", "id": "v1-assumption-hunter-010", "title": "Design doc stability assumed during eval cycle", "severity": "Minor", "phase": {"primary": "design", "contributing": null}, "section": "FR0 / FR6", "issue": "The design uses design_doc_hash to detect stale ground truth (metadata.json stores the SHA256 of the design doc). This assumes the design doc path is stable and that the document changes infrequently enough that hash drift is an exceptional event. In practice, the design doc is actively evolving — Session 19 notes explicitly discovered that ground truth goes stale during active development. If the design doc changes between make eval and make regression, hash drift fires a warning but the regression comparison still runs against stale ground truth.", "why_it_matters": "Hash drift detection warns but does not block. An eval that runs against stale ground truth with a hash-drift warning produces numbers that look valid. If the developer ignores the warning (common under time pressure), the baseline is updated with bad data.", "suggestion": "Change hash drift from a warning to a blocking error in make eval. Only allow eval to proceed if ground truth hash matches current design doc. Provide make eval --force to override for explicit intentional use. Document in the Makefile help text that hash drift means 'run make validate first.'"}
{"type": "finding", "id": "v1-assumption-hunter-011", "title": "Manual validation UI assumed to exist as a prerequisite", "severity": "Important", "phase": {"primary": "plan", "contributing": "calibrate"}, "section": "FR0 / FR3.3", "issue": "FR0 requires human expert validation of 22 v3 Critical findings before the eval framework can run. FR3.3 describes 'a script to extract findings from review artifacts' and a subagent validation step. The design doc references tools/validate_findings.py (Flask browser UI) as the validation mechanism. However, the implementation plan treats the validation UI as built in 'a parallel worktree' and explicitly defers it from Phase 1 scope. The requirements treat validation as a defined prerequisite workflow, but the tool enabling that workflow is out of scope.", "why_it_matters": "Phase 0 cannot complete without the validation UI or an explicit manual workflow substitute. If the UI is deferred, ground truth population blocks on an undefined manual process. The 2-4 week Phase 0 timeline assumes the validation workflow is available from day one.", "suggestion": "Either include validate_findings.py in Phase 1 scope explicitly, or document the manual validation workflow (e.g., 'open each finding in a text editor, add validation_status field manually') as the fallback. The requirements must not leave Phase 0 dependent on an out-of-scope tool with no documented alternative."}
{"type": "finding", "id": "v1-assumption-hunter-012", "title": "Single dataset sample assumed sufficient for eval task coverage", "severity": "Important", "phase": {"primary": "design", "contributing": null}, "section": "FR3 / FR1", "issue": "The dataset loader converts all validated findings into a single Inspect AI Sample (one design doc + all expected findings). This means every eval run is a single API call with a 15-22 finding ground truth list. Inspect AI's evaluation metrics (accuracy, recall) are computed across samples. With only one sample, per-sample variance is invisible — if the model gets 14/15 findings on one run and 13/15 on the next, those are two separate eval runs with no sample-level breakdown to diagnose which findings are consistently missed.", "why_it_matters": "A single-sample dataset cannot distinguish 'model consistently misses finding X' from 'model randomly misses one finding per run.' Regression detection based on a single sample measurement has high noise. The eval framework will produce a single recall number per run with no diagnostics about which findings are stable vs. flaky.", "suggestion": "Consider converting each finding into its own Sample, where the task is 'does the model identify this specific flaw given this design doc?' This gives N=15 independent measurements per run, enables per-finding statistics, and makes regression detection more precise. Alternatively, run the eval multiple times (3-5 runs) and report mean recall with variance."}
{"type": "finding", "id": "v1-assumption-hunter-013", "title": "95% confidence threshold for auto-add assumed measurable without definition", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR3.3", "issue": "FR3.3 specifies that findings with ≥95% confidence are auto-added to ground truth and those below 95% require user approval. The confidence score is used in this gate but is never defined. The requirement appears to reference the `confidence` field in the dataset schema (0.92 in the example), but this field is populated during validation — by whom, how, and using what methodology is not specified. The requirement conflates the validator's subjective confidence rating with a measurable threshold.", "why_it_matters": "A gate based on an undefined measurement is unimplementable. The subagent validation step in FR3.3 cannot make a ≥95% vs <95% decision without a definition. In practice, the gate will be bypassed or implemented as an arbitrary human judgment call that doesn't match the requirement.", "suggestion": "Replace confidence threshold with a structured classification: validators mark each finding as 'confirmed' or 'needs_review' (binary, no numeric score). All 'needs_review' findings require user approval before entering ground truth. Remove the 95% confidence threshold entirely — it implies false precision on a subjective evaluation."}
