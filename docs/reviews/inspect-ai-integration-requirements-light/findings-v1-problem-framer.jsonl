{"type": "finding", "id": "v1-problem-framer-001", "title": "Problem statement buried and incomplete — document opens with solution context", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Problem Statement", "issue": "The Problem Statement section exists but opens with 'Parallax skills are developed iteratively with no systematic way to measure effectiveness' — this describes an operational gap, not the root problem. The root problem is: design review decisions are made without knowing whether the review skill is reliable. 'No systematic way to measure' is a symptom of that root problem. The actual cost of the missing measurement is never stated: how many bad skill changes have shipped, how many false reviews have been trusted, what is the blast radius of an undetected regression.", "why_it_matters": "A symptom-level problem statement leads to a solution scoped around measurement infrastructure (Inspect AI integration) rather than around the core need (trustworthy design review outputs). If the problem is later re-examined, the entire scope may shift.", "suggestion": "Rewrite the Problem Statement to lead with the root cause and its impact: 'Parallax design review outputs are trusted without evidence they are reliable. When skill prompts change, no mechanism detects whether review quality improved, degraded, or stayed the same. The risk: a prompt regression silently reduces finding coverage, and downstream design decisions are made against incomplete reviews. Measurement infrastructure is the solution to this problem, not the problem itself.'"}
{"type": "finding", "id": "v1-problem-framer-002", "title": "Jobs-to-be-done embed the solution inside the problem description", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Jobs-to-Be-Done", "issue": "Each Job statement pairs a Pain with a Solution in the same breath: 'Pain: no empirical way to know... Solution: Eval framework that measures...' This conflates problem framing with solution selection. Job 1 says the pain is 'no empirical way to know if skills catch design flaws' but the solution immediately assumes Inspect AI is the answer. The problem framing work — establishing that measurement is the right intervention, and that Inspect AI is the right tool — is buried in ADR-005 and never surfaced in the requirements. A reader cannot evaluate whether the solution fits the problem without going offsite.", "why_it_matters": "When solution and problem are fused, reviewers cannot challenge the solution without appearing to challenge the problem. This suppresses alternatives and makes it harder to detect when the solution is solving the wrong job.", "suggestion": "Separate Jobs from Solutions in the Jobs-to-be-Done section. State each Job as a pure need ('I need to know whether a skill change degraded detection capability before it reaches production'), then reference the solution separately. This allows each job to be independently validated as real and unsolved before the solution is evaluated."}
{"type": "finding", "id": "v1-problem-framer-003", "title": "Jobs 4 and 5 solve hypothetical problems with no evidence of current pain", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Jobs-to-Be-Done", "issue": "Job 4 ('Optimize cost without sacrificing quality') and Job 5 ('Validate multi-model portability') both describe future-state problems, not current pain. The document cites no evidence that cost is a current blocker — in fact the NFR cost target is $0.50–$2.00 per run, well within the stated $2000/month budget. Multi-model portability (Job 5) is explicitly deferred to post-MVP (FR8.2). These are not 'validated needs from the parallax development workflow' as the introduction claims — they are anticipated needs that have not yet caused pain.", "why_it_matters": "Including unvalidated jobs inflates scope, draws requirements to post-MVP features (FR8, FR9), and weakens the coherence of the MVP. If Jobs 4–5 are post-MVP, their jobs-to-be-done belong in a post-MVP section, not the primary problem framing.", "suggestion": "Remove Jobs 4 and 5 from the MVP problem framing, or explicitly label them as 'anticipated post-MVP needs.' The current section header 'five validated needs from the parallax development workflow' is inaccurate — Jobs 4–5 are speculative. Limit the MVP problem statement to the three jobs that the MVP scope actually addresses."}
{"type": "finding", "id": "v1-problem-framer-004", "title": "Core challenge frames circular dependency as a design constraint rather than the actual problem", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "Problem Statement", "issue": "The 'Core challenge' paragraph identifies the circular validation dependency: 'We need verified design flaws to test detection capability, but we need detection capability to find design flaws.' This is correctly identified, but then immediately dismissed with 'Initial approach: Manual validation of v3 Critical findings.' This buries the most important problem framing issue in the document. The circular dependency is not a footnote — it is the reason the eval framework cannot be built as described. The MVP (Phase 1) builds an eval framework against unvalidated v3 findings, which means it does not break the circularity; it accepts it.", "why_it_matters": "If Phase 1 proceeds before ground truth is validated, the entire eval framework measures 'consistency with prior LLM outputs' not 'accuracy of finding detection.' Regressions and improvements will be measured against a potentially flawed baseline. All downstream decisions based on eval results will be compromised.", "suggestion": "Elevate the circular dependency to a primary problem statement, not a subordinate challenge. Frame the document around two sequential problems: (1) 'We have no validated ground truth — we cannot trust v3 findings as a measurement baseline'; (2) 'Once ground truth exists, we have no systematic way to measure skill effectiveness against it.' Requirements should address problem (1) before problem (2). FR0 already captures this but the problem statement needs to make this sequence explicit and mandatory, not a background note."}
{"type": "finding", "id": "v1-problem-framer-005", "title": "Problem scope conflates two distinct problems: ground truth creation and skill measurement", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "Problem Statement", "issue": "The document treats 'establish ground truth' and 'build eval framework' as a single problem with a single solution (Inspect AI integration). These are two separate problems with different methods, timelines, and failure modes. Ground truth creation is a human judgment problem — it requires expert review, classification criteria, inter-rater agreement, and a defensible validation process. Eval framework construction is an engineering problem — it requires Inspect AI integration, scorer design, and dataset format specification. Fusing them obscures the prerequisite relationship: the engineering problem cannot be started until the judgment problem is resolved.", "why_it_matters": "The MVP Scope Summary lists Phase 0 as prerequisite but Phase 1 has a '1 week' timeline starting in parallel. If the team conflates both problems, implementation will begin before ground truth is ready, producing a framework with no valid baseline. The 2–4 week Phase 0 estimate is buried in the MVP Summary, not in the problem statement where it would gate scope commitments.", "suggestion": "Split the document into two problem statements with an explicit gate. Problem 1: 'We have no validated ground truth for parallax design review quality.' Scope: human validation process, acceptance criteria for ground truth. Problem 2: 'Given validated ground truth, we have no systematic way to measure skill effectiveness.' Scope: Inspect AI integration. Require Problem 1 resolution before Problem 2 begins. The current FR0 is underweighted relative to its importance."}
{"type": "finding", "id": "v1-problem-framer-006", "title": "Ground truth validation circular dependency — eval measures consistency not accuracy", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "Problem Statement", "issue": "v3 findings are unvalidated LLM outputs, not confirmed design flaws. Building an eval framework that measures detection rate against v3 findings will produce a system that measures 'do new skills produce similar outputs to old skills' — not 'do skills catch real design flaws.' A prompt change that improves accuracy but changes output format would register as a regression. A prompt change that generates different-but-equally-wrong findings would pass. The eval framework is measuring consistency, not quality.", "why_it_matters": "Any eval results produced before ground truth validation are misleading. Baseline metrics, regression thresholds, and ablation test results will all be defined relative to an unvalidated reference. If v3 has a 30% false positive rate, the eval framework will optimize to reproduce that false positive rate.", "suggestion": "Add to the Problem Statement: 'The eval framework can only measure skill quality if the ground truth it measures against is itself validated. Using unvalidated v3 findings as ground truth produces a consistency metric, not an accuracy metric. Ground truth validation (FR0) is a prerequisite that gates all other requirements in this document, not an optional phase.'"}
{"type": "finding", "id": "v1-problem-framer-007", "title": "User outcome is a latency target, not a problem outcome", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Jobs-to-Be-Done", "issue": "The Jobs section closes with: 'User outcome: Run evals locally to validate skill changes in <5 minutes, with clear pass/fail signals and regression detection.' This is an implementation constraint (latency, interface), not a user outcome. A user outcome would be: 'Skill changes are deployed with confidence that detection capability has not regressed.' The <5 minute target belongs in NFR5 (Development Velocity), not in the problem framing.", "why_it_matters": "Conflating implementation constraints with outcomes means the success criterion for the problem is 'evals run fast' rather than 'skills are reliably validated.' The <5 minute target could be met while the core problem remains unsolved.", "suggestion": "Replace the User Outcome statement with a problem-level outcome: 'Skill changes are validated against confirmed design flaws before deployment. Regressions are detected with statistical confidence. The team has empirical evidence of skill quality, not subjective impressions.' Move the <5 minute target to NFR5."}
{"type": "finding", "id": "v1-problem-framer-008", "title": "MVP solves integration, not validation — Phase 1 success does not answer the core question", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "MVP Scope Summary", "issue": "Phase 1 success criteria are: Inspect AI installed, severity calibration scorer runs, baseline detection rate established, ablation test passes, eval runs in <5 minutes. None of these criteria answer the actual question: 'Do parallax skills reliably catch design flaws?' Phase 1 validates that Inspect AI works, not that skills work. The finding quality scorer — which is required to assess whether detected findings are actually valid — is deferred to Phase 2. An MVP that cannot answer its own core question is not an MVP; it is a technical foundation.", "why_it_matters": "The team will complete Phase 1 and still not know if skills catch real design flaws. Success Criteria #4 ('baseline detection rate ≥90% on validated ground truth') is the only criterion that addresses quality — but it depends on ground truth existing, and the threshold is labeled 'provisional.' If the baseline is set against unvalidated data, the ≥90% target is meaningless.", "suggestion": "Reframe the MVP goal: 'Phase 1 proves the measurement instrument is valid (ground truth exists, eval framework can run against it). Phase 2 proves the skill under test is effective (detection rate meets threshold, ablation validates content contribution).' This reframing ensures the MVP does not declare success before the core question is answerable."}
{"type": "finding", "id": "v1-problem-framer-009", "title": "Problem statement does not define what 'broken skill' means", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Problem Statement", "issue": "The Problem Statement says the lack of measurement 'risks shipping broken skills to production.' But 'broken' is undefined. A skill could be broken in multiple ways: (a) detects fewer Critical findings (coverage regression), (b) detects the same findings but misclassifies severity, (c) generates more false positives, (d) works on the training document but fails to generalize. The requirements address (a) via detection rate and (c) via precision, but (b) and (d) are not addressed. Without defining what broken means, the eval framework cannot be designed to catch all breakage modes.", "why_it_matters": "The eval framework will be designed around the failure modes that are specified. Unspecified failure modes will not be caught. If severity miscalibration is a known failure mode (v3 had 'too many Important findings'), it should be explicitly named in the problem statement so FR2.1 (severity calibration scorer) is clearly solving a named problem, not just an implementation detail.", "suggestion": "Add to the Problem Statement: 'A skill is broken when it: (a) misses Critical design flaws below the detection rate threshold, (b) miscalibrates severity such that Important findings crowd out Critical ones, or (c) produces findings that do not correspond to real design issues (false positives). The eval framework must detect all three failure modes.' This scopes FR2.1, FR2.2, and FR5 to named breakage categories."}
{"type": "finding", "id": "v1-problem-framer-010", "title": "Why this problem matters now is not stated — urgency and stakes are missing", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Problem Statement", "issue": "The Problem Statement does not explain why solving this now matters. The parallax skill is under active development (Session 18 shows 79 findings from a recent review run, Session 20 shows 29 tests written). But the document does not state: how many skill changes have been made without measurement, what is the current false positive rate estimate, or what event triggered this requirement (was there a detected regression? A near-miss?). Without context, the problem reads as theoretical rather than urgent.", "why_it_matters": "Teams prioritize problems they understand are urgent. Without stated stakes, the 2–4 week Phase 0 blocker will be rationalized away in favor of starting Phase 1 implementation. The urgency framing also affects threshold setting: if stakeholders don't feel the pain, provisional thresholds will never be tuned.", "suggestion": "Add one sentence to the Problem Statement: 'This requirement is triggered by the v3 review cycle — 87 findings from an unvalidated skill, 22 marked Critical, with no mechanism to confirm they are real or to detect if a future prompt change degrades coverage. The next skill iteration ships without measurement unless this framework exists first.'"}
{"type": "finding", "id": "v1-problem-framer-011", "title": "FR0 acceptance criteria define process but not done-ness — when is ground truth good enough?", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR0: Ground Truth Validation (Prerequisite)", "issue": "FR0 requires human expert validation of v3 Critical findings and targets ≥15 confirmed real flaws. But the acceptance criteria do not define when ground truth is 'good enough' to proceed. Specifically: (a) the Cohen's kappa ≥0.6 inter-rater threshold is gated on 'if multiple reviewers' — making single-reviewer validation an escape hatch with no quality floor; (b) 68% validation rate (15/22) is presented as a target, not a minimum — what happens if only 12 findings are confirmed?; (c) there is no criterion for what to do if ground truth quality is insufficient (pivot? expand dataset? halt?).", "why_it_matters": "Without a clear done-ness criterion, FR0 will be declared complete when the minimum work is done, not when ground truth is actually trustworthy. A single reviewer marking 15 findings as 'real' with no documented criteria is technically compliant but produces unreliable ground truth.", "suggestion": "Add explicit done-ness criteria to FR0: 'Ground truth is ready to proceed when: (1) ≥15 findings confirmed by reviewer with documented classification criteria; (2) if single reviewer, confidence scores documented for each finding (no averaging below 0.8); (3) false positive rate documented (findings marked false positive / total reviewed); (4) if false positive rate >30%, halt and escalate — the v3 skill may be insufficiently reliable to use as ground truth source.' Make the 'if multiple reviewers' kappa clause mandatory for any production ground truth dataset."}
{"type": "finding", "id": "v1-problem-framer-999", "title": "Blind spot check: Problem Framer perspective", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Meta", "issue": "This review focused on problem framing gaps: root cause vs symptom, scope conflation, solution-embedded problem statements, and missing urgency framing. It may have underweighted: (1) the quality of the existing FR0 section — it is well-structured and does address the circular dependency, which means some of the framing issues identified here are present in the intro but corrected deeper in the document; (2) the possibility that the audience (a small team with full context) does not need explicit urgency framing because it is shared knowledge; (3) technical feasibility constraints that affect whether problem decomposition is practical given the tools available.", "why_it_matters": "Blind spot awareness helps catch gaps in the review process.", "suggestion": "Consider: The document's problem framing weaknesses are mostly in the introductory sections — FR0 through FR3 show evidence of careful thinking about the root cause. A targeted rewrite of the Problem Statement and Jobs-to-be-Done sections (to reflect the sophistication already present in FR0) may be sufficient to address the framing gaps without restructuring the entire document. Also verify: does the summary.md already capture these findings from prior reviewers? Cross-check to avoid duplicate findings in the synthesis step."}
