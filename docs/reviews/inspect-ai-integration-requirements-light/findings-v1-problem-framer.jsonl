{"type":"finding","id":"v1-problem-framer-001","title":"Problem statement missing - root cause not explicitly stated","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Jobs-to-Be-Done","issue":"The document jumps directly to five specific jobs without stating the underlying problem being solved. The root cause (\"no way to validate that parallax skills actually work\") is implicit across the jobs but never declared as the primary problem statement.","why_it_matters":"Without an explicit problem statement, readers cannot evaluate whether the proposed solution addresses the right issue. The jobs appear solution-oriented (\"integrate Inspect AI\") rather than problem-oriented (\"skills might be broken and we wouldn't know\").","suggestion":"Add a \"Problem Statement\" section before Jobs-to-Be-Done that states: \"Parallax skills have no empirical validation mechanism. Changes to skill prompts, persona descriptions, or synthesis logic could silently break detection capability, and we would only discover this through manual inspection or production failures. This creates unacceptable risk for a tool intended to catch design flaws systematically.\""}
{"type":"finding","id":"v1-problem-framer-002","title":"Root cause conflated with solution - Jobs frame Inspect AI as the answer","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Jobs-to-Be-Done","issue":"Jobs 1-5 present Inspect AI integration as the solution within the problem statement itself. Example: Job 1 says \"Solution: Eval framework that measures detection rate\" rather than stating the problem (\"no detection rate measurement exists\"). This conflates problem and solution, making it impossible to evaluate alternative approaches.","why_it_matters":"If the jobs are framed around a pre-selected solution, alternative validation approaches (manual A/B testing, adversarial human review, comparative LLM evaluation) cannot be considered. The decision to use Inspect AI (made in ADR-005) should be separate from the problem definition.","suggestion":"Reframe jobs to separate problem from solution. Job 1 problem: \"No empirical way to know if skills catch design flaws or if prompt changes break detection.\" Job 1 solution: \"(Addressed by FR1-FR3: Inspect AI integration with custom scorers and test datasets).\""}
{"type":"finding","id":"v1-problem-framer-003","title":"User outcome specifies implementation detail (<5 minutes) before validating necessity","severity":"Minor","phase":{"primary":"calibrate","contributing":null},"section":"Jobs-to-Be-Done","issue":"The user outcome states \"Run evals locally to validate skill changes in <5 minutes\" but doesn't establish why local execution or the 5-minute threshold is essential to the problem. This feels like a solution constraint (local = no API latency, 5 min = interactive feedback loop) rather than a validated user need.","why_it_matters":"If the core problem is \"validate skills work,\" local execution and 5-minute runtime are implementation details that should be justified, not assumed. A slower but more comprehensive eval might be acceptable if it catches more issues.","suggestion":"Move the <5 minute constraint to NFR4 (already present as NFR4.1) and reframe the user outcome as: \"Know whether skill changes improve or degrade detection capability, with fast enough feedback to enable iterative prompt development.\" Then justify the 5-minute target in NFR4 rationale."}
{"type":"finding","id":"v1-problem-framer-004","title":"Job 4 solves a cost problem that may not exist yet","severity":"Important","phase":{"primary":"calibrate","contributing":"survey"},"section":"Jobs-to-Be-Done (Job 4)","issue":"Job 4 states \"Pain: Running evals on every model/configuration is expensive\" but there's no evidence this pain point exists. The project hasn't run evals yet, so the cost impact is theoretical. This appears to be premature optimization based on ADR-005 analysis rather than observed pain.","why_it_matters":"Solving theoretical problems leads to over-engineering. If cost isn't actually a blocker (e.g., v3 eval costs $0.20, well under budget), building cost optimization infrastructure (model tiering, batch API) wastes development time that could validate the core eval pattern first.","suggestion":"Downgrade Job 4 from \"validated need\" to \"anticipated constraint.\" Reframe as: \"Future concern: At scale, running evals on every model/config could exceed budget. Deferred to post-MVP pending actual cost data from initial eval runs (target: <$0.50 per run per NFR1.2).\""}
{"type":"finding","id":"v1-problem-framer-005","title":"Job 5 multi-model portability assumes cross-model deployment without justification","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Jobs-to-Be-Done (Job 5)","issue":"Job 5 states \"Pain: Skills developed on Sonnet might fail on Haiku or Opus\" but doesn't explain why cross-model portability is necessary. The project uses opusplan mode (Opus for planning, Sonnet for execution) with Sonnet as the review baseline. If skills only need to work on Sonnet, this entire job is solving a non-problem.","why_it_matters":"Multi-model validation adds significant complexity (FR8, FR9) and cost. If the actual requirement is \"skills work reliably on Sonnet\" (the baseline reviewer model), testing Haiku/Opus portability is gold-plating. Need clear justification for why portability matters.","suggestion":"Add explicit rationale: \"Why cross-model portability matters: [Option A: Future cost optimization requires Haiku-compatible skills] OR [Option B: Quality baseline requires Opus validation] OR [Option C: Multi-model review teams need all tiers].\" If none apply, remove Job 5 and defer FR8/FR9 entirely."}
{"type":"finding","id":"v1-problem-framer-006","title":"Ground truth validation problem buried in FR3.3, not surfaced as core challenge","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"FR3: Test Datasets","issue":"FR3.3 introduces a manual validation gate for ground truth creation (<95% confidence findings require user approval) but this represents a fundamental problem: we don't have verified ground truth yet. The v3 review findings are unvalidated LLM outputs, not confirmed design flaws. This chicken-and-egg problem (need ground truth to validate evals, need evals to validate ground truth) is the actual blocker, not Inspect AI integration.","why_it_matters":"Without verified ground truth, the entire eval framework measures \"do new skills produce similar outputs to old skills\" rather than \"do skills catch real design flaws.\" This is a false validation—if v3 findings include false positives, the eval framework will learn to reproduce those false positives. The problem statement should acknowledge this foundational issue.","suggestion":"Add to problem statement: \"Core challenge: Ground truth validation is circular. We need verified design flaws to test detection capability, but we need detection capability to find design flaws. Initial approach: Manual validation of v3 Critical findings to establish high-confidence ground truth baseline, then expand incrementally with manual review gates.\""}
{"type":"finding","id":"v1-problem-framer-007","title":"Ablation test framing inverted - tests skill content, not skill effectiveness","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"FR4: Ablation Tests","issue":"FR4.1 states ablation tests validate \"skill content actually matters (not just model inference).\" This frames the problem as \"does the skill file do anything\" rather than \"does the skill catch design flaws that would otherwise be missed.\" The ablation test (drop content → detection drops >50%) proves content affects output, not that output is valuable.","why_it_matters":"An ablation test could pass (detection rate drops when content removed) even if the skill produces garbage findings. The test validates correlation (content → output) but not causation (content → better design). The real problem is \"do skills improve design quality\" not \"do skills affect LLM behavior.\"","suggestion":"Reframe FR4 as: \"Ablation tests validate that skill components contribute to finding quality, not just finding quantity.\" Add acceptance criteria: \"Ablation must degrade BOTH detection rate (>50%) AND finding quality score (measured by LLM-as-judge severity calibration).\" This requires FR2 quality scorer (currently deferred to Phase 2), which reveals Phase 1 cannot actually validate skill effectiveness."}
{"type":"finding","id":"v1-problem-framer-008","title":"MVP scope solves measurement problem, not validation problem","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"MVP Scope Summary (Phase 1)","issue":"Phase 1 success criteria (line 386-391) focuses on \"eval runs locally, produces metrics, costs <$0.50\" but doesn't address the core problem: \"do skills catch design flaws.\" The MVP validates Inspect AI integration works, not that parallax skills work. This is building measurement infrastructure before confirming there's something worth measuring.","why_it_matters":"If Phase 1 completes successfully but the baseline detection rate is 30% (terrible) or finding quality is poor (false positives), the eval framework is operational but useless. The MVP should validate \"skills catch real design flaws\" not \"evals run successfully.\" Current scope defers the actual problem (finding quality) to Phase 2 (line 370, finding_quality_scorer deferred).","suggestion":"Add to Phase 1 success criteria: \"Manual validation confirms ≥70% of detected v3 Critical findings are legitimate design flaws (not false positives).\" This forces ground truth validation before declaring MVP complete. If manual validation reveals <70% precision, the problem shifts from \"validate skills\" to \"fix skills first.\""}
{"type":"finding","id":"v1-problem-framer-009","title":"Open Question 1 exposes unvalidated assumption about ground truth confidence","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Open Questions","issue":"Open Question 1 asks \"Is 95% confidence the right threshold\" but doesn't question the underlying assumption that LLM confidence scores correlate with actual correctness. This assumes the model can accurately self-assess finding validity, which is unproven. If confidence scores are uncalibrated (common in LLMs), the threshold is meaningless.","why_it_matters":"If a 95% confidence finding is actually 60% correct, the manual validation gate becomes a bottleneck (most findings require review) or a source of false negatives (auto-accepting bad findings). The problem isn't \"what threshold\" but \"is confidence a valid signal.\" Without calibration data, the 95% threshold is arbitrary.","suggestion":"Reframe Open Question 1 as: \"Ground truth validation strategy: Should we (A) trust LLM confidence scores with manual review gate at X%, (B) manually validate ALL findings until calibration data exists, or (C) use external validation (human expert review of sample findings)?\" Then test assumption: Run v3 Critical findings through manual review, compare actual correctness to LLM confidence scores, establish whether confidence is predictive."}
{"type":"finding","id":"v1-problem-framer-010","title":"Requirements assume single-user workflow, ignore team validation use case","severity":"Minor","phase":{"primary":"calibrate","contributing":null},"section":"Jobs-to-Be-Done (all jobs)","issue":"All five jobs frame validation as a solo developer activity (\"I change a skill, I run eval, I see results\"). No mention of team scenarios: sharing eval results, collaborative ground truth creation, validating skills before pushing to shared repo. This is inconsistent with the branch protection discipline (pre-commit hooks, PR workflow) established in CLAUDE.md.","why_it_matters":"If parallax is intended for team use (implied by PR workflow, public repo goal), eval results need to be shareable and reproducible across team members. Current scope (local evals, manual validation gates) doesn't support \"reviewer validates skill change before approving PR.\" This could lead to rework when team workflow is introduced.","suggestion":"Add Job 6 or expand Job 1: \"Share eval results with reviewers to justify skill changes (supports PR review workflow).\" Add to NFR5: \"Eval results are portable (no local paths, API keys redacted) and reproducible (same dataset + skill version = same results).\""}
{"type":"finding","id":"v1-problem-framer-011","title":"Blind spot check: Problem Framer perspective","severity":"Minor","phase":{"primary":"calibrate","contributing":null},"section":"Meta","issue":"What might I have missed by focusing on problem framing? Potential blind spots: (1) Assumed Jobs-to-Be-Done section is the problem statement location—may have missed implicit problem framing in references (ADR-005, Issue #5). (2) Focused on whether problems are real vs theoretical, but didn't validate whether the problems are high-priority (maybe skill validation isn't urgent if skills work well enough). (3) Evaluated root cause identification but didn't assess problem scope boundaries (what's explicitly out of scope for this integration?).","why_it_matters":"Blind spot awareness helps catch gaps in the review process. If I missed implicit problem statements in ADR-005 or Issue #5, my findings about missing problem framing could be wrong. If the problems are low-priority, the entire integration could be premature optimization.","suggestion":"Cross-reference: Read ADR-005 and Issue #5 to validate whether the problem statement exists there and was intentionally excluded from requirements doc. Priority check: Compare this integration to other open issues (#33 synthesis patterns, #34 blind spot checks, #37 transcript analysis)—is skill validation the highest-leverage problem to solve next? Scope check: Confirm what's intentionally out-of-scope (e.g., human expert validation, production deployment scenarios)."}
