{"type": "finding", "id": "v1-scope-guardian-001", "title": "FR5.1 planted flaw detection is in Phase 1 scope but requires weeks of undocumented authoring work", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR5: Adversarial Tests (Robustness)", "issue": "FR5.1 requires a test dataset containing design docs with planted flaws (architectural gaps, assumption violations, edge case failures), each with ground truth labels. This implies authoring multiple synthetic design documents and labeling them — work that belongs to dataset preparation, not integration setup. The 1-week Phase 1 timeline in 'MVP Scope Summary' contains no authoring budget for this. FR5.1 is listed under Phase 1 but its prerequisites (design doc authoring) have no placement in any phase.", "why_it_matters": "If FR5.1 is treated as in-scope for Phase 1, it will silently extend the timeline by 2-4 weeks. If it is deferred, FR5.1 acceptance criteria become dead code and the requirement misleads implementers about what is actually required in Phase 1.", "suggestion": "Move FR5.1 to the 'Explicitly Deferred' list under 'MVP Scope Summary'. State explicitly: 'Planted flaw test datasets require synthetic design doc authoring — deferred to Phase 2 alongside finding_quality_scorer. Phase 1 uses only real v3 validated findings.' Remove FR5.1 acceptance criteria from the Phase 1 section or mark them as Phase 2 targets."}
{"type": "finding", "id": "v1-scope-guardian-002", "title": "FR7.1 and FR7.2 contradict each other: full acceptance criteria written for a deferred requirement", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR7: Version Comparison", "issue": "FR7.1 has four complete acceptance criteria (unique+stable version identifiers, filter/compare by version, side-by-side report). FR7.2 immediately marks the entire feature as deferred to post-MVP citing 'Phase 2 design research required.' The acceptance criteria in FR7.1 imply the requirement is in scope; FR7.2 says it is not. An implementer reading FR7.1 alone will build version comparison infrastructure. An implementer reading FR7.2 alone will skip it. The document supports both interpretations.", "why_it_matters": "Contradictory scope signals cause rework. An implementer starts on FR7.1 (it has acceptance criteria), then discovers FR7.2 defers it, and discards the work. Or they skip it and the acceptance criteria in FR7.1 go unmet at MVP review, causing a failed scope check.", "suggestion": "Remove acceptance criteria from FR7.1 entirely, leaving only a description of the intent and the deferred rationale. Consolidate to: 'FR7: Version Comparison — Deferred to Phase 2. Each eval run should capture git commit hash in EvalLog metadata (FR1.3) as a low-cost first step; full version comparison infrastructure deferred.' This matches what the design doc actually implements (git tag in Makefile)."}
{"type": "finding", "id": "v1-scope-guardian-003", "title": "FR8.1 and FR9.1 have full acceptance criteria but are immediately deferred by FR8.2 and FR9.2", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR8: Multi-Model Comparison / FR9: Model Role Configuration", "issue": "FR8.1 specifies three acceptance criteria (task_with() pattern, three named models, comparison report). FR9.1 specifies two acceptance criteria (model aliases, scorer model reference). FR8.2 and FR9.2 both state 'Deferred to post-MVP.' This is the same contradiction as FR7: scope-in-scope-out conflict on every multi-model requirement. The pattern appears three times (FR7, FR8, FR9), indicating a structural problem in how deferred requirements are documented.", "why_it_matters": "Writing acceptance criteria for deferred requirements wastes reviewer time validating unimplementable criteria and creates scope ambiguity at every subsequent review gate. It also inflates the apparent MVP scope when stakeholders scan the requirements.", "suggestion": "Establish a consistent pattern: deferred requirements get a one-line description and a 'Post-MVP scope' note — no acceptance criteria. Apply this to FR7.1, FR8.1, and FR9.1. Move the rationale and design questions from FR7.2/FR8.2/FR9.2 into a single 'Deferred Features' appendix section so the main requirements body only contains in-scope work."}
{"type": "finding", "id": "v1-scope-guardian-004", "title": "NFR numbering is internally inconsistent — same ID used for different requirements across NFR sections", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Non-Functional Requirements", "issue": "NFR2 (Cost Tracking) uses sub-IDs 'NFR1.1' and 'NFR1.2' (should be NFR2.1/NFR2.2). NFR3 (Model Tiering) uses 'NFR2.1', 'NFR2.2', 'NFR2.3'. NFR4 (Batch API) uses 'NFR3.1', 'NFR3.2'. NFR5 (Dev Velocity) uses 'NFR4.1', 'NFR4.2'. NFR6 (Reproducibility) uses 'NFR5.1', 'NFR5.2'. Every NFR section is numbered one behind its parent. This means NFR1.1 (API Key Security) and NFR1.1 (Cost Tracking log token usage) have identical IDs.", "why_it_matters": "The summary file and prior review findings already cite IDs from this doc (e.g., 'v1-constraint-finder-002' references NFR1.2). If the ID collision is not fixed before implementation, cross-references from review findings to requirements become ambiguous. Traceability from finding to requirement breaks.", "suggestion": "Renumber NFR sub-IDs to match their parent section: NFR2.1/NFR2.2 for Cost Tracking, NFR3.1/NFR3.2/NFR3.3 for Model Tiering, NFR4.1/NFR4.2 for Batch API, NFR5.1/NFR5.2 for Dev Velocity, NFR6.1/NFR6.2 for Reproducibility. Cross-check all existing review finding citations after renumbering."}
{"type": "finding", "id": "v1-scope-guardian-005", "title": "MVP Scope Summary phase timelines are present despite user preference to never include time estimates", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "MVP Scope Summary", "issue": "Phase 0 is labeled 'Target: 2-4 weeks' and Phase 1 is labeled 'Target: 1 week'. CLAUDE.md explicitly states 'Never include time estimates — no hour/week/day effort predictions in designs or plans (they're always wrong).' The requirements document violates this stated preference.", "why_it_matters": "The 1-week estimate for Phase 1 is already contradicted internally: FR5.1 planted flaw authoring alone would exceed 1 week. The 2-4 week estimate for ground truth validation is speculative. These estimates anchor expectations incorrectly and will be used to drive urgency or timeline decisions despite being unreliable.", "suggestion": "Remove time estimates from Phase 0 and Phase 1 headings. Replace with dependency-based ordering: 'Phase 0 (prerequisite): complete before starting Phase 1.' Let success criteria define completeness, not calendar targets."}
{"type": "finding", "id": "v1-scope-guardian-006", "title": "Validation UI (Flask tool) appears in design doc Phase 1 scope but is absent from requirements", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "MVP Scope Summary", "issue": "The design doc's 'MVP Scope (Phase 1)' section explicitly lists 'tools/validate_findings.py — Browser-based validation UI (Flask)' as in-scope for Phase 1. The requirements document makes no mention of a Flask-based validation UI, its acceptance criteria, or its scope. The implementation plan (2026-02-16-inspect-ai-integration-implementation.md) notes the validation UI is 'being built in a parallel worktree — not part of this plan.' The requirements are silent on which scope the validation UI belongs to.", "why_it_matters": "FR3.3 requires a 'manual validation gate' for ground truth creation and references a script to validate findings — but doesn't specify whether this is the Flask UI or a simpler CLI. If the Flask UI is Phase 1 scope (as the design doc says), requirements must capture it. If it's deferred to a parallel track, requirements must say so explicitly.", "suggestion": "Add one sentence to FR3.3 or MVP Scope Summary: 'Validation UI (Flask, tools/validate_findings.py) is a prerequisite for Phase 0 ground truth creation. It is tracked separately from this requirements document and must be operational before Phase 0 can complete.' This makes the dependency explicit without expanding requirements scope."}
{"type": "finding", "id": "v1-scope-guardian-007", "title": "Open Questions 1-4 are all decision-blocking but lack a resolution process or owner", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Open Questions", "issue": "Four open questions are listed: (1) ablation threshold correctness, (2) baseline update frequency, (3) multi-model grading model choice, (4) dataset size sufficiency. Questions 1, 2, and 4 directly gate acceptance criteria in FR4.1, FR6.2, and FR2.2 respectively. None of the questions has an assigned resolution method, resolution timing, or owner. They are stated as unresolved at draft v1.0 with no indication of when or how they will be resolved before implementation starts.", "why_it_matters": "Questions 1 and 4 gate Phase 1 success criteria directly. If ablation threshold is wrong or 15 findings is insufficient, Phase 1 success criteria cannot be met. Implementers proceeding without resolution will either pick arbitrary values or block waiting for answers.", "suggestion": "For each open question, add: (a) the phase in which it must be resolved (Phase 0 vs Phase 1), (b) the resolution approach ('resolve empirically in first eval run' or 'resolve via ADR before implementation'). Example: 'Q1: Ablation threshold — resolve empirically during Phase 1 first run. Set provisional 50% threshold, observe actual baseline-vs-ablation gap, calibrate after first data point.' This converts open questions into deferred decisions with explicit resolution plans."}
{"type": "finding", "id": "v1-scope-guardian-008", "title": "Scope header states 'Eval framework integration for systematic skill validation' but NFR3 (Model Tiering) is entirely post-MVP", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Non-Functional Requirements", "issue": "NFR3 (Model Tiering) contains three sub-requirements all marked 'Post-MVP scope.' The section header implies these are active requirements. A reader scanning the NFR list sees 'NFR3: Model Tiering' and assumes it is in scope for this document's deliverable. Only reading through to 'Post-MVP scope' reveals otherwise. The same issue applies to NFR4 (Batch API), where both sub-requirements are deferred.", "why_it_matters": "Post-MVP requirements in the active NFR list inflate perceived scope and add review burden. They also risk being treated as acceptance criteria during MVP validation if reviewers do not read each sub-requirement carefully.", "suggestion": "Move NFR3 (Model Tiering) and NFR4 (Batch API) to the 'Explicitly Deferred' section at the bottom of 'MVP Scope Summary'. Replace them in the NFR body with single-line placeholders: 'NFR3: Model Tiering — deferred to Phase 3, see MVP Scope Summary.' This keeps the active NFR section scoped to Phase 1 deliverables only."}
{"type": "finding", "id": "v1-scope-guardian-009", "title": "No explicit out-of-scope statement for parallax skills other than parallax:requirements", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "MVP Scope Summary", "issue": "The requirements cover eval of 'parallax:requirements' skill only. No statement explicitly excludes other skills (parallax:review, parallax:calibrate, parallax:orchestrate, parallax:eval, parallax:survey). ADR-005 confirms the broader eval vision covers all review artifacts, but this requirements doc is silent on whether other skills are in or out of scope for this integration effort.", "why_it_matters": "Without an explicit exclusion, a reasonable implementer could extend the eval framework to cover parallax:review findings (v3 review, not requirements-light). The datasets directory structure already anticipates multi-skill expansion (FR3.2 mentions requirements-light and pattern-extraction datasets). The boundary between Phase 1 (v3 only) and Phase 2 (expand) would be clearer with an explicit statement.", "suggestion": "Add one bullet to 'Explicitly Deferred': 'Eval coverage for skills other than parallax:requirements — Phase 2 expands to parallax:review, parallax:calibrate. Phase 1 validates the integration pattern on a single skill only.'"}
{"type": "finding", "id": "v1-scope-guardian-010", "title": "FR3.3 confidence threshold (95%) contradicts its own mechanism — appears 5 times undefined", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR3: Test Datasets", "issue": "FR3.3 states 'Only ≥95% confidence findings auto-added to ground truth' and '<95% confidence findings require user approval.' The 95% threshold appears in FR2.2, FR3.3, and the prior review summary flags it in 5 locations. The requirements document never defines what confidence means, how it is measured, or who/what produces the confidence score. The mechanism — 'subagent validates each finding against design doc' — is described but the output of that validation (a confidence score) is not specified.", "why_it_matters": "FR3.3 is the sole automated guard against false positives entering ground truth. If confidence is undefined, the subagent validation step cannot be implemented. The manual review gate for <95% confidence findings also cannot trigger because the threshold has no measurement method.", "suggestion": "Define confidence measurement in FR3.3: 'Confidence score is produced by the validation subagent as a 0-100 integer representing certainty that the finding identifies a real design flaw in the target design doc. Score reflects: (a) whether the finding cites specific evidence from the doc, (b) whether the issue is reproducible by a second reviewer, (c) absence of factual errors about the design. Score ≥95 = auto-accept, <95 = flag for human review.' If LLM self-assessment is too unreliable, replace with human-only workflow: remove auto-accept entirely and require manual classification for all findings."}
{"type": "finding", "id": "v1-scope-guardian-999", "title": "Blind spot check: Scope Guardian perspective", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Meta", "issue": "The scope-guardian lens focused on in/out boundaries, MVP clarity, and deferred item contradictions. This may have under-weighted: (1) implicit scope in the ground truth refresh workflow — the design doc describes a 'living ground truth' pattern (refresh after design changes) that has no corresponding requirement in the requirements doc, creating a gap between what is specified and what the design assumes will happen regularly; (2) the scope relationship between the validation UI (built in parallel) and this requirements document — the requirements treat ground truth creation as atomic but the design splits it across two artifacts (Flask UI + this eval framework); (3) whether the NFR numbered correctly (I found and reported the numbering collision, but may have missed other cross-reference issues in the full requirements v1.1 update history).", "why_it_matters": "Blind spot awareness helps catch gaps in the review process.", "suggestion": "Consider: Did I miss that FR0 (ground truth validation) and the validation UI are actually the same Phase 0 work split across two documents with no explicit dependency documented? Verify the living ground truth refresh workflow is captured as a requirement (or explicitly deferred) rather than only described in the design doc."}
