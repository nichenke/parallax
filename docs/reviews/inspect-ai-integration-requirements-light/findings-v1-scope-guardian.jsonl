{"type": "finding", "id": "v1-scope-guardian-001", "title": "MVP Phase 1 timeline undefined creates scope expansion risk", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "MVP Scope Summary - Phase 1", "issue": "Phase 1 target is '1 week' but has no start date, end date, or calendar anchoring. This creates ambiguity about what 'done' means and when MVP completion occurs.", "why_it_matters": "Without calendar boundaries, 'Phase 1' can expand indefinitely. 'Target: 1 week' suggests effort estimate, not schedule commitment. If week 1 delivers 80% of items, is Phase 1 done or extended? This ambiguity enables scope creep ('just one more tweak before calling it MVP').", "suggestion": "Replace 'Target: 1 week' with explicit definition: 'Phase 1 Complete When: All 6 success criteria met (steps 1-6 from Phase 1 list). Estimated effort: 1 week.' Separate effort estimate from completion criteria."}
{"type": "finding", "id": "v1-scope-guardian-002", "title": "Coarse-grained ablation scope undefined - which sections are dropped?", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR4.2: Coarse-grained ablation for MVP", "issue": "FR4.2 lists example sections to drop ('persona descriptions, verdict logic, synthesis instructions') but doesn't specify whether this is the complete list or examples. Acceptance criteria says 'entire sections' but doesn't enumerate which sections constitute valid ablation targets.", "why_it_matters": "Without explicit section enumeration, implementer must reverse-engineer skill structure to determine valid ablation boundaries. This creates scope ambiguity: Does 'synthesis instructions' mean the Synthesizer agent's full prompt, or just the consolidation rules? Does 'verdict logic' include severity calibration thresholds? Unclear boundaries lead to incomplete testing or scope expansion debates during implementation.", "suggestion": "Add FR4.2A: 'Ablation targets for MVP: (1) All persona descriptions from reviewer agents, (2) Verdict classification logic (CRITICAL/IMPORTANT/MINOR rules), (3) Synthesis consolidation instructions. Each tested independently. Other sections (e.g., output format, tool usage) out of scope for MVP ablation.'"}
{"type": "finding", "id": "v1-scope-guardian-003", "title": "Manual validation workflow lacks operational boundary definition", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR2.3: Manual validation workflow, FR3.3: Manual validation gate", "issue": "Requirements state '<95% confidence findings require user approval' but don't define how confidence is measured, who the 'user' is, or what 'approval' means operationally. FR2.3 says 'subagent validates' but FR3.3 says 'user approval' — which is authoritative?", "why_it_matters": "Without operational boundaries, this requirement creates scope uncertainty. If subagent validation IS user approval (automated), then 'manual' is misleading. If user must review subagent output, that's an interactive workflow outside Inspect AI's batch execution model. This could expand scope to include: confidence scoring model, subagent orchestration logic, user approval UI, approval state persistence. Each is non-trivial.", "suggestion": "Clarify operational model in new FR2.3A: 'Manual validation = subagent reviews finding against design doc, outputs accept/reject decision with rationale. User audits subagent decisions (spot-check 10% of accepted findings). 95% confidence threshold measured by subagent's stated confidence in rationale. MVP: Subagent auto-acceptance, user audit deferred to post-MVP.'"}
{"type": "finding", "id": "v1-scope-guardian-004", "title": "Ground truth dataset format not specified - Inspect AI Dataset schema unknown", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR3.1: Convert v3 review artifacts to Inspect AI Dataset format", "issue": "Acceptance criteria says 'Dataset format' but doesn't specify Inspect AI's Dataset schema. Does it require specific JSON structure? What fields are mandatory? How are findings mapped to Inspect AI's expected input format? FR3.1 says 'JSONL findings' but Inspect AI Datasets typically wrap samples with metadata.", "why_it_matters": "Without explicit schema definition, implementer must research Inspect AI docs to determine format. If schema is complex (e.g., requires Sample objects with input/target/metadata fields), conversion becomes non-trivial engineering work. This could expand MVP scope to include: dataset schema validation, metadata generation, format conversion tooling. Blocks implementation start without additional research.", "suggestion": "Add FR3.1A: 'Dataset schema: Inspect AI Sample format with fields: input (design doc text), target (ground truth JSONL findings), metadata (review date, version, finding count). Conversion script maps v3 JSONL findings to Sample objects. MVP: Manual conversion for v3 dataset (22 Critical findings), automated conversion tooling deferred to post-MVP.'"}
{"type": "finding", "id": "v1-scope-guardian-005", "title": "'Planted flaw detection' implies test dataset creation not captured in MVP scope", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR5.1: Planted flaw detection tests", "issue": "FR5.1 requires 'test dataset includes design docs with planted flaws' but MVP Scope Summary (Phase 1) only lists 'Convert v3 Critical findings' — no mention of creating planted flaw datasets. Acceptance criteria implies net-new design docs with intentionally introduced flaws (architectural gaps, assumption violations, edge cases), which is dataset authoring work.", "why_it_matters": "Creating planted flaw datasets is non-trivial scope: (1) Author design docs with realistic but flawed content, (2) Plant specific flaw types at known locations, (3) Create ground truth labels for each planted flaw, (4) Validate planted flaws are detectable (not too obvious or too subtle). This is weeks of work, not captured in 'Phase 1: 1 week' estimate. If MVP includes FR5.1, timeline is underestimated. If MVP defers FR5.1, it should be explicit.", "suggestion": "Move FR5.1 to 'Explicitly Deferred' list in MVP Scope Summary. Add note: 'Planted flaw detection tests require custom dataset authoring (design docs with intentional flaws). Deferred to Phase 2 when baseline eval infrastructure validated. MVP focuses on existing ground truth (v3 review findings).'"}
{"type": "finding", "id": "v1-scope-guardian-006", "title": "Regression detection threshold (>10%) lacks justification or configurability plan", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR6.2: Compare subsequent eval runs to baseline", "issue": "Acceptance criteria states 'Regression detected if detection rate drops >10%' but provides no rationale for 10% threshold. Is this based on empirical data, arbitrary choice, or borrowed from prior art? No mention of configurability (what if 10% is too sensitive or too lax?).", "why_it_matters": "Hardcoded thresholds without justification create maintainability debt. If 10% proves wrong after first real eval run, changing it requires requirements update. If threshold should vary by finding type (stricter for Critical, looser for Minor), that's scope expansion. Low severity because it's easily adjustable, but worth clarifying now to avoid rework.", "suggestion": "Add rationale to FR6.2: 'Regression threshold 10% chosen as starting point (2/22 Critical findings = observable change without excessive noise). Configurable via eval runner parameter. Post-MVP: Tune threshold based on empirical variance across skill versions.'"}
{"type": "finding", "id": "v1-scope-guardian-007", "title": "Cost calculation requires pricing data source not specified in scope", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "NFR1.1: Log token usage and cost per eval run", "issue": "Acceptance criteria says 'Cost calculated using Claude API pricing (Sonnet $3/$15 per MTok, Opus $15/$75, Haiku $0.25/$1.25)' but doesn't specify where pricing data lives or how it's updated when Claude changes pricing. Hardcoded pricing in scorer logic? External config file? API query?", "why_it_matters": "Claude pricing changes periodically. If pricing is hardcoded in scorer, every price change requires code update + re-test. If pricing lives in config file, that's additional scope (config management, validation). If pricing is queried from API, that's integration work. Minor because it's small scope, but worth defining now to avoid later confusion.", "suggestion": "Add NFR1.1A: 'Pricing data stored in evals/config/model_pricing.json (manually updated). Cost calculation uses JSON lookup. MVP: Hardcode current pricing (2026-02-16 rates), config file migration deferred to post-MVP when pricing changes become frequent.'"}
{"type": "finding", "id": "v1-scope-guardian-008", "title": "Multi-model comparison deferred but FR8.1 acceptance criteria imply MVP implementation", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR8.1: Support model roles pattern, FR8.2: Deferred to post-MVP", "issue": "FR8.2 explicitly defers multi-model comparison to post-MVP, but FR8.1 has full acceptance criteria ('Eval task uses task_with() pattern, run same eval on 3 models, report compares metrics'). These contradict — FR8.1 reads as MVP requirement, FR8.2 says it's deferred.", "why_it_matters": "Scope contradiction creates implementation confusion. If FR8.1 is MVP, then Phase 1 timeline (1 week) underestimates work (multi-model testing is non-trivial). If FR8.2 is correct (deferred), then FR8.1 should be marked 'Post-MVP' or removed. This is a blocker for implementation planning — unclear whether multi-model support is in or out of MVP.", "suggestion": "Resolve contradiction: Move FR8.1 to 'Post-MVP Requirements' section or mark it 'FR8.1 (Post-MVP): Support model roles pattern'. Update MVP Scope Summary to explicitly exclude multi-model comparison: 'MVP uses Sonnet only (claude-sonnet-4-5). Multi-model comparison (Opus, Haiku) deferred to Phase 2 after single-model integration validated.'"}
{"type": "finding", "id": "v1-scope-guardian-009", "title": "Success criteria include 8 items but Phase 1 lists 6 steps - misalignment risk", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "MVP Scope Summary - Phase 1, Success Criteria", "issue": "Phase 1 lists 6 steps (1-6) with 'Success criteria: Eval runs locally, validates Critical findings, ablation test passes' (3 items). Success Criteria section lists 8 checkboxes (1-8). Which is authoritative for MVP completion? If all 8 checkboxes must be met, but Phase 1 only describes 6 steps, there's a 2-item gap.", "why_it_matters": "Misalignment between phase steps and success criteria creates completion ambiguity. If Phase 1 finishes 6 steps but 2 success criteria remain unmet, is MVP done? This enables scope creep ('Well, technically we need to finish all 8 checkboxes') or premature closure ('We did the 6 steps, ship it'). Low severity because items overlap significantly, but worth aligning for clarity.", "suggestion": "Align Phase 1 steps with Success Criteria section. Either: (A) Update Phase 1 to explicitly list all 8 success criteria as steps 1-8, or (B) Clarify that Phase 1's 6 steps implicitly satisfy 8 checkboxes (map each checkbox to corresponding step). Prefer (A) for explicitness."}
{"type": "finding", "id": "v1-scope-guardian-010", "title": "Incremental dataset expansion (FR3.2) implies tooling not captured in MVP", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR3.2: Support incremental dataset expansion", "issue": "FR3.2 acceptance criteria includes 'Dataset loading supports multiple sources, evals can run on single dataset or combined datasets, ground truth counts tracked per dataset'. This implies dataset management tooling (loading logic, source selection, merging, per-source tracking). Not mentioned in Phase 1 scope (6 steps), which only lists 'Convert v3 Critical findings'.", "why_it_matters": "Dataset management tooling is non-trivial scope: (1) Multi-source dataset loader, (2) Dataset merging logic (dedupe? conflict resolution?), (3) Per-source ground truth tracking (separate baseline metrics?), (4) CLI or config to select dataset combinations. This could add 20-40% to Phase 1 effort. If this is MVP, Phase 1 timeline underestimates work. If this is post-MVP (when adding requirements-light dataset), it should be marked deferred.", "suggestion": "Clarify scope: Add FR3.2A: 'MVP: Single dataset support only (v3 Critical findings). Multi-source loading, combined datasets, per-source tracking deferred to Phase 2 when second dataset added (requirements-light). Phase 1 builds foundation (single-dataset eval runner), Phase 2 adds multi-source capability.'"}
{"type": "finding", "id": "v1-scope-guardian-011", "title": "Ablation test pass threshold (>20% per section) conflicts with overall >50% requirement", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR4.1: Drop skill content degrades >50%, FR4.2: Each section contributes >20%", "issue": "FR4.1 requires 'detection rate drops >50%' when all/most content removed. FR4.2 requires 'each section contributes >20%'. If 3 sections exist (personas, verdict, synthesis), and each contributes 20%, total contribution is 60% — consistent. But if 4+ sections exist, or contributions overlap, these thresholds conflict. No guidance on how to resolve.", "why_it_matters": "Conflicting thresholds create test failure ambiguity. Scenario: Drop all content → detection rate drops 55% (passes FR4.1). Drop personas → drops 15%, verdict → drops 12%, synthesis → drops 18% (all fail FR4.2). Does ablation test pass or fail? Without clear precedence or resolution logic, implementer must make arbitrary decision, which could be challenged later.", "suggestion": "Clarify relationship: Add FR4.3A: 'Threshold interpretation: FR4.1 validates overall skill value (at least 50% contribution). FR4.2 validates no single section is trivial (<20% contribution). Both must pass. If FR4.1 passes but FR4.2 fails, investigate section overlap (redundant content across sections). If FR4.1 fails, skill content insufficient regardless of per-section breakdown.'"}
{"type": "finding", "id": "v1-scope-guardian-012", "title": "Inspect View UI accessibility (NFR4.2) implies web server setup not in scope", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "NFR4.2: Inspect View UI accessible for debugging", "issue": "Acceptance criteria says 'inspect view command launches web UI' but doesn't specify whether this is built-in Inspect AI functionality or requires setup. If Inspect AI provides this out-of-box, no issue. If it requires web server configuration, port management, or additional dependencies, that's scope not captured in Phase 1.", "why_it_matters": "If Inspect View requires non-trivial setup (e.g., separate web server, Node.js dependencies, port conflicts with other local services), it adds scope to MVP. Low severity because Inspect AI likely provides this out-of-box (based on ADR-005 references), but worth confirming to avoid surprise scope during implementation.", "suggestion": "Add NFR4.2A: 'Inspect View is built-in Inspect AI feature (no additional setup). If inspect view command fails or requires web server configuration, defer UI debugging to post-MVP (use JSON log analysis instead). MVP can succeed without web UI if CLI output sufficient.'"}
{"type": "finding", "id": "v1-scope-guardian-013", "title": "Ground truth validation confidence measurement undefined - blocks FR2.3 implementation", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR2.3: Manual validation workflow, Open Questions #1", "issue": "FR2.3 requires '<95% confidence findings flagged for manual review' but nowhere defines how confidence is measured. Is it subagent self-reported confidence? LLM logprobs? Heuristic based on finding characteristics? Open Question #1 asks about 95% threshold but assumes confidence measurement exists.", "why_it_matters": "Without confidence measurement definition, FR2.3 is unimplementable. This blocks MVP completion (Success Criteria #5: 'Manual validation workflow operational'). If confidence requires LLM logprobs integration, that's API complexity. If confidence is heuristic (e.g., finding has reference to specific design doc section = high confidence), that's custom logic to design and test. Either way, this is missing scope that must be defined before implementation starts.", "suggestion": "Define confidence measurement in new FR2.3B: 'Confidence measurement: Subagent outputs confidence score 0-100 based on: (1) Direct design doc quote supporting finding = 95-100%, (2) Inference from design doc section = 70-94%, (3) Assumption about unstated context = <70%. MVP: Confidence self-reported by subagent in validation output. LLM logprobs integration deferred to post-MVP.'"}
{"type": "finding", "id": "v1-scope-guardian-014", "title": "Version comparison (FR7) marked deferred but git integration already assumed elsewhere", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR7.1: Support running evals against different skill versions, FR7.2: Deferred to post-MVP", "issue": "FR7.2 defers version comparison due to 'git integration complexity', but NFR5.1 already requires 'Eval runs use git commit hash for versioning' and 'Reproducing eval requires: git checkout'. This suggests git integration is already MVP scope, contradicting FR7.2's deferral rationale.", "why_it_matters": "If git integration is already MVP (for reproducibility), then FR7.1 (run evals against different skill versions) has less incremental complexity than FR7.2 implies. This could mean FR7.1 should be MVP, not deferred. Alternatively, if git integration is deferred, NFR5.1 is mis-scoped. Either way, there's scope misalignment that could lead to rework or missed opportunities.", "suggestion": "Resolve scope: Clarify whether git integration is MVP or post-MVP. If MVP (for NFR5.1 reproducibility), update FR7.2: 'Deferred to post-MVP, but git integration exists for reproducibility. Incremental work: CLI parameter for skill version selection, side-by-side comparison reporting.' If post-MVP, update NFR5.1 to defer git integration and use manual versioning (datestamped directories) for MVP."}
{"type": "finding", "id": "v1-scope-guardian-015", "title": "Dataset size question (Open Question #5) suggests MVP scope is uncertain", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Open Questions #5, FR3.1", "issue": "Open Question #5 asks 'Is 22 Critical findings sufficient sample size, or should we include requirements-light Critical findings too?' This suggests uncertainty about whether MVP dataset scope is adequate. FR3.1 locks in 'v3 Critical findings only' but doesn't address sample size validity.", "why_it_matters": "If 22 findings is insufficient sample size for statistical validity, MVP eval results will be noisy and unreliable. This undermines Job 1 (validate skill effectiveness systematically) and Job 3 (iterate empirically). If requirements-light findings are needed, that's additional dataset conversion work not captured in Phase 1 (6 steps). This creates risk: Either (A) MVP ships with statistically invalid results, or (B) MVP scope expands mid-implementation to add requirements-light dataset.", "suggestion": "Resolve before implementation starts: (A) If 22 findings is sufficient (based on statistical power analysis or precedent), document rationale in FR3.1 and close Open Question #5. (B) If insufficient, add requirements-light Critical findings to Phase 1 scope and update timeline estimate. (C) If uncertain, document assumption: 'MVP assumes 22 findings sufficient for integration validation (not statistical rigor). Phase 2 expands dataset for robust metrics.'"}
{"type": "finding", "id": "v1-scope-guardian-999", "title": "Blind spot check: Scope Guardian perspective", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Meta", "issue": "Focusing on scope boundaries may have caused me to miss: (1) Implicit scope in Jobs-to-Be-Done that isn't explicitly captured in FRs (e.g., Job 4 mentions 'model tiering' but NFR2 defers it all — is Job 4 achievable in MVP?). (2) Scope hidden in acceptance criteria (e.g., 'Scorer accepts review artifact' implies JSONL parsing, error handling, validation — small but real scope). (3) Dependencies between deferred items that could block MVP (e.g., if batch API defers but cost target requires it, MVP can't meet NFR1.2). (4) Scope creep from 'configurable' parameters mentioned but not bounded (e.g., 'configurable thresholds' in FR2.1 — how many configs, what's the config surface area?).", "why_it_matters": "Blind spots in scope review lead to scope creep during implementation. If implicit scope isn't surfaced now, it manifests as 'just one more thing' requests mid-development, breaking the 1-week timeline.", "suggestion": "Cross-check Jobs-to-Be-Done against MVP scope (do MVP FRs actually deliver Job outcomes?). Audit acceptance criteria for hidden scope (parsing, validation, error handling). Review deferred items for MVP-blocking dependencies. Enumerate 'configurable' parameters to bound configuration surface area."}
