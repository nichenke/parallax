{"type": "finding", "id": "v1-success-validator-001", "title": "FR2.2 detection rate thresholds present in requirements but absent from FR2.1 scorer definition", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR2: Custom Scorers", "issue": "FR2.1 defines the severity_calibration_scorer but its acceptance criteria contain no pass/fail thresholds — only that it 'validates Critical findings < 30%, Important 40-50%, Minor 20-30%' and 'returns pass/fail + distribution breakdown'. The criteria for what constitutes a pass on the severity distribution check are never stated. Is 29% Critical a pass? Is 31% a fail? The threshold values listed describe the target distribution, not a scoring rule. FR2.2 correctly lists recall/precision/F1 targets, but these apply to a different function (detection rate against ground truth) than FR2.1's severity distribution check. A developer implementing FR2.1 cannot write a test that verifies the scorer correctly emits pass vs fail.", "why_it_matters": "Two separate scorer functions are implied (distribution validator and detection-rate scorer), but only one has numeric pass/fail criteria. The FR2.1 scorer ships with no testable acceptance criteria, making it impossible to regression-test or verify during code review.", "suggestion": "Add explicit pass/fail rule to FR2.1: 'Scorer returns PASS if: Critical ≤30% AND Important ≥40% AND Important ≤50% AND Minor ≥20% AND Minor ≤30%. FAIL if any threshold violated. Boundaries are inclusive.' Confirm whether FR2.1 and FR2.2 are one scorer or two, and if one, consolidate their acceptance criteria into a single coherent definition-of-done."}
{"type": "finding", "id": "v1-success-validator-002", "title": "FR4.1 ablation baseline X is defined by forward reference but the reference is circular", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR4: Ablation Tests", "issue": "FR4.1 reads: 'Baseline (X): Full skill content achieves detection rate X% … established in FR6.1.' FR6.1 reads: 'First eval run on v3 Critical findings establishes baseline detection rate.' The ablation test is defined in terms of a baseline that is itself defined as whatever the first eval run returns. This means the ablation test has no fixed target — it is parameterized on an unknown quantity at requirements time. The 'Baseline prerequisite: X must be ≥90%' is the only hard anchor. The acceptance criteria list three example thresholds (90%→<40%, 90%→60% FAIL, 70%→<20%) but these examples embed the still-open question of what happens when the baseline lands between 70% and 90% at first run. If baseline is 75%, does the ablation target become <25% or <40%? The formula (X - 50)% is stated but the conditional logic for baseline values other than the examples is absent.", "why_it_matters": "A developer building FR4.1 cannot write the ablation pass/fail logic without making an undocumented assumption about the formula edge cases. Two different developers will produce two different implementations that both match the examples but diverge on intermediate values.", "suggestion": "State the ablation formula unconditionally: 'Test PASSES if ablation_rate < (baseline - 50 percentage points). Test FAILS if baseline < 90% (baseline prerequisite not met — fix skill before running ablation). No special cases.' Remove the three-example enumeration and replace with the formula alone. Add a note: baseline established in FR6.1 must meet the ≥90% prerequisite before ablation results are considered valid."}
{"type": "finding", "id": "v1-success-validator-003", "title": "Success Criteria #5 conflicts with FR4.2 on what the ablation threshold measures", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "Success Criteria", "issue": "Success Criteria #5 states: 'Ablation test validates skill content contributes >50% to detection rate.' FR4.2 states: 'Test passes if each section contributes >20% to detection rate.' These are measuring different things with contradictory thresholds. The Success Criteria is a whole-skill ablation (drop all content → 50% drop). FR4.2 is per-section ablation (drop one section → 20% drop). Both use the phrase 'contributes X% to detection rate' but they refer to different units of ablation and different pass thresholds. A reader evaluating whether the MVP is done cannot determine which threshold to check: is 45% whole-skill drop a pass or a fail? Is a section that contributes exactly 20% a pass or a fail (boundary ambiguity)?", "why_it_matters": "The MVP gate (Success Criteria #5) cannot be evaluated without knowing which ablation criterion applies. If a developer runs the whole-skill ablation and gets 45% drop, they have no way to know if MVP is complete. This blocks the definition of done for Phase 1.", "suggestion": "Separate the two ablation tests explicitly in Success Criteria: 'SC5a: Whole-skill ablation drops detection rate ≥50 percentage points (absolute). SC5b: Each major section contributes ≥20 percentage points when removed independently.' Remove the ambiguous phrase 'contributes X% to detection rate' and replace with 'causes detection rate to drop by X percentage points when removed.'"}
{"type": "finding", "id": "v1-success-validator-004", "title": "FR0 acceptance criteria measure process completion, not ground truth quality", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR0: Ground Truth Validation", "issue": "FR0 acceptance criteria verify that the validation process was followed: expert examined each finding, findings classified, metadata present, ≥15 confirmed findings. None of the criteria measure whether the resulting ground truth is high quality. 'Target: ≥15 confirmed real flaws (68% validation rate)' is a count threshold on an arbitrary denominator (22 Critical findings). There is no criterion for: minimum average confidence score across accepted findings, maximum proportion of 'ambiguous' findings that slip through as 'real_flaw', or consistency between the validation decisions and the design doc state at validation time. Cohen's kappa ≥0.6 applies only 'if multiple reviewers' — but the requirements describe a single reviewer workflow as primary. Kappa is therefore optional and the only inter-rater quality check is not required.", "why_it_matters": "Phase 0 is the prerequisite that the entire eval framework depends on. If low-quality ground truth passes FR0's process checklist, all subsequent detection rate metrics are meaningless. The requirement has a checkbox definition of done rather than a quality definition of done.", "suggestion": "Add quality gates to FR0: (1) 'Average confidence score across accepted findings ≥0.80' (using the confidence field already in the schema). (2) 'Ambiguous findings excluded from ground truth — zero ambiguous findings in datasets/v3_review_validated/critical_findings.jsonl (validation_status must be real_flaw).' (3) 'Each accepted finding must be traceable to a specific section of the design doc (cross-reference present in validation_notes).' These three additions make FR0 a quality gate, not a process gate."}
{"type": "finding", "id": "v1-success-validator-005", "title": "NFR2 cost target is contradicted within the same document", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "NFR2: Cost Tracking", "issue": "The requirements contain three different cost targets for the same eval run: NFR1.2 (under NFR2 section header) states '<$0.50 per eval run'. Success Criteria #7 states '<$2.00 per run (revised from $0.50, see constraint-finder-003)'. The summary doc passed to reviewers also states '<$0.50 in heading, <$2.00 in success criteria — inconsistency.' The document was partially updated (Success Criteria updated to $2.00) but NFR1.2 still reads '$0.50'. This is not a minor textual issue: NFR1.2 includes acceptance criteria ('v3 Critical findings eval costs <$0.50') that conflict with the Success Criteria checkpoint. A developer building the cost reporting feature will hardcode the wrong threshold.", "why_it_matters": "NFR1.2 acceptance criteria and Success Criteria #7 will produce contradictory test results against the same eval run. The system cannot simultaneously pass both. This creates an impossible definition of done for cost compliance.", "suggestion": "Update NFR1.2 acceptance criteria to match Success Criteria #7: change '<$0.50' to '<$2.00' in the NFR1.2 acceptance criteria text. Add a note: 'Target revised from $0.50 after token budget analysis (constraint-finder-003). $0.50 remains aspirational for post-MVP optimization via Haiku + batch API.' Then add a token budget breakdown showing $2.00 is achievable at current model pricing."}
{"type": "finding", "id": "v1-success-validator-006", "title": "FR3.3 confidence threshold (95%) has no defined measurement method", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR3: Test Datasets", "issue": "FR3.3 acceptance criteria state: 'Only ≥95% confidence findings auto-added to ground truth' and '<95% confidence findings require user approval.' The confidence score referenced here appears in the FR0 dataset schema as a field ('confidence': 0.92), but the requirements never specify how confidence is calculated. There are three plausible interpretations: (a) the human validator self-reports a score (0–1 scale), (b) an LLM subagent generates a score during extraction, (c) the score is derived from multiple reviewers flagging the same finding. The requirements use the term 'confidence' in both FR0 (human validation confidence) and FR3.3 (auto-add gate) but may mean different things in each context. Without a defined measurement procedure, the 95% gate cannot be implemented or tested.", "why_it_matters": "FR3.3 is the manual validation gate that prevents false positives from entering ground truth. If the confidence measurement is undefined, the gate either never fires (all findings auto-added) or blocks everything (no defined calculation means no calculation runs). Both failure modes corrupt ground truth.", "suggestion": "Define confidence explicitly in FR3.3: 'Confidence is the human validator's self-assessed certainty (0.0–1.0 scale) entered during the validation UI review session. Scale: 1.0=certain the finding is a real flaw with direct evidence in design doc, 0.9=high confidence, 0.8=moderate confidence, <0.8=ambiguous. Findings with confidence <0.95 are flagged in the UI for a second review pass. The confidence field in the JSONL schema stores this value.' This makes the gate unambiguous and testable."}
{"type": "finding", "id": "v1-success-validator-007", "title": "FR4.2 section contribution threshold (20%) has no empirical basis and is not anchored to the baseline", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR4: Ablation Tests", "issue": "FR4.2 acceptance criteria: 'Test passes if each section contributes >20% to detection rate.' The 20% threshold is asserted without justification. No explanation is given for why 20% distinguishes a meaningful contribution from noise. Furthermore, 20% absolute contribution is evaluated against an unspecified reference: if baseline is 90%, a section that drops detection to 75% (15% drop) would fail — but whether this represents a meaningless section or an appropriate distribution of contribution across many sections is not evaluated. With 4-6 skill sections, expected per-section contribution at baseline 90% might reasonably be 15-20% each. Setting the threshold at 20% means a skill with perfectly distributed contributions across 5 equal sections (18% each) would fail every ablation test even though the skill is working as designed.", "why_it_matters": "A threshold with no justification will generate false failures during the first ablation run, requiring ad-hoc threshold adjustment. Without a principled basis, the team cannot know whether to adjust the threshold or fix the skill when failures appear.", "suggestion": "Change FR4.2 to: 'Each section must account for a measurable contribution (>10% absolute drop when removed). The 10% lower bound is chosen to distinguish signal from noise at N=15-22 sample sizes. Sections contributing 10-20% are investigated but not automatic failures. Sections contributing <10% are flagged as potentially redundant or insufficiently tested.' Add a note: 'Threshold calibrated after first pilot ablation run — if all sections cluster near the threshold, adjust threshold to 2x the noise floor observed.'"}
{"type": "finding", "id": "v1-success-validator-008", "title": "FR6.2 regression threshold (10%) has no statistical grounding and no pass/fail state definition", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR6: Baseline Metrics and Regression Detection", "issue": "FR6.2 states: 'Regression detected if detection rate drops >10%.' The design doc (approved, separate from requirements) notes: 'Statistical significance tests deferred: N=15-22 is too small for reliable p-values. Right fix is more test data, not stats machinery.' This is a reasonable decision. However, the requirements document gives the 10% threshold without specifying: (a) is this a 10% absolute drop (0.90 → 0.80) or 10% relative drop (0.90 → 0.81)? (b) what is the pass/fail state when regression is detected — does it block merging, trigger a warning, or produce a report only? The design doc (separate artifact) provides PASS/WARN/FAIL levels with 5-10% WARN and >10% FAIL, but this detail is absent from the requirements, meaning the requirements and design doc are inconsistent.", "why_it_matters": "The regression detection feature has no clear definition of done. A developer building compare_to_baseline.py can implement the 10% threshold check but cannot determine the required behavior when the threshold is crossed — block, warn, or log — without referencing the design doc, creating an implicit dependency between two documents that should be independent.", "suggestion": "Promote the PASS/WARN/FAIL model from the design doc into the requirements FR6.2 acceptance criteria: 'Regression levels: PASS = all metrics within 10% of baseline (absolute). WARN = any metric drops 5-10% (report generated, merge not blocked). FAIL = any metric drops >10% (must be resolved before merging skill changes). Absolute drop definition: 10% absolute = delta of 0.10, e.g., 0.90 → 0.80.' This makes FR6.2 a complete testable specification without requiring the design doc."}
{"type": "finding", "id": "v1-success-validator-009", "title": "Success Criteria #2 ('Inspect AI installed, configured') is binary and not testable at the boundary", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Success Criteria", "issue": "Success Criteria #2 states: 'Inspect AI installed, configured with Anthropic provider.' This is a state assertion, not a measurable criterion. There is no definition of what 'configured' means beyond 'installed.' FR1.1 adds detail ('inspect eval CLI runs successfully on test dataset', 'Inspect View displays results') but these appear in functional requirements, not in the success criteria checkpoint. The MVP gate requires checking all 8 criteria — if a reviewer is evaluating SC #2 against FR1.1 acceptance criteria, they are implicitly combining two separate sections. The success criteria section should be self-contained enough to evaluate without cross-referencing functional requirements.", "why_it_matters": "MVP gate evaluations are high-stakes checkpoints. If a reviewer uses only the Success Criteria section to decide whether Phase 1 is done, SC #2 as written would pass after pip install alone. The actual definition of 'configured' is distributed across FR1.1, FR1.2, and FR1.3.", "suggestion": "Expand SC #2 to: 'Inspect AI installed and verified: (a) inspect eval CLI runs successfully against test dataset without errors, (b) Inspect View web UI launches and displays results, (c) eval produces EvalLog JSON artifact with per-sample token counts. All three must pass.' This makes SC #2 self-contained and aligns it precisely with FR1.1-FR1.3 acceptance criteria without requiring cross-reference."}
{"type": "finding", "id": "v1-success-validator-010", "title": "Post-MVP success criteria lack any measurable definition", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Success Criteria", "issue": "The 'Post-MVP Expansion Criteria' section lists five outcomes: 'All four scorers operational', 'Multi-model comparison suite', 'CI/CD integration', 'Batch API integration', 'Fine-grained ablation tests.' None of these have acceptance criteria, thresholds, or definitions of done. They are feature descriptions, not success criteria. This is particularly important because the requirements document is versioned (Draft v1.0) and will be referenced during Phase 2 planning — if Phase 2 success criteria are undefined, Phase 2 has no definition of done at planning time. The document acknowledges Phase 2 and Phase 3 scopes but treats them as a feature list rather than measurable outcomes.", "why_it_matters": "Post-MVP criteria are the natural starting point for Phase 2 requirements. Treating them as a feature list rather than measurable outcomes means Phase 2 will start from scratch on defining success, adding avoidable rework and scope creep risk.", "suggestion": "For each post-MVP criterion, add a one-line measurable outcome: e.g., 'All four scorers operational: each scorer produces a numeric score, EvalLog records the score, and a pilot run on v3 dataset completes without errors.' These do not need to be fully specified now — placeholder measurability criteria prevent the list from becoming a wishlist. Alternatively, explicitly label them as 'out of scope for this document' and create a stub requirements doc for Phase 2."}
{"type": "finding", "id": "v1-success-validator-011", "title": "FR5.1 planted flaw detection target (70%) sources from a misapplied cross-reference", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR5: Adversarial Tests", "issue": "FR5.1 acceptance criteria state: 'Target: >70% detection rate for planted Critical flaws (from FR1.2 acceptance criteria in parallax-review-requirements-v1.md).' The review summary flags this as a misapplied threshold — the cited source documents a diversity metric, not a detection rate. The 70% figure therefore has no valid empirical or analytical basis for planted flaw detection. The requirements cite a source to justify a threshold but the source does not support the threshold. This creates the appearance of a justified target while the actual number is unsupported.", "why_it_matters": "When the first planted flaw eval runs and achieves 65%, the team cannot determine whether to adjust the threshold or investigate the skill, because the threshold was never justified. An unjustified threshold either produces false confidence (too low) or endless rework (too high) with no principled way to choose.", "suggestion": "Remove the cross-reference to parallax-review-requirements-v1.md FR1.2. Replace with: 'Target: >70% detection rate — provisional, based on judgment that missing 3 in 10 planted flaws is unacceptable for a tool that reviewers will trust. Validate against first planted flaw pilot run; adjust threshold if 70% is unreachable given model capability or if 70% is trivially exceeded.' This makes the target honest about its provisional status rather than falsely attributed."}
{"type": "finding", "id": "v1-success-validator-012", "title": "NFR1.1 API key security criterion is a policy statement, not a testable acceptance criterion", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "NFR1: API Key Security", "issue": "NFR1.1 acceptance criteria: 'API keys stored in environment variables or local config files (gitignored)', '.gitignore includes patterns for credential files', 'Pre-commit hook blocks commits containing API key patterns.' The first criterion is a developer practice, not a testable state — there is no automated test that verifies a key 'is stored in an env var' at runtime. The third criterion (pre-commit hook) is the only testable item, and it already exists in the repo (the hook is referenced in CLAUDE.md). There is no criterion for verifying the hook actually detects API key patterns vs only blocking direct main commits.", "why_it_matters": "Security requirements that cannot be tested produce a false sense of compliance. If the pre-commit hook does not scan for API key patterns (only for branch protection), NFR1.1 passes its process checklist but fails its security intent.", "suggestion": "Replace the non-testable criteria with a verifiable test: 'Pre-commit hook test: attempt to commit a file containing string matching /sk-ant-[a-zA-Z0-9]+/ — hook must reject the commit with a credential leak warning. Test passes if hook rejects; fails if commit succeeds.' If the hook does not yet have credential scanning, add it to scope and make NFR1.1 acceptance conditional on its implementation."}
{"type": "finding", "id": "v1-success-validator-013", "title": "NFR4.1 '<5 minutes' timing criterion has no defined measurement conditions", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "NFR5: Development Velocity", "issue": "NFR4.1 acceptance criteria: 'v3 Critical findings eval (22 findings) completes in <5 minutes on MacBook Pro.' The acceptance criterion specifies the machine class but not: (a) network conditions (local API vs remote with high latency), (b) whether timing includes validation UI startup or eval-only, (c) whether 22 findings run in parallel or serially, (d) cold vs warm model response times. A 5-minute target may be met in isolation but missed when network latency adds 30 seconds per sample (22 samples × 30s = 11 minutes). The acceptance criterion cannot be verified without measurement conditions.", "why_it_matters": "NFR4.1 directly enables the tight feedback loop described in Job 1. If the timing criterion is met in lab conditions but fails in normal usage (high API latency), the requirement passes acceptance testing but fails in practice. This is a classic lab-vs-production gap.", "suggestion": "Add measurement conditions to NFR4.1: 'Measured under: normal home broadband conditions, Anthropic API p50 latency. Timing starts when inspect eval command executes, ends when CLI returns. Does not include Inspect View startup. If eval exceeds 5 minutes on first run: (1) measure per-sample latency, (2) determine whether parallelism is enabled, (3) flag for Haiku optimization if parallelism alone is insufficient.'"}
{"type": "finding", "id": "v1-success-validator-014", "title": "FR1.3 EvalLog acceptance criteria do not specify the format or schema of required fields", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR1: Inspect AI Integration", "issue": "FR1.3 acceptance criteria: 'Each eval run produces EvalLog JSON artifact', 'Log includes per-sample token counts (input, output, total)', 'Log includes latency metrics with retry tracking', 'Logs viewable in Inspect View UI.' The first three criteria describe content requirements but not schema. 'Per-sample token counts' could mean a flat sum or a structured object per sample. 'Latency metrics with retry tracking' does not specify what retry information is required (retry count, retry reason, total retry time). Without schema, two implementations both satisfy the criteria but produce incompatible JSON that cannot be processed by downstream scripts (compare_to_baseline.py, cost tracking).", "why_it_matters": "FR6.1 depends on FR1.3 — baseline metrics are extracted from EvalLog. If EvalLog schema is undefined, the baseline extraction script (compare_to_baseline.py) must make assumptions about field names that may not match actual Inspect AI output. This is the integration risk that blocks regression detection.", "suggestion": "Either (a) reference Inspect AI's documented EvalLog schema and state 'per Inspect AI EvalLog specification, no additional fields required' — this leverages the existing standard rather than reinventing it, or (b) add a minimal schema note: 'EvalLog must include: results[].score.value.recall (float), results[].score.value.precision (float), results[].usage.input_tokens (int), results[].usage.output_tokens (int), total_cost (float). Field names match Inspect AI defaults; do not rename.'"}
{"type": "finding", "id": "v1-success-validator-999", "title": "Blind spot check: Success Validator perspective", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Meta", "issue": "This review focused on whether success criteria are measurable, specific, and sufficient to evaluate completion. Potential blind spots: (1) I treated the requirements document as the sole source of truth for success criteria, but the approved design doc contains additional acceptance details (PASS/WARN/FAIL regression levels, fuzzy matching thresholds) that the requirements omit. I flagged the inconsistency but did not evaluate whether the design doc itself has measurability gaps. (2) I may have under-weighted the open questions at the end of the document — four open questions directly affect threshold values in success criteria (ablation %, baseline update frequency, grader model, dataset size). These are not minor clarifications; they are prerequisites to finalizing the success criteria. (3) I assumed the v1.1 requirements document presented in the prompt is the current authoritative version, but the session notes reference v1.1 while the document header reads v1.0. If the prompt reflects v1.0 and v1.1 addresses some gaps, my findings may duplicate resolved issues.", "why_it_matters": "Blind spot awareness helps catch gaps in the review process.", "suggestion": "Consider: (1) Review the design doc's acceptance criteria against requirements for consistency — promote any design-doc-only criteria into requirements to make requirements self-contained. (2) Resolve the four open questions before finalizing Success Criteria — at minimum, add placeholder values with explicit 'provisional, requires first-run calibration' labels. (3) Confirm whether this is v1.0 or v1.1 — if v1.1, re-check which findings in this review were already addressed."}
