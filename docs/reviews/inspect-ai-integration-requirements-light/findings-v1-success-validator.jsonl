{"type": "finding", "id": "v1-success-validator-001", "title": "FR2.2 detection rate thresholds undefined", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR2: Custom Scorers (Phase 1: Severity Calibration)", "issue": "FR2.2 requires 'detection rate (recall), precision, F1 score' but provides no target thresholds. Acceptance criteria say 'reports metrics' but don't define what values constitute success vs failure.", "why_it_matters": "Cannot objectively determine if the scorer succeeded. A 30% detection rate technically satisfies 'reports detection rate' but would indicate fundamental failure. No actionable pass/fail signal for developers.", "suggestion": "Add quantitative thresholds: 'Detection rate ≥70% for Critical findings (matches FR5.1 planted flaw target), precision ≥80% (minimize false positives), F1 ≥0.70 (balanced performance).'"}
{"type": "finding", "id": "v1-success-validator-002", "title": "FR4.1 ablation detection rate baseline undefined", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR4: Ablation Tests (Negative Testing)", "issue": "FR4.1 states 'detection rate < (X - 50)%' but X is undefined. Acceptance criteria require establishing baseline X first, but this is buried in implication rather than stated as explicit prerequisite.", "why_it_matters": "Cannot run ablation tests without baseline. Test could falsely pass (e.g., baseline 40%, ablation 20% = only 20% drop, fails >50% threshold) or falsely fail (baseline 95%, ablation 60% = 35% drop but still 60% absolute performance).", "suggestion": "Revise to: 'Baseline: Full skill content achieves ≥70% detection rate on v3 Critical findings (X ≥ 70%). Ablation: Detection rate drops to <(X - 50%) when skill content removed. Example: 70% baseline → <20% ablation = pass, 70% → 40% ablation = fail.'"}
{"type": "finding", "id": "v1-success-validator-003", "title": "FR4.2 section contribution threshold arbitrary", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR4: Ablation Tests (Negative Testing)", "issue": "FR4.2 requires each section contributes '>20% to detection rate' but provides no justification for 20% threshold. Is this per-section (3 sections × 20% = 60% total) or independent? How do overlapping contributions between sections factor in?", "why_it_matters": "Arbitrary threshold may fail to detect meaningful degradation (18% contribution marked as pass despite being significant) or create false negatives (removing 'verdict logic' might have 15% direct impact but 40% indirect impact through reviewer confusion).", "suggestion": "Clarify: 'Each section must contribute ≥20% to detection rate when removed independently. Rationale: 3 major sections (personas, verdict, synthesis) should each contribute meaningful signal. Overlapping effects acceptable—sum may exceed 100%. Threshold validates no section is pure filler.' Add post-MVP: 'Measure interaction effects between sections.'"}
{"type": "finding", "id": "v1-success-validator-004", "title": "FR6.2 regression threshold lacks justification", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR6: Baseline Metrics and Regression Detection", "issue": "FR6.2 uses '>10% drop' as regression threshold with '(configurable)' parenthetical but no guidance on when to adjust. Is 10% appropriate for all metrics (detection rate, precision, F1)? Is this absolute (70% → 60%) or relative (70% → 63%)?", "why_it_matters": "Miscalibrated threshold causes false alarms (10% relative = 7% absolute for 70% baseline, possibly noise) or missed regressions (10% absolute too lenient for 90% baseline). Configurable without guidance leads to arbitrary adjustments when tests fail.", "suggestion": "Specify: 'Regression = detection rate drops >10 percentage points (absolute, not relative). Example: 70% → 59% triggers regression, 70% → 61% does not. Precision/F1 use same 10pp threshold. Rationale: Matches statistical significance for n=22 Critical findings (binomial proportion test, 95% confidence). Adjust threshold if dataset size changes: smaller datasets need wider tolerance.'"}
{"type": "finding", "id": "v1-success-validator-005", "title": "NFR1.2 cost target lacks breakdown", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "NFR1: Cost Tracking", "issue": "NFR1.2 targets '<$0.50 per eval run' for 22 findings but provides no token budget breakdown. At $3/$15 per MTok (Sonnet input/output), what input/output token counts are expected? Acceptance criteria say 'flag for optimization' if exceeded but don't define optimization path.", "why_it_matters": "Cannot validate cost target is achievable before building. Example calculation: 22 findings × 4K tokens/finding input × $3/MTok = $0.26 input alone. Add 2K output tokens × $15/MTok = $0.66 total, already exceeds budget. May need Haiku or prompt compression from day 1.", "suggestion": "Add token budget: 'Target assumes 22 findings × 3K input tokens (66K total) + 1K output tokens/finding (22K total) = 88K total tokens. Cost: 66K × $3/MTok + 22K × $15/MTok = $0.20 + $0.33 = $0.53. Optimization paths if exceeded: (1) Use Haiku ($0.04 at same token count), (2) reduce input tokens via summarization, (3) batch findings (process 5 at once).'"}
{"type": "finding", "id": "v1-success-validator-006", "title": "NFR4.1 completion time lacks failure response", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "NFR4: Development Velocity", "issue": "NFR4.1 requires '<5 minutes' completion with 'flag for optimization' if exceeded, but doesn't define whether exceeding 5 minutes blocks deployment or just triggers investigation. Is this hard requirement or aspirational goal?", "why_it_matters": "Unclear whether a 7-minute eval run is deployment blocker or just suboptimal. 'Rapid iteration' implies hard requirement but 'flag for optimization' implies soft suggestion. Developer doesn't know if 6-minute run needs immediate fix or can ship and optimize later.", "suggestion": "Clarify severity: 'Hard requirement: <5 minutes for interactive development (blocks deployment if exceeded). Soft target: <2 minutes optimal. If eval exceeds 5 minutes: (1) reduce dataset to 10 Critical findings for dev iteration, (2) use full 22-finding dataset for CI/CD only, (3) investigate Haiku or prompt caching for speedup. Post-MVP: batch API acceptable for overnight runs (no time constraint).'"}
{"type": "finding", "id": "v1-success-validator-007", "title": "Success Criteria checkboxes not measurable", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Success Criteria", "issue": "MVP completion checkboxes (#1-8) use ambiguous verbs: 'installed' (no version specified), 'runs' (successfully on what inputs?), 'established' (what constitutes documented?), 'operational' (manual workflow—how do we test?). Only #6, #7 have quantitative targets.", "why_it_matters": "Checklist appears complete but lacks objective verification. Two developers could disagree whether #2 ('severity calibration scorer runs') is satisfied—does it need to pass tests, or just execute without errors? Ambiguity causes premature 'done' declarations or endless perfectionism.", "suggestion": "Make each checkbox verifiable: '1. Inspect AI ≥v0.3.0 installed, `inspect eval --version` succeeds. 2. Severity scorer processes v3 JSONL, returns metrics dict, no exceptions. 3. Baseline JSON file exists at evals/baselines/v3_critical_baseline.json with detection_rate, precision, recall, F1, cost fields populated. 4. Ablation test script exists, baseline run achieves ≥70% detection, ablation run drops to <(baseline - 50%). 5. Manual validation script prompts user for accept/reject on <95% confidence findings, updates ground truth JSONL. 6-8: Already quantitative (keep unchanged).'"}
{"type": "finding", "id": "v1-success-validator-008", "title": "FR3.3 confidence threshold lacks grounding", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR3: Test Datasets", "issue": "FR3.3 and FR2.3 use '95% confidence' threshold for auto-adding findings but don't define how confidence is calculated. Is this model-reported logprob? Human annotation agreement? LLM-as-judge score? Open Question #1 asks if 95% is right, but question is premature without defining what 95% means.", "why_it_matters": "Cannot implement manual validation gate without confidence calculation method. If using model logprobs, 95% may be uncalibrated (models are overconfident). If using LLM-as-judge, need grader prompt and threshold. If using human agreement, need multiple annotators (expensive).", "suggestion": "Define confidence calculation: 'Phase 1 (MVP): Manual review for all findings—no automated confidence calculation. Human reviewer rates each finding as Valid/Invalid/Uncertain. Uncertain → exclude from ground truth. Phase 2 (post-MVP): Build LLM-as-judge confidence scorer (Sonnet grades finding quality 0-100, threshold TBD based on Phase 1 correlation). 95% confidence target is aspirational pending Phase 2 validation.'"}
{"type": "finding", "id": "v1-success-validator-009", "title": "FR5.1 planted flaw detection rate borrowed from different skill", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR5: Adversarial Tests (Robustness)", "issue": "FR5.1 targets '>70% detection rate for planted Critical flaws' citing 'FR1.2 acceptance criteria in parallax-review-requirements-v1.md'. This requirement is for a different skill (parallax:review) with different scope. Inspect AI integration may have different performance characteristics.", "why_it_matters": "Borrowed target may be inappropriate. If parallax:review targets 70% with 6 reviewers but Inspect AI evals use single-model grading, 70% may be unattainable. Alternatively, Inspect AI might achieve 85% through better prompting, making 70% too lenient. Need target derived from this skill's capabilities.", "suggestion": "Establish independent baseline: 'Phase 1 (MVP): Run planted flaw test on v3 Critical findings to establish baseline detection rate (no pre-defined target—measure actual performance). Phase 2: Set target at baseline - 10pp as regression threshold. If baseline is 75%, regression = <65%. If baseline is 60%, investigate skill effectiveness before setting target. Do not inherit targets from different skills without validation.'"}
{"type": "finding", "id": "v1-success-validator-010", "title": "Post-MVP expansion criteria lack triggers", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Success Criteria", "issue": "Post-MVP Expansion Criteria lists 5 items but doesn't specify conditions that trigger expansion. Is this sequential (complete all 5 for Phase 2)? Parallel (choose based on needs)? Conditional (only expand if MVP shows promise)?", "why_it_matters": "Unclear whether post-MVP is automatic next step or contingent on MVP outcomes. If MVP reveals Inspect AI doesn't fit parallax workflow, expanding to all four scorers wastes effort. Need decision gate between MVP and expansion.", "suggestion": "Add expansion gate: 'Expand to Phase 2 if MVP achieves: (1) Detection rate ≥60% on v3 Critical findings (validates integration pattern), (2) Cost <$0.50/run (validates budget fit), (3) Developer feedback: useful for skill iteration (validates workflow fit). If any criterion fails, investigate root cause before expansion. Phase 2 prioritization: (1) severity scorer → quality scorer → patterns scorer → transcripts scorer (increasing complexity), OR (2) choose based on highest-impact skill gaps identified in MVP runs.'"}
{"type": "finding", "id": "v1-success-validator-011", "title": "Open Question #5 dataset size not answerable without pilot", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Open Questions", "issue": "Question #5 asks 'is 22 Critical findings sufficient sample size' but provides no statistical framework to answer. What statistical power is required? What effect size are we detecting? Cannot answer theoretically—needs empirical pilot.", "why_it_matters": "Delaying decision wastes effort (if 22 is insufficient, expanding dataset later causes rework) or premature optimization (if 22 is sufficient, adding more findings increases cost/complexity for no gain). Question format implies answer exists without data.", "suggestion": "Reframe as testable hypothesis: 'MVP hypothesis: 22 Critical findings provide sufficient statistical power to detect ≥20pp detection rate differences (e.g., 70% baseline vs 50% ablation) with 80% power, 95% confidence (binomial proportion test). Validation: Run power analysis after MVP baseline established. If underpowered, expand dataset to n=35 (calculated). If overpowered, consider reducing to n=15 for faster iteration. Track variance in detection rate across runs—high variance indicates need for larger dataset.'"}
{"type": "finding", "id": "v1-success-validator-012", "title": "FR1.3 latency metrics missing targets", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR1: Inspect AI Integration", "issue": "FR1.3 requires 'latency metrics with retry tracking' but provides no performance targets. Is 10-second eval run acceptable? 5 minutes? Relates to NFR4.1 (5-minute completion) but that measures end-to-end, not per-sample latency.", "why_it_matters": "Latency metrics without targets become logging-only (no actionable threshold). High per-sample latency might indicate prompt inefficiency or API throttling, but won't be detected without threshold. NFR4.1 (5-minute total) could be satisfied with highly variable latency (1s per sample × 20 samples + 3-minute outlier).", "suggestion": "Add per-sample latency targets: 'Per-sample latency p50 <5 seconds, p95 <15 seconds, p99 <30 seconds for 22-finding v3 dataset (Sonnet, includes API round-trip). Total eval time <5 minutes (NFR4.1). Retry tracking: flag samples with >1 retry (indicates API instability or rate limiting). If p95 exceeds 15s, investigate: (1) prompt length reduction, (2) output token limit, (3) API rate limit hit.'"}
{"type": "finding", "id": "v1-success-validator-013", "title": "Definition of 'done' scattered across document", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Success Criteria", "issue": "MVP completion criteria appears in 3 locations: 'Success Criteria' section (8 checkboxes), 'MVP Scope Summary Phase 1' (6 items), and FR acceptance criteria throughout doc. No single authoritative definition of done.", "why_it_matters": "Developer completes 'Success Criteria' 8 checkboxes but misses 'coarse-grained ablation test' from MVP Scope Summary (item #5). Or completes Phase 1 scope but doesn't realize NFR4.1 and NFR1.2 are hard requirements. Scattered definition causes incomplete delivery.", "suggestion": "Consolidate into single Definition of Done section: 'MVP v1.0 complete when all criteria met: (1) Functional: FR1.1, FR1.2, FR1.3, FR2.1, FR2.2, FR3.1, FR4.1, FR4.2 acceptance criteria satisfied. (2) Performance: NFR4.1 (<5 min), NFR1.2 (<$0.50). (3) Validation: Success Criteria checkboxes #1-8 all verified. (4) Documentation: Baseline metrics documented in evals/baselines/v3_critical_baseline.json. Refer to this section as single source of truth—other sections describe requirements, this section defines done.'"}
{"type": "finding", "id": "v1-success-validator-999", "title": "Blind spot check: Success Validator perspective", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Meta", "issue": "What might I have missed by focusing on success criteria? (1) Assumed quantitative targets should exist for all metrics—some qualitative outcomes may be valid (e.g., 'manual validation workflow operational' is inherently subjective). (2) Focused on measurement precision—may have over-specified for MVP exploratory phase where learning is more valuable than hitting exact thresholds. (3) Did not validate whether success criteria align with Jobs-to-Be-Done outcomes (e.g., Job 1 outcome is 'data-driven confidence' but FR2.2 has no confidence metric). (4) Missed validation of negative criteria—how do we know if eval framework is NOT working? (e.g., baseline detection rate <40% should trigger framework redesign, not just 'optimization').", "why_it_matters": "Blind spot awareness helps catch gaps in the review process. Over-quantification can create false precision. Missing alignment between jobs and success criteria means requirements pass review but don't solve user problems. No failure criteria means zombie projects that meet metrics but provide no value.", "suggestion": "Add failure criteria to Success Criteria section: 'Abort MVP if: (1) Baseline detection rate <50% (indicates ground truth mismatch or skill fundamental failure), (2) Cost per run >$2.00 (10× over budget suggests architecture mismatch), (3) Eval completion time >30 minutes (workflow friction too high for iteration), (4) Developer feedback: eval results not actionable (framework doesn't answer skill improvement questions). Validate JTBD alignment: Job 1 (data-driven confidence) → FR2.2 needs confidence intervals on detection rate, not just point estimate.'"}
