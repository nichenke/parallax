{"type": "finding", "id": "v1-constraint-finder-001", "title": "Python environment constraints absent from requirements", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "Non-Functional Requirements", "issue": "No Python version, virtual environment strategy, or dependency conflict policy is stated in the requirements document. The implementation chose Python 3.11 (pyproject.toml line 8) and .venv-evals/, but this decision is not documented as a requirement. Inspect AI requires Python 3.10+, and the Makefile uses `python3` (not `python3.11`), so a machine with a different default Python will silently install against the wrong interpreter.", "why_it_matters": "Developers cloning the repo on a machine with Python 3.9 as default will get a broken install. The <5-minute eval target (NFR4.1) fails immediately if setup fails. No requirement documents what to do if system Python conflicts with a Claude Code Python environment.", "suggestion": "Add NFR: Python 3.11 required (not just >=3.10). Makefile setup target must invoke `python3.11 -m venv` explicitly, not `python3`. Document that evals run in a dedicated .venv-evals/ isolated from any Claude Code Python env. Add `.python-version` = 3.11 to git and document pyenv or equivalent as the management tool."}
{"type": "finding", "id": "v1-constraint-finder-002", "title": "API key security policy states intent but not mechanism", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "NFR1: API Key Security", "issue": "NFR1.1 states keys must not be in version control and should use environment variables. NFR1.2 states work/personal separation is required. Neither requirement specifies the enforcement mechanism. The pre-commit hook (mentioned in CLAUDE.md) blocks direct commits to main but has no key-pattern scanning. The .gitignore additions listed in the design doc cover .env files but not inline ANTHROPIC_API_KEY assignments in Python test files. Bedrock IAM role names, account IDs, and the key rotation frequency are all TBD.", "why_it_matters": "A developer writing a test that hardcodes a key for debugging can commit it without the hook firing. Work/personal billing attribution is undefined — running evals on the personal key against work data (or vice versa) violates corporate policy and will surface in unexpected billing attribution. Key rotation TBD means it will never happen until an incident forces it.", "suggestion": "Add a pre-commit check using `detect-secrets` or `gitleaks` that scans staged files for sk-ant- patterns and AWS credential patterns. Add NFR: ANTHROPIC_API_KEY for personal dev, AWS_PROFILE=parallax-work for Bedrock. Document key rotation as 90-day maximum. Close the 'rotation procedure TBD' gap before Phase 1 ships."}
{"type": "finding", "id": "v1-constraint-finder-003", "title": "Cost target revised in Success Criteria but original NFR still contradicts it", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "NFR2: Cost Tracking", "issue": "NFR2 (labeled NFR1 in the document due to numbering bug) contains two contradictory cost targets: NFR1.2 (the second NFR1.2) targets '<$0.50 per eval run (MVP)' while Success Criteria item 7 states '<$2.00 (revised from $0.50, see constraint-finder-003)'. The NFR body has not been updated to match the Success Criteria revision. The token math is also absent: 22 samples x design doc input (~2k tokens) x Sonnet input rate ($3/MTok) + 22 x ~8k output tokens x Sonnet output rate ($15/MTok) = ~$2.76 before caching. Even with 90% prompt caching on input, output tokens alone are ~$2.64. The $2.00 target may also be infeasible without model tiering or batch API.", "why_it_matters": "Contradictory cost targets in two sections means any implementation team will choose one and ignore the other. If output token count per sample is higher than estimated, even the $2.00 revised target will be missed on the first eval run, triggering an immediate optimization detour. No token budget breakdown means the target cannot be validated before running.", "suggestion": "Remove the $0.50 target from NFR1.2 entirely. In Success Criteria #7, add a token budget table: 22 samples, estimated input/output tokens per sample using a sample run, resulting cost at Sonnet rates, and the caching discount applied. State explicitly: 'Target is $2.00 with prompt caching enabled; if caching unavailable, cost may reach $3-4 and batch API becomes required.'"}
{"type": "finding", "id": "v1-constraint-finder-004", "title": "Manual validation workflow timing not separated from eval automation target", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR3: Test Datasets", "issue": "FR3.3 requires manual validation of <95% confidence findings. NFR4.1 targets <5 minutes for eval runs. The requirements do not explicitly state that manual validation is a one-time setup cost, not part of the eval runtime. A developer reading FR3.3 and NFR4.1 together will see a conflict: how can a run complete in <5 minutes if it requires human review of flagged findings? The 87-finding estimation (78 minutes for the full set) is only in the review summary, not in the requirements themselves.", "why_it_matters": "Without this separation documented in the requirements, the <5-minute NFR will be interpreted as applying to the full workflow including validation, which it cannot meet. This creates false failure signals against the success criteria.", "suggestion": "Add a note to NFR4.1: 'The <5-minute target applies to automated eval runs only. Ground truth creation (FR0, FR3.3) is a one-time human validation cost estimated at 45-90 minutes for 22 Critical findings, occurring before eval runs begin. Subsequent evals run fully automated.'"}
{"type": "finding", "id": "v1-constraint-finder-005", "title": "Inspect AI version constraint missing — pinned to >=0.3 without upper bound", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR1: Inspect AI Integration", "issue": "FR1.1 requires installing Inspect AI but specifies no version. The pyproject.toml pins `inspect-ai>=0.3` with no upper bound. Inspect AI is actively developed (v0.2.34 referenced in MEMORY.md for inspect_swe). A breaking API change in a minor version will silently break evals unless pinned. The `MemoryDataset`, `Sample`, and `@scorer` APIs referenced in the design doc are version-sensitive; ADR-005 footnotes that API deviations were discovered during implementation (Session 20 notes).", "why_it_matters": "An unbounded version pin means `pip install` on a new machine six months from now will pull a newer Inspect AI version that may break the scorer API or Dataset schema. There is no documented procedure to handle Inspect AI upgrades, meaning regressions will appear as mysterious eval failures rather than identified dependency changes.", "suggestion": "Add a requirement: Pin Inspect AI to a tested version range (e.g., `inspect-ai>=0.3,<0.4` or a specific minor version). Document the tested version in FR1.1 acceptance criteria. Add a policy: 'Inspect AI upgrades require running full test suite before updating pin.' Note this in pyproject.toml with a comment explaining the pin rationale."}
{"type": "finding", "id": "v1-constraint-finder-006", "title": "Bedrock provider support listed as optional with no constraint on when it applies", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR1: Inspect AI Integration", "issue": "FR1.2 states 'Bedrock optionally supported for work context' but defines no constraint on when Bedrock is used vs the direct Anthropic API. The Makefile MODEL defaults to `anthropic/claude-sonnet-4-5` (direct API). Bedrock uses a different model identifier format (`bedrock/anthropic.claude-sonnet-4-5-20251001-v2:0`). Switching providers requires changing the MODEL variable, but no documentation of the Bedrock model ID, required IAM permissions, or region configuration exists in the requirements.", "why_it_matters": "A developer on the work machine who runs `make eval` without setting MODEL will hit the direct Anthropic API using personal credentials, violating the work/personal key separation in NFR1.2. There is no guard or documentation to prevent this.", "suggestion": "Add a requirement: Document exact Bedrock model IDs for all three Claude tiers (Sonnet, Opus, Haiku). Add an NFR: 'Work context must use MODEL=bedrock/... to avoid personal key usage. Makefile must warn if ANTHROPIC_API_KEY is set in a work context.' Add a .env.example with the expected environment variable pattern for each context."}
{"type": "finding", "id": "v1-constraint-finder-007", "title": "Ground truth dataset size constraint (15 findings minimum) has no upper bound or staleness policy", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR0: Ground Truth Validation (Prerequisite)", "issue": "FR0 sets a minimum of 15 confirmed real flaws (68% validation rate from 22 Critical). No upper bound is set. No constraint exists on how many findings are too few for statistical reliability. More critically, there is no requirement for when ground truth becomes stale. The design doc notes 'ground truth is living' and warns about hash drift, but the requirements document does not make this a documented constraint with a trigger condition or maximum age.", "why_it_matters": "A ground truth dataset of 15 findings with N=22 total has wide confidence intervals. The regression threshold of 10% (FR6.2) is statistically unreliable at this sample size — a single finding's detection status changes the recall by 6.7%. No staleness policy means the dataset will silently become stale as the design doc evolves, producing false regressions.", "suggestion": "Add to FR0: 'Minimum 15 confirmed findings required; below this threshold, evals produce unreliable metrics.' Add a staleness constraint: 'Ground truth must be refreshed when design_doc_hash in metadata.json diverges from current doc hash. Stale ground truth blocks baseline updates but permits read-only eval runs with warning.' Add an NFR: 'Statistical note — N=15 to N=22 is insufficient for p<0.05 significance testing. Regression threshold is heuristic, not statistical.'"}
{"type": "finding", "id": "v1-constraint-finder-008", "title": "NFR numbering collision makes requirements ambiguous", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "Non-Functional Requirements", "issue": "The requirements document contains a structural numbering error: NFR1 is used for both API Key Security and Cost Tracking. NFR1.1 and NFR1.2 appear twice under different sections (API Key Security and Cost Tracking). NFR2, NFR3, NFR4, NFR5 are defined as section headers but internally use NFR1.x, NFR2.x, NFR3.x, NFR4.x, NFR5.x numbering that does not match the section numbers (e.g., NFR3 section contains NFR2.1). This creates implementation ambiguity: a developer referencing NFR1.2 cannot determine which requirement is meant without reading the full context.", "why_it_matters": "Cross-references to NFR numbers in other documents (ADR-005, design doc) will point to the wrong requirement. When findings are filed against 'NFR1.2' in a code review or issue tracker, the correct requirement cannot be identified without full document context. The review summary also uses these IDs and will reference the wrong requirements.", "suggestion": "Renumber NFR sections and internal IDs consistently: NFR1.x for API Key Security, NFR2.x for Cost Tracking, NFR3.x for Model Tiering, NFR4.x for Batch API, NFR5.x for Development Velocity, NFR6.x for Reproducibility. Fix internal requirement IDs to match their parent section numbers. Verify cross-references in ADR-005 and design doc are updated."}
{"type": "finding", "id": "v1-constraint-finder-009", "title": "Ablation test circular dependency: ground truth used to establish baseline that ablation tests validate against", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR4: Ablation Tests (Negative Testing)", "issue": "FR4.1 requires: (1) baseline X established from first eval run (FR6.1), (2) ablation drops below X-50%. FR6.1 requires: baseline stored after first passing run. FR2.2 requires: detection rate >=90% is a passing threshold. The requirement does not state that the ground truth dataset must be validated BEFORE the baseline is established. If ground truth contains false positives from v3 (the circular dependency identified in FR0), the baseline measures 'how often does the skill reproduce v3 outputs,' not 'how often does it detect real flaws.' Ablation tests against this baseline prove skill content contributes to v3 output reproduction, not flaw detection.", "why_it_matters": "An eval framework that validates skill behavior against unvalidated v3 outputs will pass ablation tests even if the skill detects zero real design flaws. This creates a false sense of validation that blocks discovery of the real problem.", "suggestion": "Add an ordering constraint to FR4.1 and FR6.1: 'Ground truth validation (FR0) must be complete and documented before baseline establishment (FR6.1). Baseline established against unvalidated ground truth is invalid. FR4 ablation tests require FR0 completion as a prerequisite.' Make this a gating dependency explicit in the MVP Scope Summary."}
{"type": "finding", "id": "v1-constraint-finder-010", "title": "Skill loading path constraint not documented — breaks on non-standard directory layouts", "severity": "Important", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR1: Inspect AI Integration", "issue": "The skill loader (evals/utils/skill_loader.py) resolves skills relative to `Path(__file__).parent.parent.parent / 'skills'`. This hardcodes the assumption that the parallax repo has a `skills/` directory at root. The design doc's repo structure shows `skills/parallax:requirements/SKILL.md` but the actual directory name `parallax:requirements` contains a colon, which is illegal on Windows filesystems and may cause issues in some CI environments. The requirements document does not state that Windows is unsupported or that the skills directory structure is a fixed constraint.", "why_it_matters": "Running evals in a Windows CI environment or on a machine where the repo is checked out with a different structure will fail with FileNotFoundError. The colon in `parallax:requirements` is a known filesystem constraint that is undocumented in the requirements.", "suggestion": "Add to FR1.1: 'Eval framework targets macOS/Linux only. Windows is not supported due to colon in skills directory name (parallax:requirements). CI must run on Ubuntu.' Alternatively, add a requirement to rename the skill directory to use a hyphen (parallax-requirements) and update all references. Document the platform constraint explicitly."}
{"type": "finding", "id": "v1-constraint-finder-011", "title": "Dataset size limit for git storage is undefined — 1MB threshold lacks basis", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "NFR6: Reproducibility", "issue": "NFR5.2 states datasets go in git 'if datasets <1MB' or externally with hash references 'if datasets >1MB'. No rationale for 1MB is provided. JSONL findings are small (each ~500 bytes), so 15-87 findings is ~8-44KB — well within the threshold. But the requirements do not state what 'externally' means: S3, Git LFS, a hash in a sidecar file? The fallback path is completely undefined.", "why_it_matters": "If dataset size grows beyond 1MB (e.g., adding full design doc text to each sample), the external storage path has no defined implementation. A developer hitting this threshold will make an ad-hoc decision that may not be reproducible across machines.", "suggestion": "Replace the 1MB threshold with a concrete rule: 'Datasets stored in git via LFS if binary blobs exist, otherwise inline in git as JSONL (<10MB limit). External storage (S3) deferred to post-MVP. For MVP, all datasets are JSONL and remain well under 1MB.' Remove the hash-reference option as it is not implemented."}
{"type": "finding", "id": "v1-constraint-finder-012", "title": "Inter-rater agreement requirement applies only if multiple reviewers — creates single-reviewer loophole", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR0: Ground Truth Validation (Prerequisite)", "issue": "FR0 states 'If multiple reviewers: inter-rater agreement measured (Cohen's kappa >=0.6)'. This is a conditional requirement. With a single reviewer (the documented default is validator: 'nic'), the kappa constraint never triggers. A single human reviewer classifying 22 findings with no calibration check has no quality constraint beyond self-assessment. The requirement provides no fallback quality check for the single-reviewer case.", "why_it_matters": "If the single reviewer misclassifies ambiguous findings as real flaws (or vice versa), no mechanism catches this. The entire eval framework quality depends on one human's judgment with no validation layer. This is the central risk identified across all five reviewers in the prior review summary.", "suggestion": "Add to FR0: 'For single-reviewer validation, document classification rationale for each finding (not just the verdict). Ambiguous classifications require written justification. Reviewer must complete two passes with a minimum 24-hour gap between passes to reduce anchoring bias. Agreement between passes must be >=90% (self-consistency check).'"}
{"type": "finding", "id": "v1-constraint-finder-013", "title": "Inspect View web UI port and network binding constraints not specified", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "NFR5: Development Velocity", "issue": "NFR4.2 requires Inspect View accessible via `inspect view` command. The design doc mentions `localhost:5000` in the validation UI context but does not specify whether Inspect View uses the same port or a different one, whether it conflicts with other local services (e.g., Flask validate_findings.py also runs on localhost:5000 per the design doc), or whether it works through corporate proxies or VPNs.", "why_it_matters": "Two services (Flask validation UI and Inspect View) both targeting localhost:5000 will cause a port conflict. A developer running `make validate` followed by `make view` will see one service fail to bind. This is a silent failure — no error message makes the conflict obvious.", "suggestion": "Add to NFR4.2: 'Inspect View runs on localhost:7575 (Inspect AI default). validate_findings.py Flask server runs on localhost:5000. Document both ports in README-evals.md. Add a note that both services cannot run simultaneously on the same port — run validate and eval sequentially, not in parallel.'"}
{"type": "finding", "id": "v1-constraint-finder-014", "title": "Baseline storage mechanism has no locking constraint — concurrent runs will corrupt baseline", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR6: Baseline Metrics and Regression Detection", "issue": "FR6.1 stores baseline via `make baseline`, which does `cp $(ls -t logs/*.json | head -1) evals/baselines/v3_critical_baseline.json`. There is no locking mechanism, no requirement that the eval run was passing before baseline is stored, and no protection against overwriting a good baseline with a failing run. The Makefile does not validate that the source log file is a passing run before copying it.", "why_it_matters": "A developer who runs `make eval` (which fails), then `make baseline` to 'reset' it, will store a failing run as the baseline. Subsequent regression detection will compare against the failed baseline, producing misleading PASS results for genuinely regressed runs.", "suggestion": "Add to FR6.1: 'Baseline update requires the stored run to have passed all threshold checks (recall >=90%, precision >=80%). The compare_to_baseline.py script must validate that the baseline file contains a passing run before accepting it. Add a --force flag for intentional baseline resets with explicit confirmation prompt.'"}
{"type": "finding", "id": "v1-constraint-finder-015", "title": "Fuzzy matching threshold (80% title similarity) is undocumented as a tunable constraint", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR2: Custom Scorers", "issue": "The scorer implementation uses 80% title similarity for fuzzy matching (scorers/severity_scorer.py line 36). The requirements document does not state this threshold as a requirement, nor does it document the range of acceptable values or the cost of tuning it wrong. A threshold that is too high produces false negatives (missed matches); too low produces false positives (incorrect matches counted as correct). The design doc notes '80% is a starting hypothesis — tune after first eval run' but this tuning process has no documented constraint.", "why_it_matters": "If the fuzzy match threshold is the wrong value for the actual findings dataset, the reported recall and precision metrics will be wrong. A developer who tunes the threshold post-hoc to make the numbers look better is gaming the eval without a documented constraint preventing it.", "suggestion": "Add to FR2.2: 'Fuzzy matching threshold (default 0.80) must be documented in scorer configuration and frozen before baseline is established. Threshold changes after baseline creation require baseline invalidation and re-establishment. Threshold tuning rationale must be documented in ADR or commit message.'"}
{"type": "finding", "id": "v1-constraint-finder-999", "title": "Blind spot check: Constraint Finder perspective", "severity": "Minor", "phase": {"primary": "calibrate", "contributing": null}, "section": "Meta", "issue": "This review focused on documented and inferrable technical constraints. It may have underweighted: (1) regulatory or compliance constraints beyond API key separation (GDPR if design docs contain PII, export controls on AI eval outputs); (2) rate limit constraints on the Anthropic API — 22 parallel eval samples may hit RPM limits on Sonnet, which would extend the <5-minute eval target without triggering a visible error; (3) organizational constraints on publishing eval results publicly (the repo is public — eval logs committed to git expose skill weaknesses); (4) Inspect AI's own network connectivity requirement — runs need outbound HTTPS to Anthropic API, which may be blocked in corporate network contexts where Bedrock is required.", "why_it_matters": "Blind spot awareness helps catch gaps in the review process. Rate limits and network constraints in particular are easy to discover only at runtime, after setup is complete.", "suggestion": "Consider: (1) Add an NFR for API rate limits — document Sonnet RPM and TPM limits, add retry logic requirement. (2) Add a note on log confidentiality — eval logs in the public repo expose finding IDs and design doc content; add logs/ to .gitignore and document this as a security constraint. (3) Check whether corporate proxy config is needed for Bedrock vs direct API access."}
