{"type":"finding","id":"v1-constraint-finder-001","title":"Python environment and dependency management constraints unstated","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"FR1: Inspect AI Integration","issue":"Requirements mandate Inspect AI installation (FR1.1) but don't specify Python version requirements, virtual environment strategy, or dependency conflict resolution. Inspect AI requires Python 3.10+ and has 20+ dependencies that could conflict with existing parallax tooling.","why_it_matters":"Missing environment constraints block implementation. Developer installs Inspect AI globally, breaks existing tools, or faces hours debugging version conflicts. Parallax may already use Python tools (pattern extraction uses Claude API via Python SDKs?), creating hidden conflicts.","suggestion":"Add NFR section 'Development Environment': Python 3.11+ required, pyenv for version management, dedicated venv for Inspect AI, requirements.txt with pinned versions, document any known conflicts with existing parallax Python tooling."}
{"type":"finding","id":"v1-constraint-finder-002","title":"API key security and multi-environment access patterns undefined","severity":"Critical","phase":{"primary":"calibrate","contributing":"design"},"section":"FR1.2: Anthropic provider configuration","issue":"FR1.2 specifies ANTHROPIC_API_KEY environment variable and optional Bedrock support but doesn't address: key rotation policies, separation between personal/work contexts, how evals access keys in CI/CD, whether keys are per-developer or shared team keys, what happens when work Bedrock access expires.","why_it_matters":"Insecure key handling violates corporate security policies. Work/personal key mixing creates billing/audit issues. CI/CD without key access blocks automation. Bedrock access expiration breaks evals mid-development with no fallback plan.","suggestion":"Add security constraints: personal API keys for local dev (stored in ~/.bashrc, never in git), Bedrock IAM roles for work context (document setup), CI/CD uses GitHub Actions secrets, key rotation procedure documented, fallback from Bedrock to direct API when access expires."}
{"type":"finding","id":"v1-constraint-finder-003","title":"Dataset size and token consumption constraints missing from MVP scope","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"FR3: Test Datasets","issue":"FR3.1 converts 'most comprehensive ground truth (87 findings)' but FR3.1 only includes 22 Critical findings for MVP. NFR1.2 targets <$0.50 per eval run but doesn't specify how many samples this covers or input token counts. Open Question 5 asks if 22 findings is sufficient but provides no basis for evaluation.","why_it_matters":"Token limits block feasibility. 22 design doc + finding pairs could exceed context windows if docs are large (parallax design doc is ~15k tokens). Cost estimates without token projections are guesses. Insufficient sample size (n=22) has low statistical power for detecting <10% performance changes (NFR6.2 regression threshold).","suggestion":"Add constraints section: Estimate input tokens per sample (design doc ~15k + finding ~500 = ~15.5k tokens/sample × 22 samples = ~341k input tokens = $1.02 for Sonnet input alone, exceeds $0.50 target). Specify context window limits (200k for Sonnet 4.5). Calculate minimum sample size for 80% statistical power at 10% effect size (~78 samples, not 22). Either raise cost target, reduce sample size for quick validation, or chunk large design docs."}
{"type":"finding","id":"v1-constraint-finder-004","title":"Manual validation workflow scalability constraint ignored","severity":"Important","phase":{"primary":"calibrate","contributing":"design"},"section":"FR2.3: Manual validation workflow","issue":"FR2.3 and FR3.3 mandate manual validation for <95% confidence findings via 'subagent reviews, user accepts/rejects'. No time estimate provided. If 30% of findings are <95% confidence (reasonable for borderline Important/Minor), manual review of 22 Critical + future 47 Important + 18 Minor = 87 findings × 30% = ~26 manual reviews. At 3 minutes per review = 78 minutes of human time per dataset.","why_it_matters":"Manual validation becomes implementation bottleneck. 78 minutes blocks 'local validation in <5 minutes' goal (NFR4.1). Expanding datasets (FR3.2: requirements-light, pattern-extraction, parallax-review) multiplies human time. User must be available during eval runs, blocking automation (NFR3.2 batch API deferred but manual gates make it impossible anyway).","suggestion":"Add time constraints: Estimate 2-5 minutes per manual validation. For MVP (22 Critical findings), assume 15% low confidence = 3 manual reviews = ~10 minutes human time acceptable. For post-MVP expansion to 87 findings, 26 manual reviews = 78+ minutes unacceptable. Require: batch manual validation UI (review 10 findings in one session), confidence threshold tuning (raise to 98% to reduce manual load), or LLM-as-judge pre-filter (Sonnet validates, only ambiguous cases escalate to human)."}
{"type":"finding","id":"v1-constraint-finder-005","title":"Baseline metric stability constraints undefined for evolving skills","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"FR6: Baseline Metrics and Regression Detection","issue":"FR6.1 establishes baseline from 'first eval run' and FR6.2 detects regressions when 'detection rate drops >10%'. But skills evolve (prompt changes, persona additions, new reviewer types). When do baselines get invalidated? If skill improves detection by 30%, new baseline becomes reference, but now you can't detect regressions back to original skill. No baseline versioning strategy.","why_it_matters":"Baseline staleness causes false negatives (skill regresses but still above old baseline). Baseline updates without versioning lose historical comparison (can't A/B test new prompts vs last month's version). Open Question 3 asks 'how often to recalculate baselines' but provides no decision criteria.","suggestion":"Add baseline versioning constraints: Baselines are immutable per skill version (git commit hash). Store baselines in 'evals/baselines/<skill-version>/<dataset>.json'. Regression detection compares to baseline from same skill lineage (not cross-version). When skill changes significantly (>30% detection rate shift), create new baseline branch, flag for manual review. Support 'compare to baseline' (regression) and 'compare to version' (A/B test) as separate operations."}
{"type":"finding","id":"v1-constraint-finder-006","title":"Claude API rate limits and quota exhaustion not addressed","severity":"Important","phase":{"primary":"calibrate","contributing":"plan"},"section":"NFR1: Cost Tracking","issue":"NFR1.1 tracks cost per eval but doesn't address Claude API rate limits. Anthropic enforces tier-based rate limits (e.g., 50 requests/minute for tier 1, 1000 for tier 3). Running 22 samples in parallel could hit rate limits. Bedrock has separate quotas per region. No retry/backoff strategy specified.","why_it_matters":"Rate limit violations block eval completion. NFR4.1 targets <5 minute eval runs, but rate limiting forces sequential execution (22 samples × 15 seconds each = 5.5 minutes minimum, exceeds target). Bedrock quota exhaustion (work context) fails evals with no automatic fallback to personal API key. Mid-eval failures waste partial results and cost.","suggestion":"Add rate limit constraints: Check user's Anthropic API tier, calculate max parallel requests (tier 1 = 50/min = ~16 parallel safe with 20% buffer). Implement exponential backoff with Inspect AI built-in retry logic. For Bedrock, document quota limits per region, implement fallback to direct Anthropic API on quota exhaustion. Add NFR: 'Evals gracefully degrade from parallel to sequential if rate limited, log slowdown reason.'"}
{"type":"finding","id":"v1-constraint-finder-007","title":"Disk space constraints for eval logs and datasets unspecified","severity":"Minor","phase":{"primary":"calibrate","contributing":null},"section":"FR1.3: EvalLog artifacts","issue":"FR1.3 mandates EvalLog JSON artifacts per eval run capturing inputs, outputs, scores, token usage. Each eval run on 22 samples could generate ~5MB logs (design docs + findings + outputs + metadata). Post-MVP expansion to 87 findings = ~20MB per run. NFR5 mandates datasets in git if <1MB, but v3 JSONL findings alone are ~150KB, design docs push this >1MB. No retention policy or cleanup strategy.","why_it_matters":"Disk space exhaustion on local dev machines after 100+ eval runs (100 × 5MB = 500MB). Git repo bloat if large datasets committed (<1MB threshold in NFR5.2 is arbitrary). Inspect View UI slows down with 500MB+ log directories. CI/CD runners run out of disk space without cleanup.","suggestion":"Add storage constraints: Eval logs stored in 'evals/logs/' (gitignored), 30-day retention policy with auto-cleanup script. Datasets >1MB stored in 'datasets/' (gitignored), referenced by content hash in git metadata file. Inspect View log directory configurable, defaults to tmpfs for speed. Document: 'MVP requires ~100MB disk space for logs, ~50MB for datasets, ~500MB cumulative after 100 eval runs (cleanup old logs).'"}
{"type":"finding","id":"v1-constraint-finder-008","title":"Statistical significance constraints absent from regression detection","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"FR6.2: Regression detection","issue":"FR6.2 detects regression if 'detection rate drops >10%' but doesn't address sample size effects on statistical significance. With 22 Critical findings, detecting 2 fewer findings (9% drop) vs 3 fewer (14% drop) determines pass/fail, but confidence intervals overlap heavily. No p-value threshold, confidence intervals, or minimum detectable effect size specified.","why_it_matters":"False positives block valid skill changes (noise mistaken for regression). False negatives ship broken skills (real regression under detection threshold). Small sample size (n=22) means 10% threshold has wide confidence intervals (~±8% at 95% CI), making most changes statistically indistinguishable. Post-MVP expansion helps, but FR3.2 doesn't specify target sample sizes per dataset.","suggestion":"Add statistical constraints: Report detection rate with 95% confidence intervals. Regression flagged only if upper bound of new rate < lower bound of baseline rate (non-overlapping CIs). For n=22, 10% drop requires ~14% observed drop to be significant. Either: (1) raise sample size to 80+ findings for 10% sensitivity, (2) use 20% regression threshold for n=22, or (3) combine datasets (v3 + requirements-light) for larger n. Document minimum detectable effect size per dataset."}
{"type":"finding","id":"v1-constraint-finder-009","title":"Ablation test ground truth dependency creates circular validation","severity":"Critical","phase":{"primary":"calibrate","contributing":"design"},"section":"FR4: Ablation Tests","issue":"FR4.1 tests 'dropping skill content degrades detection rate >50%' by comparing ablated skill output to baseline. But baseline detection rate is established in FR6.1 from the same v3 ground truth that may contain false positives (FR2.3 acknowledges <95% confidence findings exist). If ground truth has 30% false positives, ablation test might pass even if skill content is useless (both baseline and ablation detect the same 70% true positives).","why_it_matters":"Ablation tests validate skill content matters, but only if ground truth is accurate. False positives in ground truth create circular validation: bad skill creates bad baseline, ablation compares bad outputs to bad baseline, declares success. User Q&A specified 'manual validation gate' (FR3.3) but it happens after baseline establishment (FR6.1), not before. Contaminated ground truth invalidates entire eval framework.","suggestion":"Reorder MVP phases: (1) Manual validation of v3 Critical findings first (create clean ground truth), (2) then establish baseline with validated ground truth, (3) then run ablation tests. Add FR3.3 acceptance criteria: 'Ground truth validation completes before baseline establishment (FR6.1).' Specify ground truth quality threshold: 'Manual validation rejects >5% of findings → ground truth quality insufficient, expand dataset before establishing baseline.'"}
{"type":"finding","id":"v1-constraint-finder-010","title":"Multi-model comparison feasibility constrained by model availability and API access","severity":"Important","phase":{"primary":"calibrate","contributing":"plan"},"section":"FR8: Multi-Model Comparison","issue":"FR8.1 mandates evals on 'claude-sonnet-4-5, claude-opus-4-6, claude-haiku-4-5' but doesn't verify these models are accessible via user's API keys. Opus 4.6 may have restricted access (closed beta). Bedrock model availability differs from direct API (regional constraints, version lags). Haiku 4.5 doesn't exist yet (current is Haiku 3.5, Haiku 4 unreleased as of Feb 2026).","why_it_matters":"Model unavailability blocks multi-model comparison entirely. FR8 deferred to post-MVP but Job 5 ('validate multi-model portability') is a validated user need. Specifying nonexistent models (Haiku 4.5) in acceptance criteria creates unachievable requirements. Bedrock version lag means work context can't reproduce personal API results.","suggestion":"Add model availability constraints: Verify accessible models via user's API tier (direct Anthropic vs Bedrock). Update FR8.1 to 'latest available versions of Sonnet, Opus, Haiku' (not specific version numbers). Document Bedrock model availability lags 1-3 months behind direct API. Add fallback: 'If Opus unavailable, use Sonnet as quality baseline, document limitation.' Check current Haiku version (3.5, not 4.5) and update references."}
{"type":"finding","id":"v1-constraint-finder-011","title":"Development time estimate missing for MVP Phase 1","severity":"Important","phase":{"primary":"calibrate","contributing":"plan"},"section":"MVP Scope Summary","issue":"Phase 1 lists 6 implementation tasks with 'Target: 1 week' but no breakdown of time per task or dependency sequencing. Task 3 ('Convert v3 Critical findings to Dataset format with manual validation gate') alone could take 1-2 days if 22 findings require manual review (FR2.3, FR3.3). No buffer for learning Inspect AI APIs, debugging integration issues, or rework from failed ablation tests.","why_it_matters":"Underestimated timelines block planning. '1 week' assumes full-time focus (40 hours), but parallax is side project (~5-10 hours/week from session history). 1 week calendar time = 5-10 hours actual work = 1-2 tasks max. Unsequenced tasks hide blockers (can't run eval until dataset converted, can't test ablation until baseline established). Failed time estimates create pressure to cut corners (skip manual validation, ship with contaminated ground truth).","suggestion":"Add time constraints to Phase 1: Task 1 (install + config) = 2 hours, Task 2 (severity scorer) = 4 hours, Task 3 (dataset conversion + manual validation) = 8 hours, Task 4 (first eval run) = 1 hour, Task 5 (ablation test) = 3 hours, Task 6 (validation) = 2 hours. Total = 20 hours = 2-4 weeks calendar time at 5-10 hrs/week. Sequence: 1→2→3→4→5→6 (strict dependencies). Add 50% buffer for learning curve and rework = 30 hours = 3-6 weeks realistic estimate."}
{"type":"finding","id":"v1-constraint-finder-012","title":"Eval reproducibility constraints conflict with API non-determinism","severity":"Minor","phase":{"primary":"calibrate","contributing":"design"},"section":"NFR5: Reproducibility","issue":"NFR5.1 mandates reproducibility via 'git commit hash, dataset, API key' but Claude API is non-deterministic by default (temperature=1.0). Same eval on same dataset with same skill version will produce different results. NFR5 doesn't specify temperature=0 or random seed fixing. FR6.2 regression detection compares detection rates, but rate variance from non-determinism could mask real regressions or create false alarms.","why_it_matters":"Non-reproducible evals make debugging impossible (can't isolate whether skill change or API variance caused difference). FR6.2's 10% regression threshold could be entirely noise with high-temperature sampling. Multiple eval runs required to establish confidence intervals, multiplying cost (NFR1.2 <$0.50 target assumes single run). Post-MVP CI/CD integration (Phase 3) requires deterministic pass/fail, not probabilistic.","suggestion":"Add reproducibility constraints: Set temperature=0 for all eval runs (deterministic sampling). Document: 'Evals are deterministic within Claude API version, but API updates may change outputs. Pin @claude-sonnet-4-5-20250929 version string (not alias) for strict reproducibility.' For non-deterministic temperature>0, specify: 'Run eval N=5 times, report mean detection rate ± std dev, regression detected only if difference exceeds 2× std dev.' Update NFR1.2 cost target to account for multiple runs if non-deterministic."}
{"type":"finding","id":"v1-constraint-finder-013","title":"Planted flaw test dataset creation constraints undefined","severity":"Important","phase":{"primary":"calibrate","contributing":"design"},"section":"FR5.1: Planted flaw detection tests","issue":"FR5.1 requires 'design docs with planted flaws' but doesn't specify who creates them (user manually? LLM-generated? Modified from real docs?), how to ensure flaws are realistic (not strawman obvious issues), how many variants per flaw type, or validation that unmodified skills would miss these flaws (otherwise test is circular).","why_it_matters":"Strawman planted flaws (e.g., 'TODO: add authentication') pass tests but don't validate real-world performance. User-created flaws require domain expertise and 10+ hours per test case. LLM-generated flaws may be unrealistic or already detectable by baseline skills. No validation that planted flaws are actually missed by current skills means test could pass trivially. FR5.1 acceptance criteria targets '>70% detection rate' but baseline might already achieve 80%, making test meaningless.","suggestion":"Add planted flaw constraints: (1) Use historical false negatives from v3 review as planted flaws (real issues reviewers missed). (2) Modify real design docs to remove sections fixing known issues (regression to earlier iteration). (3) Validate planted flaws: run current skill, confirm <30% detection rate, then test improved skill targets >70%. (4) Document per-flaw-type: architectural gaps (5 variants), assumption violations (3 variants), edge cases (5 variants). Time estimate: 4 hours per flaw type × 3 types = 12 hours dataset creation (add to Phase 2 scope)."}
{"type":"finding","id":"v1-constraint-finder-014","title":"MacOS-specific development environment assumed without cross-platform constraints","severity":"Minor","phase":{"primary":"calibrate","contributing":null},"section":"NFR4.1: Local eval runs","issue":"NFR4.1 specifies '<5 minutes on MacBook Pro' but doesn't address Windows/Linux compatibility. Inspect AI is Python-based (cross-platform), but file paths, shell scripts, git hooks in parallax repo use Unix conventions. No Windows testing planned. Session history shows development on MacOS only (Darwin 25.2.0 from env context).","why_it_matters":"Windows users can't run evals without modification (path separators, shell scripts fail). Eval framework intended for 'work contexts' (ADR-005) where Windows common. Post-MVP CI/CD (Phase 3) runs on Linux (GitHub Actions), requiring separate testing. Cross-platform issues discovered late block adoption.","suggestion":"Add platform constraints: 'MVP targets MacOS and Linux. Windows compatibility post-MVP (requires pathlib for cross-platform paths, PowerShell script equivalents, WSL2 testing).' Document known limitations: 'Pre-commit hook requires Unix shell (use WSL2 on Windows).' Update NFR4.1: '<5 minutes on MacBook Pro M1/M2 or equivalent Linux workstation (8+ GB RAM, 4+ cores).' Add hardware requirements to FR1.1 acceptance criteria."}
{"type":"finding","id":"v1-constraint-finder-015","title":"Prompt caching cost optimization constraints conflict with eval reproducibility","severity":"Minor","phase":{"primary":"calibrate","contributing":"design"},"section":"NFR1: Cost Tracking","issue":"ADR-005 specifies 'prompt caching for repeated system prompts (90% input cost reduction)' but NFR1 doesn't integrate this. Prompt caching requires cache-control headers and has 5-minute TTL. Eval runs on 22 samples could reuse cached prompts if sequential, but parallel execution (NFR4.1 <5 minute target) hits cache misses. NFR5 reproducibility requires fixed inputs, but prompt caching introduces temporal dependency (first run vs cached run have different costs).","why_it_matters":"Cost estimates without prompt caching are 10× too high (NFR1.2 <$0.50 target infeasible). Cost tracking with prompt caching is non-deterministic (first run = $5, cached run = $0.50, exceeds budget). Parallel execution for speed vs sequential for cost creates tradeoff not documented. Prompt caching TTL means evals >5 minutes apart can't share cache, blocking batch workflows.","suggestion":"Add prompt caching constraints: (1) Use Inspect AI built-in caching support (sends cache-control headers automatically). (2) Structure eval prompts with cacheable system prompt prefix (skill content) + variable suffix (per-sample design doc). (3) Run samples sequentially within 5-minute cache TTL to maximize cache hits. (4) Update NFR1.2: '<$0.50 per eval run with prompt caching, $2-5 first run without cache.' (5) Document: 'First eval run pays full cost, subsequent runs within 5 minutes cost 10× less. Budget for cache-cold runs during development.'"}
{"type":"finding","id":"v1-constraint-finder-016","title":"Bedrock regional constraints and failover strategy undefined","severity":"Important","phase":{"primary":"calibrate","contributing":"plan"},"section":"FR1.2: Anthropic provider configuration","issue":"FR1.2 mentions 'Bedrock provider optionally supported for work context' but doesn't specify regions (us-east-1 only? multi-region?), failover behavior when region quota exhausted, latency implications (Bedrock adds ~200ms overhead vs direct API), or credential management (IAM roles vs API keys).","why_it_matters":"Work context evals fail when Bedrock us-east-1 quota exhausted with no automatic failover to us-west-2 or direct API. Bedrock regional outages block evals (happened Oct 2025, 4-hour outage). Higher latency pushes NFR4.1 <5 minute target out of reach (22 samples × 200ms extra = 4.4 seconds overhead, tight margin). IAM role expiration fails evals mid-run with cryptic errors.","suggestion":"Add Bedrock constraints: (1) Primary region: us-east-1 (lowest latency). (2) Failover: Automatic retry in us-west-2 if us-east-1 quota exhausted. (3) Fallback: If both Bedrock regions fail, fallback to direct Anthropic API (requires ANTHROPIC_API_KEY env var as backup). (4) Document latency impact: 'Bedrock adds ~200ms per request, 22 samples = ~4.4s overhead, budget 5-7 minutes for Bedrock evals.' (5) IAM role setup guide with work IT contact info. (6) Add health check: 'Test Bedrock access before running full eval suite (single sample validation run).'"}
{"type":"finding","id":"v1-constraint-finder-017","title":"Legal and compliance constraints for storing design artifacts in evals","severity":"Minor","phase":{"primary":"calibrate","contributing":"plan"},"section":"FR3: Test Datasets","issue":"FR3 stores design docs and findings in datasets/ directory, NFR5.2 commits datasets <1MB to git (public repo). No mention of IP constraints, confidentiality requirements, or data retention policies. Work context design docs may contain proprietary information, customer names, or security details that can't be in public git repos.","why_it_matters":"Accidental commit of proprietary design docs to public parallax repo violates corporate policy, creates legal liability. Customer-identifying information in datasets violates privacy policies. Eval framework unusable in work contexts if datasets can't be stored (blocks Job 4 and Job 5 outcomes). Post-MVP CI/CD (Phase 3) publishes eval results, potentially leaking sensitive findings.","suggestion":"Add data handling constraints: (1) Test datasets use only public or anonymized design docs (parallax self-review, open-source project designs). (2) Work context: datasets stored outside git (local only), referenced by content hash, eval results sanitized before git commit. (3) Document: 'Datasets may contain confidential design content. Never commit work-related design docs to public repo. Use sanitized summaries or synthetic examples for CI/CD validation.' (4) Add dataset classification: public (committable), internal (local only), confidential (work-only, not in parallax repo). (5) Update NFR5.2: 'Public datasets in git, internal/confidential datasets gitignored with hash references.'"}
{"type":"finding","id":"v1-constraint-finder-999","title":"Blind spot check: Constraint Finder perspective","severity":"Minor","phase":{"primary":"calibrate","contributing":null},"section":"Meta","issue":"Focusing on constraints may have caused me to: (1) Assume Python expertise (FR1 install complexity might be overstated), (2) Over-index on statistical rigor (FR6/FR8 significance testing may be overkill for MVP), (3) Miss implicit constraints from domain knowledge (e.g., Claude API tier limits are well-documented, user likely knows them), (4) Prioritize technical feasibility over user workflow fit (manual validation gates may be acceptable friction for data quality), (5) Assume worst-case scenarios (rate limits, quota exhaustion) that may not occur in practice.","why_it_matters":"Over-constraining the design increases implementation complexity and delays MVP delivery. User explicitly prioritizes 'build to learn' and YAGNI (CLAUDE.md). Some findings may flag theoretical risks that don't materialize in practice. Statistical rigor (finding 008) might be premature for MVP focused on integration validation, not production deployment.","suggestion":"Triage findings by MVP criticality: (1) Blockers (001, 002, 009) address before starting implementation. (2) Important but deferrable (003, 004, 005, 013) document as known limitations, revisit in Phase 2. (3) Nice-to-have (007, 012, 014, 017) defer to post-MVP unless user flags as concern. Re-read requirements with 'what's the simplest thing that could work?' lens—some constraints may be over-engineered. Validate assumptions: Does user actually need statistical significance (008) or just directional signal? Is manual validation gate (004) acceptable if it's batched and only happens once per dataset?"}
