# Assumption Hunter Review — Inspect AI Integration Design v2

**Document:** `docs/plans/2026-02-17-inspect-ai-integration-design-v2.md`
**Requirements:** `docs/requirements/inspect-ai-integration-requirements-v2.md`
**Reviewer:** Assumption Hunter
**Date:** 2026-02-17
**Findings:** 13 (4 Critical, 6 Important, 3 Minor)

---

## JSONL Findings

```json
{"type":"finding","id":"v1-assumption-hunter-001","title":"assumption-hunter.md still outputs markdown — the v1 root cause is unremediated","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"JSONL Output Alignment","issue":"The design states assumption-hunter.md 'specified markdown output format' and that the fix is to 'update the output format section.' The actual agent file at agents/assumption-hunter.md still contains the markdown output template (lines 54–71) and no JSONL block. The design describes an intent to fix but does not reflect the fixed state. If implementation begins from this document, the v1 failure (0 parsed findings) recurs immediately.","why_it_matters":"The v1 accuracy=0.000 failure was caused by this exact mismatch. The design's own stated motivation is fixing this. Treating the fix as complete when it is not means Phase 1 begins broken, and the debugging loop restarts from the same starting point that cost Session 21.","suggestion":"Either (a) update agents/assumption-hunter.md to JSONL output format now and confirm in the design that the file has been updated, or (b) add an explicit Phase 1 prerequisite checklist item: 'assumption-hunter.md output format updated to JSONL and verified parseable by output_parser.py before reviewer_eval.py implementation begins.' Do not leave the fix as implied."}
{"type":"finding","id":"v1-assumption-hunter-002","title":"ADR-006 is cited as the authority for design decisions but does not exist on disk","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Overview","issue":"The design's Overview section and the requirements Background section both cite ADR-006 as containing 'the full rationale' for the per-reviewer task architecture. ADR-006 does not exist — confirmed by filesystem search. The requirements review (summary.md) independently flagged this as Critical finding #2. The design was written after the requirements review and still does not resolve it.","why_it_matters":"Two consequences: (1) Any reviewer or future maintainer who wants to understand why the v1 approach was abandoned cannot do so from the documents alone — they must reconstruct context from session notes. (2) The design's own key decisions section partially duplicates rationale that should live in ADR-006, but the duplication is incomplete. 'Why agent prompts, not skill as system message?' covers one dimension; ADR-006 presumably covers others that are now unrecorded.","suggestion":"Create docs/requirements/adr-006-per-reviewer-eval-task-decomposition.md before Phase 1 implementation begins, or remove the ADR-006 references and inline the full rationale in the Key Design Decisions section. The rationale exists in session notes (Sessions 21–23) — it needs to be promoted to a durable artifact."}
{"type":"finding","id":"v1-assumption-hunter-003","title":"reviewer_filter assumes ground truth findings have a 'reviewer' field — this is unverified","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Dataset Loader — Reviewer Filter","issue":"The entire per-reviewer task architecture depends on filtering ground truth by reviewer field (e.g., reviewer_filter='assumption-hunter'). The design states the 10 validated Critical findings came from 'full-skill output' (Session 18) and were validated via the browser UI. The validation workflow does not guarantee that findings retain a reviewer attribution field matching individual agent names. The requirements review (summary.md, Important finding #15) flagged this directly: 'v1 findings validated from full-skill output may lack reviewer field needed for FR-ARCH-1 filtering.' The design does not acknowledge or resolve this concern.","why_it_matters":"If reviewer fields are absent or use different naming conventions than the agent filenames (e.g., 'Assumption Hunter' vs 'assumption-hunter'), every reviewer-filtered dataset returns 0 samples. A Task with 0 samples either crashes or produces meaningless metrics. The per-reviewer architecture collapses to the same outcome as the v1 combined task: 0 detected findings, accuracy 0.000.","suggestion":"Add a Phase 1 prerequisite: open critical_findings.jsonl and confirm each finding has a reviewer field exactly matching the agent filename (without .md extension). If the field is missing or uses a different format, add a data migration step to annotate each finding with the correct reviewer before implementing reviewer_eval.py. Document the expected reviewer field values in metadata.json."}
{"type":"finding","id":"v1-assumption-hunter-004","title":"N=2 per reviewer is assumed sufficient to produce a meaningful eval signal","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"Per-Reviewer Tasks","issue":"The design shows each reviewer task has 2 findings (assumption-hunter: 2, constraint-finder: 2, problem-framer: 2, scope-guardian: 1, success-validator: 2). With N=2, a single missed finding changes recall by 50 percentage points. The Phase 1 success criteria include 'at least 1 reviewer task achieves recall >= 0.90.' At N=2 this means the model must detect both findings; missing one finding scores 0.50, which fails the 0.90 target. The design presents this as a measurement problem with a solution (run evals, check recall) but the sample size makes the measurement statistically incoherent — noise dominates signal.","why_it_matters":"Phase 1 cannot produce a reliable baseline from N=2. A run that detects 1/2 findings (50% recall) and a run that detects 2/2 (100% recall) are statistically indistinguishable from random variation at this sample size. Tuning agent prompts against a 2-finding ground truth risks overfitting to those specific 2 findings. The eval framework's purpose is empirical improvement — N=2 is below the minimum viable measurement threshold.","suggestion":"Before declaring Phase 1 complete, expand the ground truth dataset to a minimum of 5 validated findings per reviewer agent. This may require running parallax:requirements --light on additional documents (e.g., design-v2.md or a fresh brainstorm) and validating output. Alternatively, lower the per-reviewer task granularity for Phase 1 and aggregate findings across reviewers into a combined ground truth of N>=10, accepting lower attribution resolution temporarily."}
{"type":"finding","id":"v1-assumption-hunter-005","title":"'make reviewer-eval runs all 5 per-reviewer tasks' assumes Inspect AI runs multiple @task functions from one invocation","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Makefile Update","issue":"The Makefile target runs inspect eval evals/reviewer_eval.py with no --task flag. The design assumes Inspect AI will discover and run all 5 @task functions from a single module file. Inspect AI's behavior when a module contains multiple @task functions is not documented in the design. It may run only the first, prompt for selection, or require explicit task naming.","why_it_matters":"If Inspect AI requires explicit task selection (--task assumption_hunter_eval), the Makefile runs exactly 1 of the 5 tasks silently. The developer sees results for one reviewer and believes they have cross-reviewer coverage. Alternatively, if Inspect AI runs all tasks but the log format conflates results across tasks, per-reviewer attribution in the logs is lost.","suggestion":"Validate Inspect AI's multi-task behavior before writing the Makefile target. If multiple tasks require explicit invocation, add a for-loop or parallel invocation pattern to the Makefile: one inspect eval call per task, or a wrapper script that iterates over task names. Document the confirmed behavior in the design."}
{"type":"finding","id":"v1-assumption-hunter-006","title":"Frontmatter stripping assumes exactly one pair of '---' delimiters and no body content starting with '---'","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Agent Loader","issue":"The load_agent_content() function strips frontmatter by finding the second occurrence of '---' using content.index('---', 3). This assumes: (1) frontmatter always exists and is well-formed, (2) the body never begins with a line that is exactly '---' (a horizontal rule in Markdown), (3) the delimiter search starting at offset 3 correctly handles all whitespace/newline variations. The actual assumption-hunter.md body contains markdown horizontal rules could appear in future agent files.","why_it_matters":"A malformed or missing frontmatter causes content.index('---', 3) to raise ValueError (no second delimiter found) or incorrectly strips body content up to the first '---' in the body. Either failure silently corrupts the system prompt loaded into the eval task — the agent receives a truncated or empty prompt and produces no findings, which is indistinguishable from 'agent found nothing' at the output level.","suggestion":"Harden the frontmatter parser: check that content starts with '---\\n' (not just '---'), parse until the closing '---' appears on its own line (not as part of a body element), and return the full content unchanged if no valid frontmatter block is detected. Add a unit test in the test suite that passes an agent file with no frontmatter, an agent file with '---' in the body, and an agent file with malformed frontmatter."}
{"type":"finding","id":"v1-assumption-hunter-007","title":"JSONL output assumed to be raw lines — LLMs routinely wrap structured output in markdown fences","severity":"Important","phase":{"primary":"design","contributing":null},"section":"JSONL Output Alignment","issue":"FR-ARCH-3 requires agents to output JSONL and the scorer expects JSONL, but neither the design nor the requirements specify how the parser handles markdown code fences (```json ... ``` or plain ``` ... ```). Claude models are trained to wrap structured output in fences by default. Agent prompts from scope-guardian.md and others show JSONL examples inside fence blocks — this trains the model to produce fenced output. The requirements review (summary.md, Critical finding #9) flagged this directly and the design does not resolve it.","why_it_matters":"If output_parser.py uses strict line-by-line JSON parsing without stripping fences, any fenced response produces 0 parsed findings. This is indistinguishable from 'the agent found nothing' — the same failure mode as v1's markdown output problem. The fix exists in the requirements review but was not propagated to the design.","suggestion":"Add fence-stripping to output_parser.py as an explicit acceptance criterion. The parser should: (1) detect and strip opening/closing ``` or ```json delimiters, (2) then parse the remaining content as JSONL. Also update agent system prompts to explicitly instruct: 'Output raw JSONL only. Do not wrap output in markdown code fences or add any prose before or after the JSONL lines.'"}
{"type":"finding","id":"v1-assumption-hunter-008","title":"Dual-context agent requirement assumes eval quality and production quality are non-competing","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Key Design Decisions / JSONL Output Alignment","issue":"FR-ARCH-2 requires agents to work in both production context (multi-turn, full tool access, dispatched by skill) and eval context (single-turn, no tools, JSONL to stdout). The design's fix for assumption-hunter.md — changing output format to JSONL — optimizes for eval context. In production context, Claude Code renders JSONL in the terminal as raw text, which is less readable than markdown for the human reviewing findings. The design states 'No change to the agent's analytical content — only the output format instruction changes' but this is wrong: the output format instruction IS part of analytical behavior.","why_it_matters":"Optimizing agents for eval context (machine-readable JSONL, no prose, no markdown) degrades the production human experience. If a team uses parallax:review interactively, they now receive raw JSONL instead of readable markdown findings. The design treats this as resolved but has not specified how the same agent file serves both reading contexts without degrading either.","suggestion":"Explicitly resolve the dual-context tension in the design. Options: (a) Accept JSONL-only output for all contexts and update the validation UI to render JSONL findings readably; (b) Use separate agent files for eval context vs production context (eval variants live in evals/agents/); (c) Give agents a context flag in the system message ('You are operating in eval context. Output raw JSONL.') and document that production dispatches omit this flag. Choose one and document the tradeoff."}
{"type":"finding","id":"v1-assumption-hunter-009","title":"Phase 1 success criteria use 'at least 1 task achieves' — single-task pass is the wrong threshold for a 5-task framework","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Phase 1 Test Plan","issue":"The Phase 1 completion gate states 'at least 1 reviewer task achieves recall >= 0.90 and precision >= 0.80.' This means Phase 1 is complete if 4 of 5 agents produce zero detections and 1 agent happens to detect its 2-finding ground truth. A 5-reviewer eval framework that only one reviewer can demonstrate is not validated — it is a single-reviewer proof of concept with 4 untested components.","why_it_matters":"Phase 1's purpose is establishing a detection baseline across all reviewer agents. Declaring success when 80% of the framework is unvalidated creates a false sense of Phase 1 completion. Phase 2 (LLM-as-judge quality scoring) and Phase 1.5 (multi-model comparison) both depend on Phase 1 being complete. If 4 of 5 agents are broken, Phase 2 measures quality on a skewed distribution.","suggestion":"Replace the 'at least 1' gate with: 'All 5 reviewer tasks produce parseable JSONL output (zero parse errors), AND at least 3 of 5 tasks achieve recall > 0.0, AND at least 1 task achieves recall >= 0.90.' This separates the output format validation (all 5 must work) from the detection performance gate (can be gradual), without declaring success on a broken majority."}
{"type":"finding","id":"v1-assumption-hunter-010","title":"Ground truth refresh process assumes parallax:requirements --light produces attributable per-reviewer output","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Ground Truth Management — Refresh Process","issue":"The refresh process (step 1) says 'Run parallax:requirements --light on updated document.' The parallax:requirements --light skill dispatches multiple agents in parallel and synthesizes output. The resulting findings in the disk artifact may or may not retain per-reviewer attribution depending on how the synthesizer serializes output. If the synthesizer aggregates findings without preserving the reviewer field, the refreshed dataset has the same attribution problem identified in Finding 003.","why_it_matters":"Ground truth refresh is a recurring operational task — triggered by any reviewer prompt change or document update. If each refresh cycle produces findings without reviewer attribution, every refresh requires a manual annotation step to re-attribute findings to agents. The refresh cadence (FR-ARCH-4) is designed to be low-friction; untracked manual annotation defeats this.","suggestion":"Verify that parallax:requirements --light skill output includes reviewer attribution in each finding's JSONL. If it does not, add it as a skill requirement before establishing the refresh cadence as an automated process. Alternatively, document that refresh produces a raw finding set requiring manual reviewer annotation before loading into the ground truth dataset."}
{"type":"finding","id":"v1-assumption-hunter-011","title":"Makefile git tag assumes git rev-parse --short HEAD succeeds in all execution environments","severity":"Minor","phase":{"primary":"plan","contributing":null},"section":"Makefile Update","issue":"The Makefile reviewer-eval target includes --tags 'git=$(shell git rev-parse --short HEAD)'. This fails silently (empty tag value) in detached HEAD state, in CI/CD environments without a git checkout, or in shallow clones where HEAD is not set. The failure mode is invisible: inspect eval runs successfully but log entries have an empty git tag, breaking any query that uses the tag to correlate runs to commits.","why_it_matters":"Eval log tagging is the mechanism for regression detection across commits (Phase 1 test plan item 2). If the tag is empty in 20% of CI runs, commit-correlated regression tracking produces gaps. Debugging 'why did recall drop' requires knowing which commit introduced the change — missing tags remove that context.","suggestion":"Wrap the git command in a default: --tags 'git=$(shell git rev-parse --short HEAD 2>/dev/null || echo unknown)'. This makes the failure visible (tag shows 'unknown') without breaking the eval run. Add a test in the Makefile's pre-eval validation: warn if git tag is 'unknown' before running evals."}
{"type":"finding","id":"v1-assumption-hunter-012","title":"'keep severity_calibration.py for comparison' assumes it still runs without change — it likely does not","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Key Design Decisions","issue":"The design states severity_calibration.py remains 'for comparison' and that 'ablation_tests.py remains functional (which still references the combined task pattern).' Session 21 established that the original combined task pattern failed because the skill-as-system-prompt approach produces interactive output. If severity_calibration.py loads the full skill as system prompt, it still produces accuracy=0.000 and 'comparison' is meaningless. The design treats keeping the file as costless, but a broken comparison file adds noise to the eval suite without information value.","why_it_matters":"A file described as providing 'regression signal for the orchestration layer' but which always produces 0.000 accuracy does not provide regression signal — it provides consistent zero noise. If ablation_tests.py still references the combined task and severity_calibration.py is broken, ablation tests that depend on it also produce no useful output.","suggestion":"Either (a) fix severity_calibration.py to use agent prompt (not full skill) so it produces real output and serves as a valid comparison baseline, or (b) explicitly mark it as a deprecated artifact with a comment explaining why it always produces 0.000, so future developers don't waste time debugging it. If ablation_tests.py depends on it, update ablation_tests.py to reference reviewer_eval.py tasks instead."}
{"type":"finding","id":"v1-assumption-hunter-013","title":"Design assumes v1 ground truth findings are correctly scoped to requirements-v1.md, not requirements-v2.md","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Ground Truth Management — Current Dataset","issue":"The design states the ground truth dataset is 'inspect-ai-integration-requirements-light/' with 10 validated Critical findings from the requirements-light review of requirements-v1.md. The requirements document now reviewed by this session is requirements-v2.md, which added 6 new requirements (FR-ARCH-1 through FR-ARCH-5, FR-QUALITY-1) and superseded FR2.1. Ground truth findings that point to v1 requirement sections (e.g., FR2.1, FR3) reference sections that no longer exist or have changed meaning in v2. Reviewers running against requirements-v2.md content will produce findings with different section references.","why_it_matters":"If reviewer tasks pass requirements-v2.md content as Sample.input but score against v1-derived ground truth, section mismatches cause the scorer to treat v2-valid findings as non-matches. The detection baseline established in Phase 1 measures recall against stale ground truth, making the baseline invalid for v2 requirements.","suggestion":"Resolve before Phase 1: either (a) run Phase 1 evals against requirements-v1.md content only (treat v1 ground truth as the fixed evaluation set, separate from v2 development), or (b) re-validate ground truth against requirements-v2.md content and update the dataset. If (b), note which findings are still valid in v2 and which are obsolete due to superseded requirements. Document the chosen approach in metadata.json."}
{"type":"finding","id":"v1-assumption-hunter-999","title":"Blind spot check: Assumption Hunter perspective","severity":"Minor","phase":{"primary":"design","contributing":null},"section":"Meta","issue":"This review focused on what the design assumed without stating. It may have underweighted: (1) Inspect AI API behavior specifics that are version-dependent and only discoverable by running code — the design learned this the hard way in Sessions 20–21 and I may be missing new instances; (2) The interaction between the scorer's matching logic and ID format conventions (exact match vs fuzzy match vs title similarity) — if the scorer matches on finding IDs and agents generate IDs with different prefixes than expected, recall scores zero without a clear error; (3) Whether the 'ground truth is living' insight from Session 19 is operationally implemented (is there actually a hash check in metadata.json today, or is it a planned requirement?) — I flagged the refresh process but may have missed whether the hash infrastructure exists yet.","why_it_matters":"Blind spot awareness helps catch gaps in the review process.","suggestion":"Consider: Run a quick check on the actual critical_findings.jsonl file to verify reviewer field values. Confirm output_parser.py's matching logic (exact ID? title similarity? section name?) before assuming the scorer handles the right fields. Check metadata.json for actual hash field presence vs planned."}
```

---

## Summary

### Critical (4 findings)

**001 — assumption-hunter.md still outputs markdown:** The v1 root cause is not fixed. The agent file retains the markdown output template. Phase 1 begins broken if implementation starts from the current state.

**002 — ADR-006 does not exist on disk:** The design and requirements both delegate key rationale to a missing document. This was flagged in the requirements review and persisted into the design.

**003 — reviewer field attribution unverified:** The entire per-reviewer filtering architecture depends on findings having a `reviewer` field matching agent filenames exactly. This is unverified in the existing ground truth dataset.

**004 — N=2 per reviewer produces incoherent eval signal:** At 2 findings per reviewer, one miss swings recall by 50 percentage points. Phase 1 cannot establish a reliable baseline at this sample size.

---

### Important (6 findings)

**005 — Multi-task Inspect AI invocation behavior unverified:** `inspect eval reviewer_eval.py` with no `--task` flag may not run all 5 tasks. The behavior is undocumented in the design.

**006 — Frontmatter stripping is brittle:** The `content.index('---', 3)` approach fails on missing frontmatter (ValueError) and body horizontal rules (incorrect truncation).

**007 — JSONL fence-stripping not addressed:** Claude wraps structured output in markdown fences by default. The parser must strip them. This was flagged in the requirements review and is absent from the design.

**008 — Dual-context agent constraint creates an unresolved tension:** Switching assumption-hunter.md to JSONL-only output degrades the production human experience. The design acknowledges the constraint but does not resolve it.

**009 — 'At least 1 task achieves' is the wrong Phase 1 success gate:** 4 of 5 agents broken constitutes Phase 1 complete under the current criteria. The gate allows declaring success on a mostly-broken framework.

**010 — Ground truth refresh assumes skill output retains per-reviewer attribution:** If parallax:requirements --light aggregates findings without preserving `reviewer` fields, every refresh cycle produces unannotated output that must be manually re-attributed.

---

### Minor (3 findings)

**011 — git rev-parse fails silently in detached HEAD / CI:** Empty git tags break commit-correlated regression tracking without an error.

**012 — severity_calibration.py is kept for 'comparison' but produces accuracy=0.000:** A perpetually-failing comparison file adds noise without information value. The rationale for keeping it does not survive scrutiny.

**013 — Ground truth findings are scoped to requirements-v1.md, not v2.md:** Running reviewers against v2 content scored against v1 ground truth produces section-reference mismatches that undercount valid detections.
