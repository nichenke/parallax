{"type":"finding","id":"v3-assumption-hunter-001","title":"Auto-Fix Step Assumes It Can Correctly Identify Trivial Changes","severity":"Critical","phase":{"primary":"design","contributing":"calibrate"},"section":"Step 4: Auto-Fix","issue":"Design specifies auto-fix classifies findings as trivial and applies automatically. Assumes system can reliably distinguish trivial mechanical fixes from semantic changes. Example: broken internal link could be wrong file extension (trivial) or wrong target document (semantic). No validation mechanism beyond undefined conservative criteria.","why_it_matters":"Auto-fixes modify source files and commit changes automatically. Misclassified fix that changes meaning breaks design. If auto-fix runs before human review, user cannot reject bad auto-fixes—they're already applied and committed.","suggestion":"Add validation requirements: (1) Define conservative with concrete examples and exclusion criteria, (2) Require diffs for user approval before application, (3) Add rollback mechanism, (4) Defer auto-fix to post-human-processing."}
{"type":"finding","id":"v3-assumption-hunter-002","title":"Git-Based Iteration Tracking Assumes Design Doc Lives in Git","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Output Artifacts, Cross-Iteration Finding Tracking","issue":"Design states iteration history tracked by git. Assumes design document being reviewed is git-tracked. If user reviews design doc outside repository (Google Doc exported to markdown, Confluence page, design from different repo), git diff fails. No fallback specified.","why_it_matters":"Requirements state this should be applicable to work contexts. Many teams use Confluence, Notion, or Google Docs for design documents, not git-tracked markdown. If parallax:review only works for git-tracked docs, excludes significant use cases. Cross-iteration tracking depends on git diff—without it, reviewers lose focus prioritization.","suggestion":"Add input validation checking whether design doc is git-tracked. If not: (1) Warn user that cross-iteration diff won't be available, (2) Fall back to file timestamp comparison or manual change notes, (3) Document git requirement as constraint, (4) Implement text-based diff as fallback."}
{"type":"finding","id":"v3-assumption-hunter-003","title":"Stable Finding IDs Assume Section Headings Don't Change","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Cross-Iteration Finding Tracking","issue":"Design specifies finding IDs as stable hash derived from section + issue content. Assumes design doc section headings remain stable across iterations. If designer refactors UX Flow into User Workflow and State Management between iterations, all findings anchored to UX Flow become orphaned. Hash-based IDs break when input text changes even if semantic content unchanged.","why_it_matters":"Section refactoring is normal during design iteration. Improving document structure shouldn't invalidate finding tracking. If implemented as designed, cross-iteration tracking will produce false negatives (findings marked resolved when section renamed) and false positives (findings marked new when actually rephrased).","suggestion":"Replace text-based hashing with semantic anchoring or LLM-based matching. Options: (1) Store section heading + offset position, fuzzy-match if heading changed, (2) Use LLM to semantically match findings, (3) Hybrid: hash as first pass, LLM disambiguation on miss, (4) Allow manual finding ID assignment for critical findings."}
{"type":"finding","id":"v3-assumption-hunter-005","title":"Async-First Architecture Assumes File System as Single Source of Truth","severity":"Important","phase":{"primary":"design","contributing":"calibrate"},"section":"UX Flow (async-first)","issue":"Design specifies review always writes artifacts to disk as baseline. Assumes all state lives in files under docs/reviews/<topic>/. If user runs review on machine A, processes findings on machine B (different clone), or collaborates with teammate, state diverges. File-based state requires all participants operate on same filesystem or rigorously sync via git. No synchronization mechanism specified.","why_it_matters":"Requirements emphasize applicable to work contexts and CLAUDE.md notes this repo may be worked on from multiple machines. File-based state doesn't handle distributed workflows without additional tooling. If two users process findings in parallel, last write wins—earlier dispositions silently lost.","suggestion":"Either (1) Document single-user, single-machine constraint explicitly as MVP limitation, (2) Add conflict detection (check if summary.md has uncommitted changes), (3) Require git commit after each disposition batch, (4) Evaluate external state management."}
{"type":"finding","id":"v3-edge-case-prober-001","title":"Cross-Iteration Matching Breaks When Design Structure Changes","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Cross-Iteration Finding Tracking","issue":"Design specifies stable finding IDs via hash of section + content. When designer refactors document structure (splits sections, renames headings, reorders content), hashes change. System treats structurally-relocated findings as resolved and creates duplicate new findings. Section-based anchoring fails when sections reorganize.","why_it_matters":"Major design iterations often involve restructuring for clarity. If cross-iteration tracking breaks on restructure, users lose history exactly when it's most valuable—during significant revision. False resolved signals mislead about progress. Duplicate new findings flood user with already-addressed issues.","suggestion":"Test with realistic structure change scenarios: section split, merge, rename, reorder. Implement semantic matching that survives structure changes. Consider content fingerprinting independent of section location, or LLM-based semantic equivalence checking."}
{"type":"finding","id":"v3-edge-case-prober-002","title":"Auto-Fix Re-Review Workflow May Loop Indefinitely","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Step 4: Auto-Fix","issue":"Design allows auto-fix to trigger re-review. If auto-fix produces changes that generate new findings requiring auto-fix, workflow loops. No termination condition specified. No iteration limit. No budget guard. Pathological case: auto-fix creates finding, re-review generates auto-fixable finding, infinite loop consuming API budget.","why_it_matters":"Infinite loops block workflow and burn budget. Even finite but excessive loops (10+ iterations) indicate design flaw. Without loop detection or iteration limits, user cannot safely enable auto-fix re-review feature. Must be addressed before auto-fix implementation.","suggestion":"Add termination conditions: (1) Maximum iteration limit (3-5 re-reviews), (2) Detect finding stability (stop if findings unchanged across iterations), (3) Budget guard (stop if cost exceeds threshold), (4) User approval before each re-review iteration."}
{"type":"finding","id":"v3-edge-case-prober-003","title":"Partial Reviewer Completion Corrupts Systemic Detection","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Systemic Issue Detection","issue":"Design allows 4/6 reviewers as minimum threshold for partial completion. Systemic detection counts findings by phase: if >30% share contributing phase, flag systemic issue. If 2 reviewers fail (especially if they'd flag underrepresented phases), detection percentages skew. Missing data biases results toward phases covered by completed reviewers.","why_it_matters":"Systemic detection drives escalation decisions. If partial results produce false positives (flag systemic when missing reviewers would have balanced), user escalates unnecessarily. If false negatives (don't flag systemic because missing reviewers would have pushed over threshold), user misses root cause. Partial data must not corrupt detection.","suggestion":"Options: (1) Disable systemic detection if any reviewer failed, (2) Require minimum diversity (at least one reviewer per expected phase coverage), (3) Weight detection by coverage (flag only if confidence high given partial data), (4) Mark systemic results as provisional when partial."}
{"type":"finding","id":"v3-edge-case-prober-004","title":"Severity Range Resolution Can Create False Escalations","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Verdict Logic","issue":"Design uses highest severity in range for verdict (conservative). If one pessimistic reviewer rates finding as Critical while others rate Important, verdict logic treats as Critical. Single outlier can trigger escalation. If reviewer calibration differs (one consistently rates higher), that reviewer controls verdict outcomes.","why_it_matters":"False escalations block progress when findings don't actually require escalation. If one reviewer is systematically more conservative, design never proceeds—every finding gets worst-case treatment. User loses benefit of multi-perspective review if single perspective dominates verdict.","suggestion":"Consider severity resolution strategies: (1) Majority vote (most reviewers win), (2) Escalate only if 2+ reviewers flag as Critical, (3) Present severity distribution to user, let them decide, (4) Calibrate reviewers to align severity scales, (5) Track per-reviewer severity distributions, flag outliers."}
{"type":"finding","id":"v3-requirement-auditor-002","title":"JSONL Output Format Schema Completely Missing","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"Output Artifacts (JSONL is canonical format)","issue":"Requirements specify JSONL is canonical output format for reviewer findings (FR6.1). Design references JSONL throughout (reviewers output JSONL, synthesizer reads JSONL, schema validation per FR7.5). But no schema definition exists. What fields? Required vs optional? Finding ID format? Severity enum values? Phase classification structure? Completely unspecified.","why_it_matters":"Four reviewers flagged this in v2 review. Issue persists in v3. Cannot implement FR7.5 (schema validation) without schema. Cannot implement JSONL output without field definitions. This is a design-blocking gap—referenced as decided but never specified. Must be resolved before any JSONL implementation.","suggestion":"Define JSONL schema immediately: (1) Create schema file in schemas/ directory, (2) Document required fields (finding ID, title, severity, phase, section, issue, rationale, suggestion), (3) Specify optional fields (disposition, tags, cross-refs), (4) Define enums (severity, phase labels), (5) Validate schema with JSON Schema specification."}
{"type":"finding","id":"v3-requirement-auditor-005","title":"Model Tiering Strategy Unspecified","severity":"Important","phase":{"primary":"design","contributing":"calibrate"},"section":"Reviewer Personas","issue":"Design references model tiering (Haiku for mechanical, Sonnet for analysis, Opus for deep review) but doesn't specify which reviewers use which models. Does Assumption Hunter need Opus-level reasoning? Can Prior Art Scout use Haiku for search? What's the quality vs cost tradeoff per persona? Requirements mention model tiering (NFR3.3) but assignment strategy unspecified.","why_it_matters":"Model choice impacts both cost and quality. Wrong model assignment (Haiku for complex reasoning, Opus for simple search) wastes budget or reduces quality. NFR2.1 targets <$1 per review at Sonnet pricing. If all reviewers use Opus, cost exceeds target. Need explicit model assignment with rationale.","suggestion":"Document model assignment per reviewer: (1) Specify model tier per persona with rationale, (2) Estimate cost impact (token counts x pricing), (3) Validate against NFR2.1 cost target, (4) Plan eval testing (Haiku vs Sonnet quality tradeoff), (5) Make model configurable for testing."}
{"type":"finding","id":"v3-feasibility-skeptic-003","title":"Prompt Caching Invalidation on Calibration Updates","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Reviewer Prompt Architecture","issue":"Design structures prompts as stable cacheable prefix + variable suffix. Calibration improvements (learning loop, false positive feedback) require modifying reviewer prompts. Adding calibration rules to stable prefix invalidates cache. Next review pays full input token cost. Cost optimization via caching conflicts with quality improvement via calibration.","why_it_matters":"Learning loop (Iteration 2 Finding 29 disposition) is core to improving reviewer quality over time. If every calibration change costs 90% cache benefit, improving prompts becomes prohibitively expensive. Design acknowledges conflict but provides no solution. Must resolve before implementing calibration.","suggestion":"Separate calibration rules from cacheable prefix. Three-part prompt structure: (1) Stable prefix (persona, methodology, format—rarely changes), (2) Calibration rules (versioned, changes based on feedback, NOT cached), (3) Variable suffix (artifact being reviewed). Cache stable prefix. Include calibration rules in non-cached middle section. Track calibration version separately."}
{"type":"finding","id":"v3-feasibility-skeptic-008","title":"Reviewer Tool Access Needs Coordination for External APIs","severity":"Important","phase":{"primary":"design","contributing":null},"section":"Reviewer Capabilities","issue":"Design specifies Prior Art Scout can use gh, curl for external research. Multiple reviewers independently querying same resource wastes quota. GitHub API has rate limits (60/hour unauthenticated, 5000/hour authenticated). If 6 reviewers all search GitHub for design review tools, that's 6 API calls. Expensive for same data. No coordination or caching specified.","why_it_matters":"External API quotas are finite. Redundant queries waste quota and slow review. If review runs in CI/CD (future automation), rate limits become blocking failures. Need result sharing across reviewers for quota-limited resources.","suggestion":"Add result caching for external API calls: (1) If Prior Art Scout searches GitHub for X, cache result, (2) Other reviewers can reference cached result, (3) Coordinate tool access (maybe only one reviewer per external resource), (4) Document which tools are quota-limited, (5) Plan for CI/CD rate limit handling."}
{"type":"finding","id":"v3-feasibility-skeptic-011","title":"Test Case Validation Still Missing After Three Review Iterations","severity":"Critical","phase":{"primary":"calibrate","contributing":null},"section":"Testing & Validation","issue":"After 3 review iterations, only self-review completed. Second Brain test case (real project with 40+ known design flaws) has not been run. Design claims reviewers catch design flaws, but no external validation performed. Self-review catches process issues but not effectiveness at finding real problems.","why_it_matters":"Without test case validation, unknown whether reviewers actually catch design flaws in real designs. Self-review is circular—reviewers find issues in review design, but that doesn't prove they'd find issues in Second Brain API design. Need external validation before claiming tool works. This is a requirements validation gap—build vs leverage decision deferred pending evaluation, but evaluation never performed.","suggestion":"Run Second Brain test case immediately: (1) Use known design with documented flaws, (2) Run parallax:review on Second Brain design, (3) Check if reviewers flag known issues, (4) Measure false positives (flagged non-issues), (5) Use results to validate approach before further development."}
{"type":"finding","id":"v3-first-principles-002","title":"Problem Framing Inversion: Requirements Review Higher Leverage Than Design Review","severity":"Critical","phase":{"primary":"calibrate","contributing":"survey"},"section":"Problem Statement","issue":"Requirements review prevents design flaws, design review detects them. Prevention > detection. Requirements stated as job-to-be-done in requirements doc: catch design flaws in phase that caused them (Job 2). Highest leverage is catching requirement gaps before design starts. But prototype focuses on design review, defers requirements review to later. Building detection before prevention inverts priority.","why_it_matters":"If requirements have gaps, design review will catch downstream symptoms but not root cause. Findings will escalate to calibrate phase, requiring requirements rework anyway. Building design review first means building symptom detection, then building root cause detection. More efficient to validate requirements review (higher leverage) before design review (lower leverage).","suggestion":"Consider prioritizing requirements review prototype: (1) Run adversarial review on requirements doc (parallax-review-requirements-v1.md), (2) Validate that requirement gaps are catchable via review, (3) Compare effort of fixing requirements vs fixing design, (4) If requirements review proves higher leverage, build that skill first."}
{"type":"finding","id":"v3-first-principles-003","title":"Adversarial Naming Mismatch: Coverage-Based Inspection Not Adversarial Debate","severity":"Important","phase":{"primary":"calibrate","contributing":null},"section":"Name (parallax:review)","issue":"Tool name references adversarial review but design implements coverage-based inspection with distinct personas. Adversarial implies conflicting incentives (debate, argument, competing positions). Distinct personas have non-overlapping focus areas (assumptions, edge cases, requirements, feasibility). Reviewers don't debate each other—they inspect from different angles. This is parallax (multiple viewpoints) not adversarial (opposing positions).","why_it_matters":"Naming shapes expectations. Users expecting adversarial debate may be confused by coverage-based output (6 independent reviews, no debate synthesis). Adversarial review in academic context means reviewers challenge each other's positions. This tool doesn't do that—it provides multi-perspective inspection. Naming mismatch may cause misaligned usage.","suggestion":"Consider whether adversarial accurately describes the approach: (1) If reviewers should debate (synthesizer presents contradictions for resolution), keep adversarial, (2) If reviewers provide independent coverage, consider multi-perspective review or parallax review (already in name), (3) If this is exploratory (defer to empirical eval), note naming assumption for future validation."}
{"type":"finding","id":"v3-prior-art-scout-001","title":"Design Doc Sync Addresses Decisions But Not Specifications","severity":"Important","phase":{"primary":"design","contributing":"calibrate"},"section":"Iteration 3 Changes","issue":"Iteration 2 flagged 67% documentation debt (Requirement Auditor). Between v2 and v3, 23 accepted dispositions were synchronized to design doc. Major improvement: auto-fix mechanism added, cross-iteration tracking expanded, primary + contributing phases specified. But design addresses what to do, not how it works. JSONL schema still missing despite being referenced. Auto-fix workflow added but classification criteria, validation, git safety unspecified. Prompt caching section added but cache boundary, versioning, invalidation strategy unspecified.","why_it_matters":"Documentation debt resolved (good) but architectural specification gaps remain (blocker). Saying we decided to use JSONL is different from defining the JSONL schema. Accepted decisions need implementation specifications. Without specifications, implementer makes unreviewed assumptions. Four architectural decisions documented as decided but unspecified.","suggestion":"Distinguish documentation debt (decided but not written down) from specification gaps (decided but not detailed). For each accepted decision that requires implementation: (1) Define the mechanism (JSONL schema, auto-fix workflow, finding ID algorithm), (2) Specify validation criteria, (3) Document edge cases, (4) Provide examples. Treating specifications as documentation debt incorrectly scopes the work."}
{"type":"finding","id":"v3-prior-art-scout-002","title":"Build vs Leverage Decision Deferred Despite Significant Prior Art","severity":"Critical","phase":{"primary":"calibrate","contributing":"survey"},"section":"Evaluation Strategy","issue":"Inspect AI, LangGraph, LangSmith, Braintrust identified as mature solutions solving 60-80% of custom infrastructure needs. Design defers evaluation to when limits hit. But limits are unknowable without building first. If parallax builds custom orchestration then discovers Inspect AI handles it, that's wasted effort. Build vs leverage decision should precede building, not follow it.","why_it_matters":"Iteration 2 Finding 46 flagged Compound Engineering (15 review agents, learning loop, 8.9k stars) as direct prior art. Disposition: evaluate during testing. But if evaluation discovers we should have leveraged existing tools, we've built unnecessarily. SWE principle: evaluate build vs leverage before building. Deferring to when limits hit means building first, evaluating later—backwards.","suggestion":"Time-box prior art evaluation before significant custom development: (1) Spend 2-4 hours testing Inspect AI with sample review task, (2) Validate whether it handles multi-agent review orchestration, (3) Test LangGraph for state management, (4) If existing tools handle 80%+ of needs, leverage them, (5) Only build custom if clear gaps identified. Evaluation before building, not after."}
