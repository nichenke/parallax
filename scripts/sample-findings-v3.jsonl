{"type":"finding","id":"v3-assumption-hunter-001","title":"Auto-Fix Step Assumes It Can Correctly Identify Trivial Changes","severity":"Critical","phase":{"primary":"design","contributing":"calibrate"},"section":"Step 4: Auto-Fix","issue":"Design specifies auto-fix classifies findings as trivial and applies automatically. Assumes system can reliably distinguish trivial mechanical fixes from semantic changes. Example: broken internal link could be wrong file extension (trivial) or wrong target document (semantic). No validation mechanism beyond undefined conservative criteria.","why_it_matters":"Auto-fixes modify source files and commit changes automatically. Misclassified fix that changes meaning breaks design. If auto-fix runs before human review, user cannot reject bad auto-fixes—they're already applied and committed.","suggestion":"Add validation requirements: (1) Define conservative with concrete examples and exclusion criteria, (2) Require diffs for user approval before application, (3) Add rollback mechanism, (4) Defer auto-fix to post-human-processing."}
{"type":"finding","id":"v3-assumption-hunter-002","title":"Git-Based Iteration Tracking Assumes Design Doc Lives in Git","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Output Artifacts, Cross-Iteration Finding Tracking","issue":"Design states iteration history tracked by git. Assumes design document being reviewed is git-tracked. If user reviews design doc outside repository (Google Doc exported to markdown, Confluence page, design from different repo), git diff fails. No fallback specified.","why_it_matters":"Requirements state this should be applicable to work contexts. Many teams use Confluence, Notion, or Google Docs for design documents, not git-tracked markdown. If parallax:review only works for git-tracked docs, excludes significant use cases. Cross-iteration tracking depends on git diff—without it, reviewers lose focus prioritization.","suggestion":"Add input validation checking whether design doc is git-tracked. If not: (1) Warn user that cross-iteration diff won't be available, (2) Fall back to file timestamp comparison or manual change notes, (3) Document git requirement as constraint, (4) Implement text-based diff as fallback."}
{"type":"finding","id":"v3-assumption-hunter-003","title":"Stable Finding IDs Assume Section Headings Don't Change","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Cross-Iteration Finding Tracking","issue":"Design specifies finding IDs as stable hash derived from section + issue content. Assumes design doc section headings remain stable across iterations. If designer refactors UX Flow into User Workflow and State Management between iterations, all findings anchored to UX Flow become orphaned. Hash-based IDs break when input text changes even if semantic content unchanged.","why_it_matters":"Section refactoring is normal during design iteration. Improving document structure shouldn't invalidate finding tracking. If implemented as designed, cross-iteration tracking will produce false negatives (findings marked resolved when section renamed) and false positives (findings marked new when actually rephrased).","suggestion":"Replace text-based hashing with semantic anchoring or LLM-based matching. Options: (1) Store section heading + offset position, fuzzy-match if heading changed, (2) Use LLM to semantically match findings, (3) Hybrid: hash as first pass, LLM disambiguation on miss, (4) Allow manual finding ID assignment for critical findings."}
{"type":"finding","id":"v3-assumption-hunter-005","title":"Async-First Architecture Assumes File System as Single Source of Truth","severity":"Important","phase":{"primary":"design","contributing":"calibrate"},"section":"UX Flow (async-first)","issue":"Design specifies review always writes artifacts to disk as baseline. Assumes all state lives in files under docs/reviews/<topic>/. If user runs review on machine A, processes findings on machine B (different clone), or collaborates with teammate, state diverges. File-based state requires all participants operate on same filesystem or rigorously sync via git. No synchronization mechanism specified.","why_it_matters":"Requirements emphasize applicable to work contexts and CLAUDE.md notes this repo may be worked on from multiple machines. File-based state doesn't handle distributed workflows without additional tooling. If two users process findings in parallel, last write wins—earlier dispositions silently lost.","suggestion":"Either (1) Document single-user, single-machine constraint explicitly as MVP limitation, (2) Add conflict detection (check if summary.md has uncommitted changes), (3) Require git commit after each disposition batch, (4) Evaluate external state management."}
{"type":"finding","id":"v3-edge-case-prober-001","title":"Cross-Iteration Matching Breaks When Design Structure Changes","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Cross-Iteration Finding Tracking","issue":"Design specifies stable finding IDs via hash of section + content. When designer refactors document structure (splits sections, renames headings, reorders content), hashes change. System treats structurally-relocated findings as resolved and creates duplicate new findings. Section-based anchoring fails when sections reorganize.","why_it_matters":"Major design iterations often involve restructuring for clarity. If cross-iteration tracking breaks on restructure, users lose history exactly when it's most valuable—during significant revision. False resolved signals mislead about progress. Duplicate new findings flood user with already-addressed issues.","suggestion":"Test with realistic structure change scenarios: section split, merge, rename, reorder. Implement semantic matching that survives structure changes. Consider content fingerprinting independent of section location, or LLM-based semantic equivalence checking."}
{"type":"finding","id":"v3-edge-case-prober-002","title":"Auto-Fix Re-Review Workflow May Loop Indefinitely","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Step 4: Auto-Fix","issue":"Design allows auto-fix to trigger re-review. If auto-fix produces changes that generate new findings requiring auto-fix, workflow loops. No termination condition specified. No iteration limit. No budget guard. Pathological case: auto-fix creates finding, re-review generates auto-fixable finding, infinite loop consuming API budget.","why_it_matters":"Infinite loops block workflow and burn budget. Even finite but excessive loops (10+ iterations) indicate design flaw. Without loop detection or iteration limits, user cannot safely enable auto-fix re-review feature. Must be addressed before auto-fix implementation.","suggestion":"Add termination conditions: (1) Maximum iteration limit (3-5 re-reviews), (2) Detect finding stability (stop if findings unchanged across iterations), (3) Budget guard (stop if cost exceeds threshold), (4) User approval before each re-review iteration."}
{"type":"finding","id":"v3-edge-case-prober-003","title":"Partial Reviewer Completion Corrupts Systemic Detection","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Systemic Issue Detection","issue":"Design allows 4/6 reviewers as minimum threshold for partial completion. Systemic detection counts findings by phase: if >30% share contributing phase, flag systemic issue. If 2 reviewers fail (especially if they'd flag underrepresented phases), detection percentages skew. Missing data biases results toward phases covered by completed reviewers.","why_it_matters":"Systemic detection drives escalation decisions. If partial results produce false positives (flag systemic when missing reviewers would have balanced), user escalates unnecessarily. If false negatives (don't flag systemic because missing reviewers would have pushed over threshold), user misses root cause. Partial data must not corrupt detection.","suggestion":"Options: (1) Disable systemic detection if any reviewer failed, (2) Require minimum diversity (at least one reviewer per expected phase coverage), (3) Weight detection by coverage (flag only if confidence high given partial data), (4) Mark systemic results as provisional when partial."}
{"type":"finding","id":"v3-edge-case-prober-004","title":"Severity Range Resolution Can Create False Escalations","severity":"Critical","phase":{"primary":"design","contributing":null},"section":"Verdict Logic","issue":"Design uses highest severity in range for verdict (conservative). If one pessimistic reviewer rates finding as Critical while others rate Important, verdict logic treats as Critical. Single outlier can trigger escalation. If reviewer calibration differs (one consistently rates higher), that reviewer controls verdict outcomes.","why_it_matters":"False escalations block progress when findings don't actually require escalation. If one reviewer is systematically more conservative, design never proceeds—every finding gets worst-case treatment. User loses benefit of multi-perspective review if single perspective dominates verdict.","suggestion":"Consider severity resolution strategies: (1) Majority vote (most reviewers win), (2) Escalate only if 2+ reviewers flag as Critical, (3) Present severity distribution to user, let them decide, (4) Calibrate reviewers to align severity scales, (5) Track per-reviewer severity distributions, flag outliers."}
