{"type": "finding", "id": "problem-framer-001", "title": "Problem statement unchanged despite root cause being wrong in v1", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "Problem Statement (unchanged from v1)", "issue": "The v2 problem statement is verbatim from v1: 'no systematic way to measure effectiveness.' But v1 failed not because measurement was absent — it failed because the measurement architecture assumed skills were single-turn reviewers when they are orchestration workflows. The real root cause is: the eval framework was designed against a wrong mental model of what a 'skill' is.", "why_it_matters": "If the problem is framed as 'no measurement capability,' the fix is 'add measurement.' But if the problem is 'wrong mental model of the unit under test,' the fix requires explicitly establishing what the testable unit is before writing requirements.", "suggestion": "Update the problem statement to include the v1 failure mode and what v2 corrects.", "validation_status": "real_flaw", "reviewer": "problem-framer"}
{"type": "finding", "id": "problem-framer-006", "title": "v2 has no success criteria — only a phase map", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "MVP Phase Map (updated from v1)", "issue": "v1 had a 'Success Criteria' section with eight explicit, numbered criteria. v2 removes this and replaces it with a Phase Map table that shows status but not completion gates. There is no document-level success gate that answers: 'How do we know v2 requirements are correctly implemented?'", "why_it_matters": "Without v2 success criteria, the requirements document cannot be used to declare implementation done. 'Done' is determined by whoever is implementing, not by the requirements.", "suggestion": "Add a 'v2 Success Criteria' section defining Phase 1 completion: all 5 per-reviewer tasks run without error, at least one reviewer achieves non-zero accuracy, JSONL parseable from all agents, eval cost within budget.", "validation_status": "real_flaw", "reviewer": "problem-framer"}
{"type": "finding", "id": "scope-guardian-004", "title": "Open question on ground truth size left unresolved — blocks Phase 1 scope definition", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "Open Questions (v2)", "issue": "Open Question 3 asks whether 10 validated Critical findings are sufficient for Phase 1 or whether Important findings should be added. This is not deferrable — it directly determines the scope of Phase 1. If the answer is 'add Important findings,' Phase 1 scope expands. Leaving this open makes the Phase 1 acceptance criteria for FR-ARCH-1 untestable.", "why_it_matters": "A ground truth dataset that grows mid-phase will break the detection baseline and invalidate any accuracy measurements taken before expansion.", "suggestion": "Resolve inline: specify that Phase 1 uses Critical-only ground truth (10 findings) and define a threshold for when to expand. Remove from Open Questions.", "validation_status": "real_flaw", "reviewer": "scope-guardian"}
{"type": "finding", "id": "assumption-hunter-001", "title": "Assumes agent files are already eval-compatible without modification", "severity": "Critical", "phase": {"primary": "design", "contributing": null}, "section": "FR-ARCH-2: Eval-Compatible Agent Interface", "issue": "FR-ARCH-2 requires agents to work in eval context without modifications to the agent file, but there is no audit step verifying this. If any agent currently instructs the model to use a Read tool or reference a file path, the acceptance criterion is already violated before Phase 1 begins.", "why_it_matters": "If even one agent file requires modification to become eval-compatible, this is untracked scope. Phase 1 cannot validate the no-modification constraint until all agents are tested.", "suggestion": "Add an explicit audit step as a Phase 1 prerequisite: audit all 5 reviewer agent files against FR-ARCH-2 criteria and log findings before implementing reviewer_eval.py.", "validation_status": "real_flaw", "reviewer": "assumption-hunter"}
{"type": "finding", "id": "success-validator-001", "title": "Phase 1 completion has no measurable success criterion", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "MVP Phase Map", "issue": "'Non-zero accuracy' is not a threshold — it could mean 1 finding detected out of 10. Phase 1 never formally completes and Phase 1.5 has no baseline to compare against.", "why_it_matters": "Without a concrete Phase 1 exit threshold, the team cannot know when to advance. Phase 1.5 multi-model comparison requires a stable baseline.", "suggestion": "Add Phase 1 completion criterion: (a) all 5 reviewer tasks parse non-zero findings, (b) detection rate ≥50% on ground truth Critical findings across all reviewers combined, (c) make eval exits 0.", "validation_status": "real_flaw", "reviewer": "success-validator"}
{"type": "finding", "id": "success-validator-002", "title": "v1 detection thresholds (90%/80%) not confirmed or superseded in v2", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": null}, "section": "FR-QUALITY-1", "issue": "v1 established 90% recall and 80% precision as targets. v2 references these only in an example but never restates, adjusts, or explicitly deprecates them. It is unclear whether these thresholds apply to Phase 1, Phase 2, or at all.", "why_it_matters": "The scorer cannot determine pass/fail without a stated target. Running Phase 1 evals against Phase 2 thresholds produces misleading failures.", "suggestion": "Add a 'Detection Targets' section: Phase 1 target ≥50% recall (architecture correctness floor), Phase 2 target 90% recall / 80% precision (v1 thresholds restored). Make each threshold phase-gated.", "validation_status": "real_flaw", "reviewer": "success-validator"}
{"type": "finding", "id": "problem-framer-003", "title": "ADR-006 referenced but does not exist on disk", "severity": "Critical", "phase": {"primary": "calibrate", "contributing": "design"}, "section": "Background", "issue": "ADR-006 is referenced in Background and FR-ARCH-1 but not present in the docs/requirements/ directory.", "why_it_matters": "Requirements that delegate rationale to a non-existent reference are self-incomplete.", "suggestion": "Create ADR-006 or inline the rationale.", "validation_status": "false_positive", "false_positive_reason": "Branch state artifact — ADR-006 exists on feat/inspect-ai-integration-design at fb97370. Resolves at PR merge.", "reviewer": "problem-framer"}
{"type": "finding", "id": "assumption-hunter-002", "title": "Assumes document content in Sample.input is sufficient for all reviewer types", "severity": "Critical", "phase": {"primary": "design", "contributing": null}, "section": "FR-ARCH-2: Eval-Compatible Agent Interface", "issue": "Some reviewer agents may require external context that cannot be embedded in a single Sample.input string.", "why_it_matters": "Produces misleading detection baseline for agents that require external lookups.", "suggestion": "Document which agents are viable in single-document context for Phase 1.", "validation_status": "false_positive", "false_positive_reason": "Not a gap for Phase 1 scope. All 5 requirements reviewers (assumption-hunter, scope-guardian, problem-framer, constraint-finder, success-validator) are context-independent. prior-art-scout (which requires external context) is not in scope for parallax:requirements skill.", "reviewer": "assumption-hunter"}
{"type": "finding", "id": "assumption-hunter-003", "title": "Assumes JSONL output from reviewer agents is parseable without format negotiation", "severity": "Critical", "phase": {"primary": "design", "contributing": null}, "section": "FR-ARCH-3: JSONL Output Format as Explicit Requirement", "issue": "LLMs frequently wrap structured output in markdown fences. A strict parser returns 0 findings for a valid-but-fenced response.", "why_it_matters": "Indistinguishable from agent finding nothing. Root cause invisible in eval logs.", "suggestion": "Specify parser fence-tolerance behavior or instruct agents to output raw JSONL.", "validation_status": "false_positive", "false_positive_reason": "Implementation detail, not a requirements gap. Belongs in output_parser.py or agent instructions, not requirements. Over-specified requirement (see Issue #58 — Outcome Auditor reviewer).", "reviewer": "assumption-hunter"}
