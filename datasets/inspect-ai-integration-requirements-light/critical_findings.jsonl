{"id": "v1-problem-framer-006", "type": "finding", "severity": "Critical", "title": "Ground truth validation circular dependency", "description": "v3 findings are unvalidated LLM outputs, not confirmed design flaws. Eval framework measures 'do new skills match old outputs' rather than 'do skills catch real flaws.' Chicken-and-egg problem blocks the entire approach.", "validation_status": "real_flaw", "reviewer": "problem-framer"}
{"id": "v1-problem-framer-008", "type": "finding", "severity": "Critical", "title": "MVP solves measurement, not validation", "description": "Phase 1 validates 'Inspect AI works' not 'parallax skills work.' Finding quality scorer deferred to Phase 2 means MVP cannot answer the core question: do skills catch real design flaws?", "validation_status": "real_flaw", "reviewer": "problem-framer"}
{"id": "v1-constraint-finder-009", "type": "finding", "severity": "Critical", "title": "Circular validation in ablation tests", "description": "FR6.1 establishes baseline from first eval run, then FR4.1 validates ablation against that baseline. Ground truth established before validation allows false positives to contaminate the baseline, invalidating the eval framework.", "validation_status": "real_flaw", "reviewer": "constraint-finder"}
{"id": "v1-assumption-hunter-001", "type": "finding", "severity": "Critical", "title": "Ground truth validity assumed", "description": "Entire eval framework optimizes against v3 findings without validating they are correct. If v3 has false positives or missed issues, the eval framework optimizes for wrong outcomes. Duplicate root cause of problem-framer-006.", "validation_status": "real_flaw", "reviewer": "assumption-hunter"}
{"id": "v1-scope-guardian-013", "type": "finding", "severity": "Critical", "title": "Confidence measurement undefined", "description": "FR2.3 requires flagging findings where model has <95% confidence but does not define how confidence is measured. Without a definition the requirement is unimplementable and blocks the manual validation workflow.", "validation_status": "real_flaw", "reviewer": "scope-guardian"}
{"id": "v1-constraint-finder-002", "type": "finding", "severity": "Critical", "title": "API key security undefined", "description": "No separation between work and personal API keys specified in requirements. Work context uses Bedrock IAM roles, personal uses direct API env vars. Credential separation is a real requirement that was not stated.", "validation_status": "real_flaw", "reviewer": "constraint-finder"}
{"id": "v1-assumption-hunter-013", "type": "finding", "severity": "Critical", "title": "Confidence measurement undefined across multiple requirements", "description": "95% confidence threshold appears in 5 places (FR2.3, FR3.3, NFR4.1, Success Criteria, Open Questions) but calculation methodology is never defined. Requirement is literally unimplementable without a measurement specification. Duplicate of scope-guardian-013.", "validation_status": "real_flaw", "reviewer": "assumption-hunter"}
{"id": "v1-success-validator-001", "type": "finding", "severity": "Critical", "title": "Detection rate has no target thresholds", "description": "FR2.2 specifies recall, precision, and F1 metrics but provides no target values. Cannot determine pass or fail without quantified thresholds.", "validation_status": "real_flaw", "reviewer": "success-validator"}
{"id": "v1-success-validator-002", "type": "finding", "severity": "Critical", "title": "Ablation test baseline undefined", "description": "FR4.1 references 'detection rate X%' as baseline but X is never defined anywhere. The ablation test condition 'detection rate < (X - 50)%' cannot run without a value for X.", "validation_status": "real_flaw", "reviewer": "success-validator"}
{"id": "v1-post-review-001", "type": "finding", "severity": "Critical", "title": "Skill/eval interface mismatch: orchestration skills cannot be used as eval system prompts", "description": "Requirements assume the parallax skill can be loaded directly as an eval system prompt (FR2.1, FR7.1). In reality, parallax skills are orchestration workflows â€” they ask interactive questions, dispatch parallel agents via Task tool, and write findings to disk. A single-turn eval context provides none of these affordances. No requirement specifies an eval-compatible interface (a single-turn reviewer that outputs JSONL to completion text). Discovered during implementation when make eval returned 102 output tokens of interactive prompts instead of findings.", "validation_status": "real_flaw", "reviewer": "post-review", "discovery": "implementation"}
