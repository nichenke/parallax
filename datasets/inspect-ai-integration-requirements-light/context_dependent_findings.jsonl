{"id": "problem-framer-006", "title": "Ground truth validation circular dependency", "issue": "v3 findings are unvalidated LLM outputs, not confirmed design flaws. Eval framework measures 'do new skills match old outputs' rather than 'do skills catch real flaws.' Chicken-and-egg problem blocks meaningful validation.", "severity": "Critical", "reviewer": "problem-framer", "required_context": "Requires knowing that ground truth was sourced from v3 review outputs and that v3 is itself unvalidated LLM output. A reviewer cannot determine the ground truth provenance from the requirements document alone."}
{"id": "assumption-hunter-001", "title": "Ground truth validity assumed", "issue": "Entire eval framework optimizes against v3 findings without validating they are correct. If v3 has false positives or missed issues, the eval framework optimizes for wrong outcomes.", "severity": "Critical", "reviewer": "assumption-hunter", "required_context": "Requires knowing the ground truth came from v3 LLM outputs. Same root cause as problem-framer-006 — context-dependent on knowing the ground truth provenance."}
{"id": "post-review-001", "title": "Skill/eval interface mismatch: orchestration skills cannot be used as eval system prompts", "issue": "Requirements assume the parallax skill can be loaded directly as an eval system prompt (FR2.1, FR7.1). In reality, parallax skills are orchestration workflows that ask interactive questions and dispatch subagents — they cannot run as a single-turn eval solver.", "severity": "Critical", "reviewer": "post-review", "required_context": "Requires knowing how parallax skills actually work (orchestration workflows, not simple prompts). This architectural knowledge is not in the requirements document itself."}
